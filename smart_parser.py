import aiohttp
import asyncio
import logging
import json
from datetime import datetime
import urllib.parse

logger = logging.getLogger(__name__)

class SmartNewsParser:
    def __init__(self):
        self.user_agent = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    
    async def search_news(self, query, max_results=8):
        """–£–º–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Ä–∞–∑–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏"""
        results = []
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ–∏—Å–∫–∞
        methods = [
            self.search_duckduckgo(query),
            self.search_bing_news(query),
            self.search_newsapi_public(query),
        ]
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –º–µ—Ç–æ–¥—ã
        search_results = await asyncio.gather(*methods, return_exceptions=True)
        
        for result in search_results:
            if isinstance(result, list):
                results.extend(result)
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–µ
        if not results:
            results = self.get_demo_news(query)
        
        return results[:max_results]
    
    async def search_duckduckgo(self, query):
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo (–±–æ–ª–µ–µ –ª–∏–±–µ—Ä–∞–ª—å–Ω—ã–π –∫ –∑–∞–ø—Ä–æ—Å–∞–º)"""
        results = []
        try:
            # DuckDuckGo Instant Answer API
            url = "https://api.duckduckgo.com/"
            params = {
                'q': f'{query} –†–æ—Å—Å–∏—è –Ω–æ–≤–æ—Å—Ç–∏',
                'format': 'json',
                'no_html': '1',
                'skip_disambig': '1'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=10) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã
                        for topic in data.get('RelatedTopics', [])[:5]:
                            if 'FirstURL' in topic and 'Text' in topic:
                                results.append({
                                    'title': topic['Text'],
                                    'url': topic['FirstURL'],
                                    'source': 'DuckDuckGo',
                                    'description': '–ü–æ–∏—Å–∫–æ–≤–∞—è –≤—ã–¥–∞—á–∞',
                                    'date': datetime.now().strftime("%Y-%m-%d"),
                                    'timestamp': datetime.now().timestamp()
                                })
        except Exception as e:
            logger.warning(f"DuckDuckGo error: {e}")
        
        return results
    
    async def search_bing_news(self, query):
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Bing News RSS"""
        results = []
        try:
            encoded_query = urllib.parse.quote(f"{query} site:ru")
            url = f"https://www.bing.com/news/search?q={encoded_query}&format=rss"
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers={'User-Agent': self.user_agent}, timeout=10) as response:
                    if response.status == 200:
                        content = await response.text()
                        
                        # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ RSS
                        if '<item>' in content:
                            import re
                            items = re.findall(r'<item>(.*?)</item>', content, re.DOTALL)
                            
                            for item in items[:5]:
                                title_match = re.search(r'<title>(.*?)</title>', item)
                                link_match = re.search(r'<link>(.*?)</link>', item)
                                desc_match = re.search(r'<description>(.*?)</description>', item)
                                
                                if title_match and link_match:
                                    results.append({
                                        'title': title_match.group(1),
                                        'url': link_match.group(1),
                                        'source': 'Bing News',
                                        'description': desc_match.group(1) if desc_match else '–ù–æ–≤–æ—Å—Ç—å –∏–∑ –ø–æ–∏—Å–∫–∞',
                                        'date': datetime.now().strftime("%Y-%m-%d"),
                                        'timestamp': datetime.now().timestamp()
                                    })
        except Exception as e:
            logger.warning(f"Bing News error: {e}")
        
        return results
    
    async def search_newsapi_public(self, query):
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –ø—É–±–ª–∏—á–Ω—ã–µ NewsAPI endpoints"""
        results = []
        try:
            # –ü—É–±–ª–∏—á–Ω—ã–π endpoint (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω)
            url = "https://newsapi.org/v2/everything"
            params = {
                'q': f'{query} Russia',
                'language': 'ru',
                'sortBy': 'publishedAt',
                'pageSize': 5
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=10) as response:
                    if response.status == 200:
                        data = await response.json()
                        for article in data.get('articles', []):
                            results.append({
                                'title': article['title'],
                                'url': article['url'],
                                'source': f"NewsAPI: {article['source']['name']}",
                                'description': article.get('description', '')[:100] + '...',
                                'date': datetime.now().strftime("%Y-%m-%d"),
                                'timestamp': datetime.now().timestamp()
                            })
        except Exception as e:
            logger.warning(f"NewsAPI public error: {e}")
        
        return results
    
    def get_demo_news(self, query):
        """–î–µ–º–æ-–Ω–æ–≤–æ—Å—Ç–∏ –∫–æ–≥–¥–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã"""
        demo_news = [
            {
                'title': f'üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}',
                'url': f'https://www.google.com/search?q={urllib.parse.quote(query + " –†–æ—Å—Å–∏—è –Ω–æ–≤–æ—Å—Ç–∏")}',
                'source': '–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞',
                'description': '–ù–∞–∂–º–∏—Ç–µ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –≤ Google',
                'date': datetime.now().strftime("%Y-%m-%d"),
                'timestamp': datetime.now().timestamp()
            },
            {
                'title': f'üì∞ –ù–æ–≤–æ—Å—Ç–∏ –æ–± –≠–ü–† –∏ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã—Ö –ø–µ—Å–æ—á–Ω–∏—Ü–∞—Ö',
                'url': 'https://digital.gov.ru/ru/activity/directions/regulatory_sandbox/',
                'source': 'Digital.gov.ru',
                'description': '–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã—Ö –ø–µ—Å–æ—á–Ω–∏—Ü–∞—Ö –≤ –†–§',
                'date': datetime.now().strftime("%Y-%m-%d"),
                'timestamp': datetime.now().timestamp()
            },
            {
                'title': f'üí° –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–æ–≤—ã–µ —Ä–µ–∂–∏–º—ã –≤ –†–æ—Å—Å–∏–∏',
                'url': 'https://www.economy.gov.ru/material/directions/reguliruemyy_sandboks/',
                'source': '–ú–∏–Ω—ç–∫–æ–Ω–æ–º—Ä–∞–∑–≤–∏—Ç–∏—è',
                'description': '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ–≥—É–ª–∏—Ä—É–µ–º—ã—Ö —Å—ç–Ω–¥–±–æ–∫—Å–∞—Ö –¥–ª—è –±–∏–∑–Ω–µ—Å–∞',
                'date': datetime.now().strftime("%Y-%m-%d"),
                'timestamp': datetime.now().timestamp()
            }
        ]
        return demo_news

# –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä
async def test_smart_parser():
    parser = SmartNewsParser()
    print("üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º —É–º–Ω—ã–π –ø–∞—Ä—Å–µ—Ä...")
    
    results = await parser.search_news("—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞")
    print(f"–ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
    
    for i, item in enumerate(results, 1):
        print(f"{i}. {item['title']}")
        print(f"   URL: {item['url']}")
        print(f"   Source: {item['source']}")
        print()

if __name__ == "__main__":
    asyncio.run(test_smart_parser())
