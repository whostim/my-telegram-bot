import aiohttp
import asyncio
import ssl
import certifi
import feedparser
import logging
from datetime import datetime
import urllib.parse

logger = logging.getLogger(__name__)

class FixedNewsParser:
    def __init__(self):
        self.user_agent = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        
    async def search_news(self, query):
        """–ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –æ–±—Ö–æ–¥–æ–º SSL –ø—Ä–æ–±–ª–µ–º"""
        results = []
        
        # –ü–æ–∏—Å–∫ –≤ Google News (–æ–±—ã—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –ø—Ä–æ–±–ª–µ–º)
        google_results = await self.search_google_safe(query)
        results.extend(google_results)
        
        # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ NewsAPI (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç)
        newsapi_results = await self.search_newsapi(query)
        results.extend(newsapi_results)
        
        # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ RSS —Å –æ–±—Ö–æ–¥–æ–º SSL
        rss_results = await self.search_rss_safe(query)
        results.extend(rss_results)
        
        return self.remove_duplicates(results)
    
    async def search_google_safe(self, query):
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google News"""
        results = []
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º HTTPS –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö SSL –ø—Ä–æ–≤–µ—Ä–æ–∫
            search_url = f"https://news.google.com/rss/search?q={urllib.parse.quote(query)}+–†–æ—Å—Å–∏—è&hl=ru&gl=RU&ceid=RU:ru"
            
            # –°–æ–∑–¥–∞–µ–º SSL –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–æ—Ç–æ—Ä—ã–π –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –æ—à–∏–±–∫–∏ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE
            
            connector = aiohttp.TCPConnector(ssl=ssl_context)
            timeout = aiohttp.ClientTimeout(total=10)
            
            async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
                async with session.get(search_url, headers={'User-Agent': self.user_agent}) as response:
                    if response.status == 200:
                        content = await response.text()
                        feed = feedparser.parse(content)
                        
                        for entry in feed.entries[:8]:
                            results.append({
                                'title': entry.title,
                                'url': entry.link,
                                'source': f"Google News: {entry.get('source', {}).get('title', '')}",
                                'description': entry.get('description', '')[:150] + '...' if entry.get('description') else '–û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç',
                                'date': entry.get('published', datetime.now().strftime("%Y-%m-%d")),
                                'real': True
                            })
        except Exception as e:
            logger.error(f"Google News error: {e}")
        
        return results
    
    async def search_newsapi(self, query):
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ NewsAPI (–Ω—É–∂–µ–Ω API –∫–ª—é—á)"""
        results = []
        try:
            # NewsAPI —Ç—Ä–µ–±—É–µ—Ç —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏, –Ω–æ –µ—Å—Ç—å –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –ø–ª–∞–Ω
            api_key = "your_newsapi_key_here"  # –ù—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –Ω–∞ newsapi.org
            if api_key == "your_newsapi_key_here":
                return results  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –∫–ª—é—á –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
                
            url = f"https://newsapi.org/v2/everything?q={urllib.parse.quote(query)}&language=ru&sortBy=publishedAt&apiKey={api_key}"
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        data = await response.json()
                        for article in data.get('articles', [])[:5]:
                            results.append({
                                'title': article['title'],
                                'url': article['url'],
                                'source': f"NewsAPI: {article['source']['name']}",
                                'description': article.get('description', '')[:150] + '...',
                                'date': article['publishedAt'][:10],
                                'real': True
                            })
        except Exception as e:
            logger.warning(f"NewsAPI error: {e}")
        
        return results
    
    async def search_rss_safe(self, query):
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ RSS —Å –æ–±—Ö–æ–¥–æ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–∞–π—Ç–æ–≤"""
        results = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞–¥–µ–∂–Ω—ã–µ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        safe_rss_feeds = [
            'https://rss.news.google.com/rss?hl=ru&gl=RU&ceid=RU:ru',  # Google News RSS
            'https://feeds.bbci.co.uk/russian/rss.xml',  # BBC Russian
        ]
        
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        connector = aiohttp.TCPConnector(ssl=ssl_context)
        
        async with aiohttp.ClientSession(connector=connector) as session:
            for rss_url in safe_rss_feeds:
                try:
                    async with session.get(rss_url, headers={'User-Agent': self.user_agent}, timeout=8) as response:
                        if response.status == 200:
                            content = await response.text()
                            feed = feedparser.parse(content)
                            
                            for entry in feed.entries[:5]:
                                content_text = f"{entry.title} {entry.get('description', '')}".lower()
                                if any(keyword in content_text for keyword in ['—Ä–æ—Å—Å–∏—è', '–º–æ—Å–∫–≤–∞', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–∑–∞–∫–æ–Ω', '—Ä–µ–≥—É–ª—è—Ç–æ—Ä']):
                                    results.append({
                                        'title': entry.title,
                                        'url': entry.link,
                                        'source': f"RSS: {rss_url.split('/')[2]}",
                                        'description': entry.get('description', '')[:100] + '...',
                                        'date': entry.get('published', ''),
                                        'real': True
                                    })
                except Exception as e:
                    logger.warning(f"RSS error {rss_url}: {e}")
                    continue
        
        return results
    
    def remove_duplicates(self, results):
        seen = set()
        unique = []
        for item in results:
            if item['url'] not in seen:
                seen.add(item['url'])
                unique.append(item)
        return unique

# –¢–µ—Å—Ç –ø–∞—Ä—Å–µ—Ä–∞
async def test_fixed_parser():
    parser = FixedNewsParser()
    print("üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä...")
    
    results = await parser.search_news("–ø–µ—Å–æ—á–Ω–∏—Ü–∞ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è")
    print(f"–ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
    
    for i, item in enumerate(results[:3], 1):
        print(f"{i}. {item['title']}")
        print(f"   URL: {item['url']}")
        print(f"   Source: {item['source']}")
        print()

if __name__ == "__main__":
    asyncio.run(test_fixed_parser())
