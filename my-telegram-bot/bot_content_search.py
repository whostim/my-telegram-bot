import os
import logging
import asyncio
import aiohttp
from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
from aiogram.types import ReplyKeyboardMarkup, KeyboardButton
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import feedparser
import json
from datetime import datetime, timedelta
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

BOT_TOKEN = os.getenv('BOT_TOKEN')
if not BOT_TOKEN:
    logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω")
    exit(1)

bot = Bot(token=BOT_TOKEN)
dp = Dispatcher()

class ContentSearcher:
    def __init__(self):
        self.session = None
        
    async def get_session(self):
        if self.session is None:
            timeout = aiohttp.ClientTimeout(total=15)
            self.session = aiohttp.ClientSession(timeout=timeout)
        return self.session

    async def search_news_sites(self, query):
        """–ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –ø–æ —Ç–µ–º–µ –≠–ü–†"""
        sites = [
            {
                'name': '–†–ë–ö',
                'url': f'https://www.rbc.ru/v10/search/?query={query}&page=1',
                'selectors': {
                    'articles': '.search-item',
                    'title': '.search-item__title',
                    'link': '.search-item__link',
                    'description': '.search-item__text',
                    'date': '.search-item__date'
                }
            },
            {
                'name': '–ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç',
                'url': f'https://www.kommersant.ru/search/results?query={query}',
                'selectors': {
                    'articles': '.search_result_item',
                    'title': '.search_result_title a',
                    'link': '.search_result_title a',
                    'description': '.search_result_text',
                    'date': '.search_result_date'
                }
            },
            {
                'name': '–í–µ–¥–æ–º–æ—Å—Ç–∏',
                'url': f'https://www.vedomosti.ru/search?query={query}',
                'selectors': {
                    'articles': '.search-results__item',
                    'title': '.search-results__item-title a',
                    'link': '.search-results__item-title a',
                    'description': '.search-results__item-text',
                    'date': '.search-results__item-date'
                }
            }
        ]
        
        all_results = []
        
        for site in sites:
            try:
                session = await self.get_session()
                async with session.get(site['url'], headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        articles = soup.select(site['selectors']['articles'])[:3]
                        
                        for article in articles:
                            try:
                                title_elem = article.select_one(site['selectors']['title'])
                                link_elem = article.select_one(site['selectors']['link'])
                                desc_elem = article.select_one(site['selectors']['description'])
                                date_elem = article.select_one(site['selectors']['date'])
                                
                                if title_elem and link_elem:
                                    title = title_elem.get_text().strip()
                                    link = link_elem.get('href')
                                    
                                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
                                    if link and link.startswith('/'):
                                        if site['name'] == '–†–ë–ö':
                                            link = f"https://www.rbc.ru{link}"
                                        elif site['name'] == '–ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç':
                                            link = f"https://www.kommersant.ru{link}"
                                        elif site['name'] == '–í–µ–¥–æ–º–æ—Å—Ç–∏':
                                            link = f"https://www.vedomosti.ru{link}"
                                    
                                    description = desc_elem.get_text().strip() if desc_elem else ''
                                    date = date_elem.get_text().strip() if date_elem else ''
                                    
                                    all_results.append({
                                        'title': title,
                                        'url': link,
                                        'description': description[:200] + '...' if len(description) > 200 else description,
                                        'source': site['name'],
                                        'date': date
                                    })
                            except Exception as e:
                                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏: {e}")
                                continue
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {site['name']}: {e}")
                continue
                
        return all_results[:5]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–æ 5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    async def search_government_sites(self, query):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤"""
        gov_sites = [
            {
                'name': '–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –†–§',
                'url': 'http://government.ru/news/',
                'selectors': {
                    'articles': '.news_archive_item',
                    'title': 'h4 a',
                    'link': 'h4 a',
                    'date': '.news_date'
                }
            },
            {
                'name': '–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏',
                'url': 'https://www.cbr.ru/press/',
                'selectors': {
                    'articles': '.news_item',
                    'title': '.title a',
                    'link': '.title a',
                    'date': '.date'
                }
            }
        ]
        
        results = []
        query_lower = query.lower()
        
        for site in gov_sites:
            try:
                session = await self.get_session()
                async with session.get(site['url'], headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        articles = soup.select(site['selectors']['articles'])[:5]
                        
                        for article in articles:
                            title_elem = article.select_one(site['selectors']['title'])
                            link_elem = article.select_one(site['selectors']['link'])
                            date_elem = article.select_one(site['selectors']['date'])
                            
                            if title_elem and link_elem:
                                title = title_elem.get_text().strip()
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å—É
                                if any(word in title.lower() for word in query_lower.split()):
                                    link = link_elem.get('href')
                                    if link and link.startswith('/'):
                                        if site['name'] == '–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –†–§':
                                            link = f"http://government.ru{link}"
                                        elif site['name'] == '–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏':
                                            link = f"https://www.cbr.ru{link}"
                                    
                                    date = date_elem.get_text().strip() if date_elem else ''
                                    
                                    results.append({
                                        'title': title,
                                        'url': link,
                                        'description': f"–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å —Å–∞–π—Ç–∞ {site['name']}",
                                        'source': site['name'],
                                        'date': date
                                    })
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {site['name']}: {e}")
                continue
                
        return results

    async def search_telegram_content(self, query):
        """–ò–º–∏—Ç–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞ –≤ Telegram —á–µ—Ä–µ–∑ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∫–∞–Ω–∞–ª—ã"""
        # –í —Ä–µ–∞–ª—å–Ω–æ–º –±–æ—Ç–µ –∑–¥–µ—Å—å –±—É–¥–µ—Ç Telethon API
        telegram_channels = [
            {
                'name': '–†–æ—Å—Ñ–∏–Ω–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥',
                'username': 'rosfinmonitoring',
                'posts': [
                    "–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ –≠–ü–† –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–º —Å–µ–∫—Ç–æ—Ä–µ",
                    "–ù–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–æ–π –ø–µ—Å–æ—á–Ω–∏—Ü—ã –¥–ª—è —Ñ–∏–Ω—Ç–µ—Ö–∞",
                    "–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–µ—Ä–µ—á–Ω—è –≠–ü–† –Ω–∞ 2024 –≥–æ–¥"
                ]
            },
            {
                'name': '–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏',
                'username': 'centralbank_russia', 
                'posts': [
                    "–¶–ë —É—Ç–≤–µ—Ä–¥–∏–ª –Ω–æ–≤—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –¥–ª—è –≠–ü–†",
                    "–†–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞: –∏—Ç–æ–≥–∏ –≥–æ–¥–∞",
                    "–≠–ü–† –≤ —Ü–∏—Ñ—Ä–æ–≤–æ–π —ç–∫–æ–Ω–æ–º–∏–∫–µ - –Ω–æ–≤—ã–µ –ø–æ–¥—Ö–æ–¥—ã"
                ]
            },
            {
                'name': '–¶–∏—Ñ—Ä–æ–≤–∞—è —ç–∫–æ–Ω–æ–º–∏–∫–∞',
                'username': 'digital_economy',
                'posts': [
                    "–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–æ–≤—ã–µ —Ä–µ–∂–∏–º—ã: –æ–ø—ã—Ç —Ä–µ–≥–∏–æ–Ω–æ–≤",
                    "–≠–ü–† –∫–∞–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Ä–∞–∑–≤–∏—Ç–∏—è –∏–Ω–Ω–æ–≤–∞—Ü–∏–π",
                    "–ù–æ–≤—ã–µ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å–Ω—ã–µ –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤—ã –ø–æ –≠–ü–†"
                ]
            }
        ]
        
        results = []
        query_lower = query.lower()
        
        for channel in telegram_channels:
            for post in channel['posts']:
                if any(word in post.lower() for word in query_lower.split()):
                    results.append({
                        'title': post,
                        'url': f"https://t.me/{channel['username']}",
                        'description': f"–ü–æ—Å—Ç –∏–∑ –∫–∞–Ω–∞–ª–∞ {channel['name']}",
                        'source': f"Telegram: {channel['name']}",
                        'date': '–ù–µ–¥–∞–≤–Ω–æ'
                    })
                    
        return results[:3]

    async def get_epr_rss_news(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ RSS –ø–æ —Ç–µ–º–µ –≠–ü–†"""
        rss_feeds = [
            'https://www.rbc.ru/rss/technology.rss',
            'https://www.vedomosti.ru/rss/news.xml',
            'https://www.kommersant.ru/RSS/news.xml'
        ]
        
        results = []
        
        for feed_url in rss_feeds:
            try:
                session = await self.get_session()
                async with session.get(feed_url) as response:
                    if response.status == 200:
                        xml_content = await response.text()
                        feed = feedparser.parse(xml_content)
                        
                        for entry in feed.entries[:3]:
                            title = entry.title
                            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ç–µ–º–µ –≠–ü–†
                            if any(keyword in title.lower() for keyword in ['—ç–ø—Ä', '—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω', '–ø–µ—Å–æ—á–Ω–∏—Ü', '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω']):
                                results.append({
                                    'title': title,
                                    'url': entry.link,
                                    'description': entry.summary[:200] + '...' if entry.summary else '',
                                    'source': feed.feed.title,
                                    'date': entry.published if hasattr(entry, 'published') else ''
                                })
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ RSS {feed_url}: {e}")
                continue
                
        return results

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞
content_searcher = ContentSearcher()

# –ö–ª–∞–≤–∏–∞—Ç—É—Ä–∞
main_keyboard = ReplyKeyboardMarkup(
    keyboard=[
        [KeyboardButton(text="üîç –ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏"), KeyboardButton(text="üì¢ –ü–æ—Å—Ç—ã –≤ Telegram")],
        [KeyboardButton(text="‚ö° –ù–æ–≤–æ—Å—Ç–∏ –≠–ü–†"), KeyboardButton(text="üèõÔ∏è –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")]
    ],
    resize_keyboard=True
)

@dp.message(Command("start"))
async def cmd_start(message: types.Message):
    await message.answer(
        "üìö **–£–º–Ω—ã–π –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞**\n\n"
        "–Ø –Ω–∞—Ö–æ–∂—É —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∏ –ø–æ—Å—Ç—ã –ø–æ –≤–∞—à–∏–º –∑–∞–ø—Ä–æ—Å–∞–º:\n"
        "‚Ä¢ üîç –ù–æ–≤–æ—Å—Ç–∏ —Å RBC, –í–µ–¥–æ–º–æ—Å—Ç–µ–π, –ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç–∞\n"
        "‚Ä¢ üì¢ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø–æ—Å—Ç—ã –∏–∑ Telegram\n"
        "‚Ä¢ ‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ–± –≠–ü–†\n"
        "‚Ä¢ üèõÔ∏è –û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n\n"
        "–ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–Ω–æ–ø–∫–∏!",
        reply_markup=main_keyboard
    )

@dp.message(Command("help"))
async def cmd_help(message: types.Message):
    help_text = """
üìñ **–ü–æ–º–æ—â—å –ø–æ –ø–æ–∏—Å–∫—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞:**

üîç **–ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏** - –ø–æ–∏—Å–∫ –ø–æ –Ω–æ–≤–æ—Å—Ç–Ω—ã–º —Å–∞–π—Ç–∞–º
üì¢ **–ü–æ—Å—Ç—ã –≤ Telegram** - —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø–æ—Å—Ç—ã –∏–∑ –∫–∞–Ω–∞–ª–æ–≤
‚ö° **–ù–æ–≤–æ—Å—Ç–∏ –≠–ü–†** - —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Ç–µ–º–µ
üèõÔ∏è **–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏** - –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å –≥–æ—Å—Å–∞–π—Ç–æ–≤

üí° **–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:**
‚Ä¢ "–≠–ü–† –≤ —Ñ–∏–Ω–∞–Ω—Å–∞—Ö"
‚Ä¢ "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞"
‚Ä¢ "–Ω–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –≠–ü–†"
‚Ä¢ "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º 2024"
"""
    await message.answer(help_text)

@dp.message(lambda message: message.text == "üîç –ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏")
async def find_articles(message: types.Message):
    await message.answer("üîç –ù–∞–ø–∏—à–∏—Ç–µ —Ç–µ–º—É –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π –≤ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∏–∑–¥–∞–Ω–∏—è—Ö:")

@dp.message(lambda message: message.text == "üì¢ –ü–æ—Å—Ç—ã –≤ Telegram")
async def find_telegram_posts(message: types.Message):
    await message.answer("üì¢ –ù–∞–ø–∏—à–∏—Ç–µ —Ç–µ–º—É –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Å—Ç–æ–≤ –≤ Telegram-–∫–∞–Ω–∞–ª–∞—Ö:")

@dp.message(lambda message: message.text == "‚ö° –ù–æ–≤–æ—Å—Ç–∏ –≠–ü–†")
async def epr_news(message: types.Message):
    await message.answer("‚ö° –ò—â—É —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ–± –≠–ü–†...")
    
    try:
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        rss_news = await content_searcher.get_epr_rss_news()
        gov_news = await content_searcher.search_government_sites("–≠–ü–†")
        
        all_news = rss_news + gov_news
        
        if all_news:
            response = "‚ö° **–°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ–± –≠–ü–†:**\n\n"
            
            for i, news in enumerate(all_news[:6], 1):
                response += f"{i}. **{news['title']}**\n"
                response += f"   üì∞ {news['source']}\n"
                if news['date']:
                    response += f"   üìÖ {news['date']}\n"
                response += f"   üîó {news['url']}\n"
                if news['description']:
                    response += f"   üìù {news['description']}\n"
                response += "\n"
        else:
            response = "üòî –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏.\n\n"
            response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n‚Ä¢ –ù–∞–ø–∏—Å–∞—Ç—å –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å\n‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π\n‚Ä¢ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–∑–∂–µ"
            
        await message.answer(response)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –≠–ü–†: {e}")
        await message.answer("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –Ω–æ–≤–æ—Å—Ç–µ–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –∑–∞–ø—Ä–æ—Å.")

@dp.message(lambda message: message.text == "üèõÔ∏è –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
async def official_sources(message: types.Message):
    await message.answer("üèõÔ∏è –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö:")

@dp.message()
async def handle_text(message: types.Message):
    user_text = message.text.strip()
    
    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—ã –∏ –∫–Ω–æ–ø–∫–∏
    buttons = ["üîç –ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏", "üì¢ –ü–æ—Å—Ç—ã –≤ Telegram", "‚ö° –ù–æ–≤–æ—Å—Ç–∏ –≠–ü–†", "üèõÔ∏è –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏"]
    if user_text.startswith('/') or user_text in buttons:
        return
    
    await message.answer(f"üîç –ò—â—É –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{user_text}'...")
    
    try:
        response = f"üìö **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É '{user_text}':**\n\n"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø–æ–∏—Å–∫–∞ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
        if any(word in user_text.lower() for word in ['telegram', '—Ç–≥', '–∫–∞–Ω–∞–ª', '–ø–æ—Å—Ç']):
            # –ü–æ–∏—Å–∫ –≤ Telegram
            telegram_results = await content_searcher.search_telegram_content(user_text)
            
            if telegram_results:
                response += "üì¢ **–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø–æ—Å—Ç—ã –≤ Telegram:**\n\n"
                for i, post in enumerate(telegram_results, 1):
                    response += f"{i}. **{post['title']}**\n"
                    response += f"   üë§ {post['source']}\n"
                    response += f"   üîó {post['url']}\n\n"
            else:
                response += "üì¢ –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤ –≤ Telegram –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É.\n\n"
                
            # –í—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—å–∏ –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã
            news_results = await content_searcher.search_news_sites(user_text)
            if news_results:
                response += "üì∞ **–°—Ç–∞—Ç—å–∏ –ø–æ —Ç–µ–º–µ:**\n\n"
                for i, article in enumerate(news_results[:3], 1):
                    response += f"{i}. **{article['title']}**\n"
                    response += f"   üì∞ {article['source']}\n"
                    response += f"   üîó {article['url']}\n"
                    if article['description']:
                        response += f"   üìù {article['description']}\n"
                    response += "\n"
                    
        elif any(word in user_text.lower() for word in ['–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤', '–±–∞–Ω–∫ —Ä–æ—Å—Å–∏', '–æ—Ñ–∏—Ü–∏–∞–ª', '–∑–∞–∫–æ–Ω']):
            # –ü–æ–∏—Å–∫ –≤ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
            gov_results = await content_searcher.search_government_sites(user_text)
            
            if gov_results:
                response += "üèõÔ∏è **–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:**\n\n"
                for i, doc in enumerate(gov_results, 1):
                    response += f"{i}. **{doc['title']}**\n"
                    response += f"   üìã {doc['source']}\n"
                    if doc['date']:
                        response += f"   üìÖ {doc['date']}\n"
                    response += f"   üîó {doc['url']}\n\n"
            else:
                response += "üèõÔ∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.\n\n"
                
        else:
            # –û–±—â–∏–π –ø–æ–∏—Å–∫
            news_results = await content_searcher.search_news_sites(user_text)
            telegram_results = await content_searcher.search_telegram_content(user_text)
            
            if news_results:
                response += "üì∞ **–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏:**\n\n"
                for i, article in enumerate(news_results, 1):
                    response += f"{i}. **{article['title']}**\n"
                    response += f"   üì∞ {article['source']}\n"
                    if article['date']:
                        response += f"   üìÖ {article['date']}\n"
                    response += f"   üîó {article['url']}\n"
                    if article['description']:
                        response += f"   üìù {article['description']}\n"
                    response += "\n"
                    
            if telegram_results:
                response += "üì¢ **–ü–æ—Å—Ç—ã –≤ Telegram:**\n\n"
                for i, post in enumerate(telegram_results, 1):
                    response += f"{i}. **{post['title']}**\n"
                    response += f"   üë§ {post['source']}\n"
                    response += f"   üîó {post['url']}\n\n"
                    
        if "üì∞" not in response and "üì¢" not in response and "üèõÔ∏è" not in response:
            response += "üòî –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É.\n\n"
            response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n‚Ä¢ –ò–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É\n‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫\n‚Ä¢ –£–∫–∞–∑–∞—Ç—å –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Ç–µ–º—É"
        
        # –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ, —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏
        if len(response) > 4000:
            parts = [response[i:i+4000] for i in range(0, len(response), 4000)]
            for part in parts:
                await message.answer(part)
        else:
            await message.answer(response)
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        await message.answer(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–∑–∂–µ.")

async def main():
    logger.info("–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
    await bot.delete_webhook(drop_pending_updates=True)
    await dp.start_polling(bot)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
