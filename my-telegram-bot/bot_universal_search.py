import os
import logging
import asyncio
import aiohttp
from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
from aiogram.types import ReplyKeyboardMarkup, KeyboardButton, InlineKeyboardMarkup, InlineKeyboardButton
from dotenv import load_dotenv
import urllib.parse
from bs4 import BeautifulSoup
import json

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

BOT_TOKEN = os.getenv('BOT_TOKEN')
if not BOT_TOKEN:
    logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
    exit(1)

bot = Bot(token=BOT_TOKEN)
dp = Dispatcher()

# –ö–ª–∞–≤–∏–∞—Ç—É—Ä–∞
main_keyboard = ReplyKeyboardMarkup(
    keyboard=[
        [KeyboardButton(text="üîç –ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"), KeyboardButton(text="üì¢ –ü–æ–∏—Å–∫ –≤ Telegram")],
        [KeyboardButton(text="‚ö° –ù–æ–≤–æ—Å—Ç–∏ –≠–ü–†"), KeyboardButton(text="üÜò –ü–æ–º–æ—â—å")]
    ],
    resize_keyboard=True
)

class InternetSearcher:
    def __init__(self):
        self.session = None
        
    async def get_session(self):
        if self.session is None:
            self.session = aiohttp.ClientSession()
        return self.session
        
    async def search_google(self, query, num_results=5):
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä—Å–∏–Ω–≥)"""
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://www.google.com/search?q={encoded_query}&num={num_results}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    results = []
                    # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
                    for g in soup.find_all('div', class_='g')[:num_results]:
                        link = g.find('a')
                        if link and link.get('href'):
                            title = g.find('h3')
                            snippet = g.find('span', class_='aCOpRe')
                            
                            if title:
                                result = {
                                    'title': title.get_text(),
                                    'url': link.get('href'),
                                    'snippet': snippet.get_text() if snippet else ''
                                }
                                # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–∞—Å—Ç–æ—è—â–∏–µ URL (–Ω–µ —Ä–µ–∫–ª–∞–º–∞)
                                if result['url'].startswith('/url?q='):
                                    result['url'] = result['url'].split('/url?q=')[1].split('&')[0]
                                    results.append(result)
                    
                    return results
                else:
                    return []
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Google –ø–æ–∏—Å–∫–∞: {e}")
            return []
            
    async def search_news(self, query, num_results=5):
        """–ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://news.google.com/search?q={encoded_query}&hl=ru-RU&gl=RU&ceid=RU:ru"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    results = []
                    # –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏
                    articles = soup.find_all('article')[:num_results]
                    
                    for article in articles:
                        link = article.find('a', href=True)
                        title = article.find('h3') or article.find('h4')
                        
                        if link and title:
                            result = {
                                'title': title.get_text().strip(),
                                'url': f"https://news.google.com{link['href']}",
                                'snippet': '–ù–æ–≤–æ—Å—Ç—å –æ—Ç Google News'
                            }
                            results.append(result)
                    
                    return results
                else:
                    return []
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []
            
    async def search_yandex_news(self, query, num_results=5):
        """–ü–æ–∏—Å–∫ –≤ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç—è—Ö"""
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://yandex.ru/news/search?text={encoded_query}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    results = []
                    # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
                    news_cards = soup.find_all('div', class_='mg-card')[:num_results]
                    
                    for card in news_cards:
                        title_elem = card.find('h2') or card.find('a', class_='mg-card__link')
                        source_elem = card.find('span', class_='mg-card-source__source')
                        
                        if title_elem:
                            result = {
                                'title': title_elem.get_text().strip(),
                                'url': title_elem.get('href') if title_elem.get('href') else f"https://yandex.ru/news/search?text={encoded_query}",
                                'snippet': source_elem.get_text() if source_elem else '–Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–∏'
                            }
                            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π URL –µ—Å–ª–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π
                            if result['url'].startswith('/'):
                                result['url'] = f"https://yandex.ru{result['url']}"
                            results.append(result)
                    
                    return results
                else:
                    return []
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []

class TelegramSearcher:
    async def search_telegram_channels(self, query):
        """–ü–æ–∏—Å–∫ –ø–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–º –∫–∞–Ω–∞–ª–∞–º –æ–± –≠–ü–†"""
        # –°–ø–∏—Å–æ–∫ –∫–∞–Ω–∞–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –≠–ü–†
        channels = [
            {"name": "üìä –†–æ—Å—Ñ–∏–Ω–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥", "url": "https://t.me/rosfinmonitoring", "topics": ["–≠–ü–†", "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞"]},
            {"name": "üöÄ –ò–Ω–Ω–æ–≤–∞—Ü–∏–∏ –≤ —Ñ–∏–Ω–∞–Ω—Å–∞—Ö", "url": "https://t.me/fintech_ru", "topics": ["–≠–ü–†", "—Ñ–∏–Ω—Ç–µ—Ö"]},
            {"name": "üè¶ –ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏", "url": "https://t.me/centralbank_russia", "topics": ["—Ä–µ–≥—É–ª—è—Ç–æ—Ä–∏–∫–∞", "–≠–ü–†"]},
            {"name": "üí° –¶–∏—Ñ—Ä–æ–≤–∞—è —ç–∫–æ–Ω–æ–º–∏–∫–∞", "url": "https://t.me/digital_economy", "topics": ["–≠–ü–†", "–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏"]},
        ]
        
        results = []
        query_lower = query.lower()
        
        for channel in channels:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∑–∞–ø—Ä–æ—Å—É –ø–æ —Ç–µ–º–∞–º
            if any(topic.lower() in query_lower for topic in channel['topics']):
                results.append({
                    'title': channel['name'],
                    'url': channel['url'],
                    'snippet': f"–ö–∞–Ω–∞–ª –ø–æ —Ç–µ–º–µ: {', '.join(channel['topics'])}"
                })
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—â–∏–µ –∫–∞–Ω–∞–ª—ã
        if not results:
            for channel in channels[:2]:
                results.append({
                    'title': channel['name'],
                    'url': channel['url'],
                    'snippet': "–í–æ–∑–º–æ–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É"
                })
                
        return results

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–∏–∫–æ–≤
internet_searcher = InternetSearcher()
telegram_searcher = TelegramSearcher()

@dp.message(Command("start"))
async def cmd_start(message: types.Message):
    await message.answer(
        "üåç –£–º–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –±–æ—Ç\n\n"
        "–Ø –º–æ–≥—É:\n"
        "‚Ä¢ üîç –ò—Å–∫–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ\n"
        "‚Ä¢ üì¢ –ù–∞—Ö–æ–¥–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ Telegram-–∫–∞–Ω–∞–ª—ã\n"
        "‚Ä¢ ‚ö° –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ–± –≠–ü–†\n\n"
        "–ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ —á—Ç–æ –∏—â–µ—Ç–µ!",
        reply_markup=main_keyboard
    )

@dp.message(Command("help"))
async def cmd_help(message: types.Message):
    help_text = """
üÜò –ü–æ–º–æ—â—å –ø–æ –±–æ—Ç—É:

üîç **–ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ** - –Ω–∞–π–¥–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∏ –Ω–æ–≤–æ—Å—Ç–∏
üì¢ **–ü–æ–∏—Å–∫ –≤ Telegram** - –Ω–∞–π–¥–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–∞–Ω–∞–ª—ã
‚ö° **–ù–æ–≤–æ—Å—Ç–∏ –≠–ü–†** - —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ–± —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–æ–≤—ã—Ö —Ä–µ–∂–∏–º–∞—Ö

üí° **–ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ –ª—é–±–æ–π –∑–∞–ø—Ä–æ—Å** - —è —Å–∞–º –æ–ø—Ä–µ–¥–µ–ª—é –≥–¥–µ –ª—É—á—à–µ –∏—Å–∫–∞—Ç—å!

–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:
‚Ä¢ "–≠–ü–† –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–º —Å–µ–∫—Ç–æ—Ä–µ"
‚Ä¢ "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞ 2024"
‚Ä¢ "–ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ–± –≠–ü–†"
"""
    await message.answer(help_text)

@dp.message(lambda message: message.text == "üîç –ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ")
async def search_internet_menu(message: types.Message):
    await message.answer("üîç –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ. –Ø –Ω–∞–π–¥—É —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∏ –Ω–æ–≤–æ—Å—Ç–∏!")

@dp.message(lambda message: message.text == "üì¢ –ü–æ–∏—Å–∫ –≤ Telegram")
async def search_telegram_menu(message: types.Message):
    await message.answer("üì¢ –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ Telegram. –Ø –Ω–∞–π–¥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–∞–Ω–∞–ª—ã!")

@dp.message(lambda message: message.text == "‚ö° –ù–æ–≤–æ—Å—Ç–∏ –≠–ü–†")
async def epr_news(message: types.Message):
    await message.answer("üîç –ò—â—É —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ–± –≠–ü–†...")
    
    try:
        # –ü–æ–∏—Å–∫ –≤ Google News
        news_results = await internet_searcher.search_news("–≠–ü–† —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º –†–æ—Å—Å–∏—è", 5)
        yandex_news = await internet_searcher.search_yandex_news("–≠–ü–†", 3)
        
        response = "‚ö° **–ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ–± –≠–ü–†:**\n\n"
        
        if news_results or yandex_news:
            all_news = news_results + yandex_news
            seen_titles = set()
            
            for i, news in enumerate(all_news[:6], 1):
                if news['title'] not in seen_titles:
                    seen_titles.add(news['title'])
                    response += f"{i}. **{news['title']}**\n"
                    response += f"   üìù {news['snippet']}\n"
                    response += f"   üîó {news['url']}\n\n"
        else:
            response += "üòî –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏.\n"
            response += "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∏—Å–∫–∞—Ç—å –≤—Ä—É—á–Ω—É—é:\n"
            response += "‚Ä¢ https://news.google.com/search?q=–≠–ü–†+–†–æ—Å—Å–∏—è\n"
            response += "‚Ä¢ https://yandex.ru/news/search?text=–≠–ü–†\n"
            
        await message.answer(response)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –≠–ü–†: {e}")
        await message.answer("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –Ω–æ–≤–æ—Å—Ç–µ–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")

@dp.message(lambda message: message.text == "üÜò –ü–æ–º–æ—â—å")
async def help_button(message: types.Message):
    await cmd_help(message)

@dp.message()
async def handle_text(message: types.Message):
    user_text = message.text.strip()
    
    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—ã –∏ –∫–Ω–æ–ø–∫–∏
    if user_text.startswith('/') or user_text in ["üîç –ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ", "üì¢ –ü–æ–∏—Å–∫ –≤ Telegram", "‚ö° –ù–æ–≤–æ—Å—Ç–∏ –≠–ü–†", "üÜò –ü–æ–º–æ—â—å"]:
        return
    
    await message.answer(f"üîç –ò—â—É: '{user_text}'...")
    
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
        if any(word in user_text.lower() for word in ['telegram', '—Ç–≥', '–∫–∞–Ω–∞–ª', '—á–∞—Ç']):
            # –ü–æ–∏—Å–∫ –≤ Telegram
            telegram_results = await telegram_searcher.search_telegram_channels(user_text)
            
            response = f"üì¢ **–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ Telegram-–∫–∞–Ω–∞–ª—ã –¥–ª—è '{user_text}':**\n\n"
            
            if telegram_results:
                for i, result in enumerate(telegram_results, 1):
                    response += f"{i}. **{result['title']}**\n"
                    response += f"   üìù {result['snippet']}\n"
                    response += f"   üîó {result['url']}\n\n"
            else:
                response += "üòî –ù–µ –Ω–∞—à–µ–ª –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –∫–∞–Ω–∞–ª–æ–≤.\n"
                response += f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∏—Å–∫–∞—Ç—å –≤—Ä—É—á–Ω—É—é: https://t.me/search?q={urllib.parse.quote(user_text)}"
                
        else:
            # –ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
            google_results = await internet_searcher.search_google(user_text, 4)
            news_results = await internet_searcher.search_news(user_text, 3)
            
            response = f"üåê **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è '{user_text}':**\n\n"
            
            if google_results or news_results:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                all_results = google_results + news_results
                seen_urls = set()
                
                for i, result in enumerate(all_results[:5], 1):
                    if result['url'] not in seen_urls:
                        seen_urls.add(result['url'])
                        response += f"{i}. **{result['title']}**\n"
                        if result['snippet']:
                            snippet = result['snippet'][:100] + "..." if len(result['snippet']) > 100 else result['snippet']
                            response += f"   üìù {snippet}\n"
                        response += f"   üîó {result['url']}\n\n"
            else:
                response += "üòî –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.\n"
                response += f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∏—Å–∫–∞—Ç—å –≤—Ä—É—á–Ω—É—é:\n"
                response += f"‚Ä¢ https://www.google.com/search?q={urllib.parse.quote(user_text)}\n"
                response += f"‚Ä¢ https://news.google.com/search?q={urllib.parse.quote(user_text)}\n"
        
        await message.answer(response)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        await message.answer(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞:\n"
                           f"‚Ä¢ Google: https://www.google.com/search?q={urllib.parse.quote(user_text)}\n"
                           f"‚Ä¢ Telegram: https://t.me/search?q={urllib.parse.quote(user_text)}")

async def main():
    logger.info("–ë–æ—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    await bot.delete_webhook(drop_pending_updates=True)
    await dp.start_polling(bot)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
