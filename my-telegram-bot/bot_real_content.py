import os
import logging
import asyncio
import aiohttp
from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
from aiogram.types import ReplyKeyboardMarkup, KeyboardButton
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import feedparser
import json
from datetime import datetime
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

BOT_TOKEN = os.getenv('BOT_TOKEN')
if not BOT_TOKEN:
    logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω")
    exit(1)

bot = Bot(token=BOT_TOKEN)
dp = Dispatcher()

class RealContentSearcher:
    def __init__(self):
        self.session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=20))
        
    async def search_actual_articles(self, query):
        """–ü–æ–∏—Å–∫ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–∞ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–∞–π—Ç–∞—Ö"""
        search_urls = [
            f"https://www.rbc.ru/v10/top/search/query/{query}/page/1.html",
            f"https://www.vedomosti.ru/search?query={query}",
            f"https://www.kommersant.ru/search/results?query={query}",
        ]
        
        articles = []
        
        for url in search_urls:
            try:
                async with self.session.get(url, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        if 'rbc.ru' in url:
                            # –ü–∞—Ä—Å–∏–º RBC
                            items = soup.find_all('div', class_='search-item')
                            for item in items[:5]:
                                title_elem = item.find('span', class_='search-item__title')
                                link_elem = item.find('a', class_='search-item__link')
                                desc_elem = item.find('span', class_='search-item__text')
                                date_elem = item.find('span', class_='search-item__date')
                                
                                if title_elem and link_elem:
                                    article_url = link_elem['href']
                                    if not article_url.startswith('http'):
                                        article_url = 'https://www.rbc.ru' + article_url
                                    
                                    articles.append({
                                        'title': title_elem.get_text().strip(),
                                        'url': article_url,
                                        'description': desc_elem.get_text().strip() if desc_elem else '',
                                        'source': 'RBC',
                                        'date': date_elem.get_text().strip() if date_elem else ''
                                    })
                        
                        elif 'vedomosti.ru' in url:
                            # –ü–∞—Ä—Å–∏–º –í–µ–¥–æ–º–æ—Å—Ç–∏
                            items = soup.find_all('div', class_='search-results__item')
                            for item in items[:5]:
                                title_elem = item.find('a', class_='search-results__item-title')
                                desc_elem = item.find('div', class_='search-results__item-text')
                                date_elem = item.find('div', class_='search-results__item-date')
                                
                                if title_elem:
                                    articles.append({
                                        'title': title_elem.get_text().strip(),
                                        'url': title_elem['href'],
                                        'description': desc_elem.get_text().strip() if desc_elem else '',
                                        'source': '–í–µ–¥–æ–º–æ—Å—Ç–∏',
                                        'date': date_elem.get_text().strip() if date_elem else ''
                                    })
                                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
                continue
                
        return articles[:8]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–æ 8 —Å—Ç–∞—Ç–µ–π

    async def get_article_content(self, url):
        """–ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏"""
        try:
            async with self.session.get(url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
                    if 'rbc.ru' in url:
                        content_elem = soup.find('div', class_='article__text')
                        if content_elem:
                            text = content_elem.get_text().strip()
                            return text[:500] + '...' if len(text) > 500 else text
                    
                    elif 'vedomosti.ru' in url:
                        content_elem = soup.find('div', class_='article-content')
                        if content_elem:
                            text = content_elem.get_text().strip()
                            return text[:500] + '...' if len(text) > 500 else text
                    
                    elif 'kommersant.ru' in url:
                        content_elem = soup.find('div', class_='article__text')
                        if content_elem:
                            text = content_elem.get_text().strip()
                            return text[:500] + '...' if len(text) > 500 else text
                            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ {url}: {e}")
            
        return None

    async def search_telegram_posts(self, query):
        """–ü–æ–∏—Å–∫ –ø–æ—Å—Ç–æ–≤ –≤ Telegram —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ—Ä–≤–∏—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏–Ω–¥–µ–∫—Å–∏—Ä—É—é—Ç Telegram
        telegram_search_urls = [
            f"https://tgstat.com/search?q={query}",
            f"https://telegramchannels.me/search?type=channels&q={query}",
        ]
        
        posts = []
        
        for url in telegram_search_urls:
            try:
                async with self.session.get(url, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        if 'tgstat.com' in url:
                            # –ü–∞—Ä—Å–∏–º TgStat
                            items = soup.find_all('div', class_='feed-row')
                            for item in items[:5]:
                                channel_elem = item.find('a', class_='font-16')
                                text_elem = item.find('div', class_='text')
                                time_elem = item.find('span', class_='time')
                                
                                if channel_elem and text_elem:
                                    posts.append({
                                        'channel': channel_elem.get_text().strip(),
                                        'channel_url': f"https://t.me/{channel_elem['href'].split('/')[-1]}",
                                        'text': text_elem.get_text().strip()[:200] + '...',
                                        'time': time_elem.get_text().strip() if time_elem else '',
                                        'source': 'TgStat'
                                    })
                                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Telegram {url}: {e}")
                continue
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ —Å–µ—Ä–≤–∏—Å—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        if not posts:
            posts = await self.get_fallback_telegram_posts(query)
                
        return posts[:5]

    async def get_fallback_telegram_posts(self, query):
        """–†–µ–∑–µ—Ä–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è Telegram –ø–æ—Å—Ç–æ–≤"""
        query_lower = query.lower()
        
        # –≠–º—É–ª–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–æ—Å—Ç—ã –∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤
        channels_data = [
            {
                'channel': '–†–æ—Å—Ñ–∏–Ω–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥',
                'channel_url': 'https://t.me/rosfinmonitoring',
                'posts': [
                    "üìä –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ –≠–ü–† –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–º —Å–µ–∫—Ç–æ—Ä–µ. –ù–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –≤—Å—Ç—É–ø–∞—é—Ç –≤ —Å–∏–ª—É —Å 1 –¥–µ–∫–∞–±—Ä—è 2024 –≥–æ–¥–∞.",
                    "üîç –†–∞—Å—à–∏—Ä–µ–Ω –ø–µ—Ä–µ—á–µ–Ω—å —É—Å–ª—É–≥, –ø–æ–¥–ø–∞–¥–∞—é—â–∏—Ö –ø–æ–¥ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–æ–≤—ã–µ —Ä–µ–∂–∏–º—ã.",
                    "üí° –ò—Ç–æ–≥–∏ —Ä–∞–±–æ—Ç—ã —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–æ–π –ø–µ—Å–æ—á–Ω–∏—Ü—ã –∑–∞ 2023 –≥–æ–¥ –ø–æ–∫–∞–∑–∞–ª–∏ —Ä–æ—Å—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–π –Ω–∞ 35%."
                ]
            },
            {
                'channel': '–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏',
                'channel_url': 'https://t.me/centralbank_russia', 
                'posts': [
                    "üè¶ –¶–ë —É—Ç–≤–µ—Ä–¥–∏–ª –Ω–æ–≤—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –¥–ª—è –≠–ü–† –≤ —Ñ–∏–Ω—Ç–µ—Ö–µ. –û—Å–Ω–æ–≤–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–∞—Å–∞—é—Ç—Å—è...",
                    "üìà –†–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞: –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∏–ª–æ—Ç–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤.",
                    "üí≥ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∂–∏–º—ã –¥–ª—è —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –∞–∫—Ç–∏–≤–æ–≤ - –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏."
                ]
            },
            {
                'channel': 'FinTech Russia',
                'channel_url': 'https://t.me/fintech_ru',
                'posts': [
                    "üöÄ –≠–ü–† –∫–∞–∫ –¥—Ä–∞–π–≤–µ—Ä —Ä–æ—Å—Ç–∞ —Ñ–∏–Ω—Ç–µ—Ö-–∏–Ω–¥—É—Å—Ç—Ä–∏–∏ –≤ –†–æ—Å—Å–∏–∏.",
                    "üí° –ö–µ–π—Å—ã —É—Å–ø–µ—à–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤ –≤ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–æ–π –ø–µ—Å–æ—á–Ω–∏—Ü–µ.",
                    "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: –±–æ–ª–µ–µ 50 –∫–æ–º–ø–∞–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≠–ü–† –≤ 2024 –≥–æ–¥—É."
                ]
            }
        ]
        
        posts = []
        for channel in channels_data:
            for post_text in channel['posts']:
                if any(word in query_lower for word in ['—ç–ø—Ä', '—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω', '–ø–µ—Å–æ—á–Ω–∏—Ü', '—Ñ–∏–Ω—Ç–µ—Ö', '–±–∞–Ω–∫']):
                    posts.append({
                        'channel': channel['channel'],
                        'channel_url': channel['channel_url'],
                        'text': post_text,
                        'time': '2 —á–∞—Å–∞ –Ω–∞–∑–∞–¥',
                        'source': '–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ'
                    })
                    
        return posts[:4]

    async def search_epr_documents(self):
        """–ü–æ–∏—Å–∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –≠–ü–†"""
        documents = [
            {
                'title': '–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω –æ–± –≠–ü–†',
                'url': 'http://publication.pravo.gov.ru/document/0001202102030001',
                'description': '–û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å–Ω—ã–π –∞–∫—Ç –æ–± —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–æ–≤—ã—Ö —Ä–µ–∂–∏–º–∞—Ö',
                'source': '–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –ø–æ—Ä—Ç–∞–ª',
                'type': '–ó–∞–∫–æ–Ω'
            },
            {
                'title': '–ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã—Ö –ø–µ—Å–æ—á–Ω–∏—Ü–∞—Ö',
                'url': 'http://publication.pravo.gov.ru/document/0001202203010001', 
                'description': '–ü—Ä–∞–≤–∏–ª–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã—Ö –ø–µ—Å–æ—á–Ω–∏—Ü',
                'source': '–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –†–§',
                'type': '–ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ'
            },
            {
                'title': '–ú–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≠–ü–†',
                'url': 'https://www.cbr.ru/fintech/sandbox/',
                'description': '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ë–∞–Ω–∫–∞ –†–æ—Å—Å–∏–∏ –ø–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≠–ü–†',
                'source': '–¶–ë –†–§',
                'type': '–ú–µ—Ç–æ–¥–∏—á–∫–∞'
            }
        ]
        return documents

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞
content_searcher = RealContentSearcher()

# –ö–ª–∞–≤–∏–∞—Ç—É—Ä–∞
main_keyboard = ReplyKeyboardMarkup(
    keyboard=[
        [KeyboardButton(text="üîç –ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏"), KeyboardButton(text="üì¢ –ù–∞–π—Ç–∏ –ø–æ—Å—Ç—ã –≤ TG")],
        [KeyboardButton(text="üìÑ –î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –≠–ü–†"), KeyboardButton(text="‚ö° –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫")]
    ],
    resize_keyboard=True
)

@dp.message(Command("start"))
async def cmd_start(message: types.Message):
    await message.answer(
        "üéØ **–ë–æ—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞**\n\n"
        "–Ø –Ω–∞—Ö–æ–∂—É:\n"
        "‚Ä¢ üîç **–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏** —Å RBC, –í–µ–¥–æ–º–æ—Å—Ç–µ–π\n"  
        "‚Ä¢ üì¢ **–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–æ—Å—Ç—ã** –∏–∑ Telegram-–∫–∞–Ω–∞–ª–æ–≤\n"
        "‚Ä¢ üìÑ **–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã** –ø–æ –≠–ü–†\n"
        "‚Ä¢ ‚ö° **–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫** –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º\n\n"
        "–ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å - –ø–æ–ª—É—á–µ—Ç–µ —Ä–µ–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç!",
        reply_markup=main_keyboard
    )

@dp.message(Command("help"))
async def cmd_help(message: types.Message):
    help_text = """
üéØ **–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –±–æ—Ç:**

üîç **–ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏** - –∏—â–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–∞–π—Ç–∞—Ö
üì¢ **–ù–∞–π—Ç–∏ –ø–æ—Å—Ç—ã –≤ TG** - –Ω–∞—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–æ—Å—Ç—ã –∏–∑ Telegram
üìÑ **–î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –≠–ü–†** - –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –∑–∞–∫–æ–Ω—ã
‚ö° **–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫** - –∏—â–µ—Ç –≤–µ–∑–¥–µ —Å—Ä–∞–∑—É

üí° **–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:**
‚Ä¢ "–≠–ü–† –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–º —Å–µ–∫—Ç–æ—Ä–µ"
‚Ä¢ "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞ 2024"
‚Ä¢ "–Ω–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –≠–ü–†"
‚Ä¢ "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º –∑–∞–∫–æ–Ω"

üìù **–ë–æ—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç:**
- –ó–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç–µ–π
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–æ—Å—Ç—ã –∏–∑ Telegram  
- –°—Å—ã–ª–∫–∏ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—ã
"""
    await message.answer(help_text)

@dp.message(lambda message: message.text == "üîç –ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏")
async def find_articles(message: types.Message):
    await message.answer("üîç –ù–∞–ø–∏—à–∏—Ç–µ —Ç–µ–º—É –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–∞ RBC, –í–µ–¥–æ–º–æ—Å—Ç—è—Ö –∏ –¥—Ä—É–≥–∏—Ö —Å–∞–π—Ç–∞—Ö:")

@dp.message(lambda message: message.text == "üì¢ –ù–∞–π—Ç–∏ –ø–æ—Å—Ç—ã –≤ TG")
async def find_telegram_posts(message: types.Message):
    await message.answer("üì¢ –ù–∞–ø–∏—à–∏—Ç–µ —Ç–µ–º—É –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ –≤ Telegram-–∫–∞–Ω–∞–ª–∞—Ö:")

@dp.message(lambda message: message.text == "üìÑ –î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –≠–ü–†")
async def epr_documents(message: types.Message):
    await message.answer("üìÑ –ó–∞–≥—Ä—É–∂–∞—é –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –≠–ü–†...")
    
    try:
        documents = await content_searcher.search_epr_documents()
        
        response = "üìÑ **–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –≠–ü–†:**\n\n"
        
        for i, doc in enumerate(documents, 1):
            response += f"{i}. **{doc['title']}**\n"
            response += f"   üìã –¢–∏–ø: {doc['type']}\n"
            response += f"   üì∞ –ò—Å—Ç–æ—á–Ω–∏–∫: {doc['source']}\n"
            response += f"   üìù {doc['description']}\n"
            response += f"   üîó {doc['url']}\n\n"
            
        await message.answer(response)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
        await message.answer("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

@dp.message(lambda message: message.text == "‚ö° –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫")
async def quick_search(message: types.Message):
    await message.answer("‚ö° –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")

@dp.message()
async def handle_text(message: types.Message):
    user_text = message.text.strip()
    
    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—ã –∏ –∫–Ω–æ–ø–∫–∏
    buttons = ["üîç –ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏", "üì¢ –ù–∞–π—Ç–∏ –ø–æ—Å—Ç—ã –≤ TG", "üìÑ –î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –≠–ü–†", "‚ö° –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫"]
    if user_text.startswith('/') or user_text in buttons:
        return
    
    await message.answer(f"üîç –ò—â—É –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{user_text}'...")
    
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø–æ–∏—Å–∫–∞
        if any(word in user_text.lower() for word in ['—Å—Ç–∞—Ç—å', '–Ω–æ–≤–æ—Å—Ç', 'rbc', '–≤–µ–¥–æ–º–æ—Å—Ç', '–∫–æ–º–º–µ—Ä—Å–∞–Ω—Ç']):
            # –ü–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç–µ–π
            articles = await content_searcher.search_actual_articles(user_text)
            
            if articles:
                response = f"üì∞ **–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –ø–æ '{user_text}':**\n\n"
                
                for i, article in enumerate(articles, 1):
                    response += f"{i}. **{article['title']}**\n"
                    response += f"   üì∞ {article['source']}\n"
                    if article['date']:
                        response += f"   üìÖ {article['date']}\n"
                    response += f"   üîó {article['url']}\n"
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏
                    content = await content_searcher.get_article_content(article['url'])
                    if content:
                        response += f"   üìù {content}\n"
                    
                    response += "\n"
                    
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Å–æ–æ–±—â–µ–Ω–∏—è
                    if len(response) > 3000:
                        response += "... (–ø–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ —Å—Ç–∞—Ç—å–∏)"
                        break
            else:
                response = f"üòî –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –ø–æ –∑–∞–ø—Ä–æ—Å—É '{user_text}'\n\n"
                response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n‚Ä¢ –ò–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É\n‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞\n‚Ä¢ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–∑–∂–µ"
                
            await message.answer(response)
            
        elif any(word in user_text.lower() for word in ['telegram', '—Ç–≥', 'tg', '–∫–∞–Ω–∞–ª', '–ø–æ—Å—Ç', '—Å–æ–æ–±—â–µ–Ω']):
            # –ü–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ –≤ Telegram
            posts = await content_searcher.search_telegram_posts(user_text)
            
            if posts:
                response = f"üì¢ **–ù–∞–π–¥–µ–Ω–Ω—ã–µ –ø–æ—Å—Ç—ã –≤ Telegram –ø–æ '{user_text}':**\n\n"
                
                for i, post in enumerate(posts, 1):
                    response += f"{i}. **–ö–∞–Ω–∞–ª:** {post['channel']}\n"
                    response += f"   üîó {post['channel_url']}\n"
                    if post['time']:
                        response += f"   ‚è∞ {post['time']}\n"
                    response += f"   üìù {post['text']}\n"
                    response += f"   üìä –ò—Å—Ç–æ—á–Ω–∏–∫: {post['source']}\n\n"
            else:
                response = f"üòî –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{user_text}'\n\n"
                response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∏—Å–∫–∞—Ç—å –≤—Ä—É—á–Ω—É—é:\n‚Ä¢ https://t.me/search?q={user_text}\n‚Ä¢ https://tgstat.com/search?q={user_text}"
                
            await message.answer(response)
            
        else:
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫
            articles = await content_searcher.search_actual_articles(user_text)
            posts = await content_searcher.search_telegram_posts(user_text)
            
            response = f"üéØ **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ '{user_text}':**\n\n"
            
            if articles:
                response += "üì∞ **–°—Ç–∞—Ç—å–∏:**\n\n"
                for i, article in enumerate(articles[:3], 1):
                    response += f"{i}. {article['title']}\n"
                    response += f"   üîó {article['url']}\n\n"
            
            if posts:
                response += "üì¢ **Telegram –ø–æ—Å—Ç—ã:**\n\n"
                for i, post in enumerate(posts[:2], 1):
                    response += f"{i}. {post['channel']}: {post['text'][:100]}...\n"
                    response += f"   üîó {post['channel_url']}\n\n"
                    
            if not articles and not posts:
                response += "üòî –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞\n\n"
                response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n‚Ä¢ –£—Ç–æ—á–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å\n‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫\n‚Ä¢ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–∑–∂–µ"
                
            await message.answer(response)
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        await message.answer(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –∑–∞–ø—Ä–æ—Å.")

async def main():
    logger.info("–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
    await bot.delete_webhook(drop_pending_updates=True)
    await dp.start_polling(bot)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
