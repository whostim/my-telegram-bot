from datetime import datetime, timedelta
import os
import logging
import asyncio
import aiohttp
from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
from aiogram.types import ReplyKeyboardMarkup, KeyboardButton
from dotenv import load_dotenv
import urllib.parse
from bs4 import BeautifulSoup
import json
import re
import random
import sys
import atexit
import signal
from aiohttp import web
import threading
import subprocess
import time

# ===== –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –õ–û–ì–ì–ò–†–û–í–ê–ù–ò–Ø =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('bot.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ===== –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ó–ê–©–ò–¢–ê –û–¢ –ú–ù–û–ñ–ï–°–¢–í–ï–ù–ù–û–ì–û –ó–ê–ü–£–°–ö–ê =====
class GracefulShutdown:
    def __init__(self):
        self.shutdown = False
        self._setup_signal_handlers()
    
    def _setup_signal_handlers(self):
        signal.signal(signal.SIGTERM, self._handle_signal)
        signal.signal(signal.SIGINT, self._handle_signal)
        signal.signal(signal.SIGUSR1, signal.SIG_IGN)
    
    def _handle_signal(self, signum, frame):
        logger.info(f"üì¢ –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª {signum}, –Ω–∞—á–∏–Ω–∞—é graceful shutdown...")
        self.shutdown = True

def cleanup_lock():
    """–£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏"""
    try:
        lock_file = "/tmp/telegram-bot.lock"
        if os.path.exists(lock_file):
            os.remove(lock_file)
            logger.info("üîì –§–∞–π–ª –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —É–¥–∞–ª–µ–Ω")
    except Exception as e:
        logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ lock-—Ñ–∞–π–ª–∞: {e}")

def kill_previous_instances():
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –±–æ—Ç–∞ –±–µ–∑ psutil"""
    try:
        current_pid = os.getpid()
        script_name = "universal_search_bot.py"
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º pgrep –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Å –Ω–∞—à–∏–º —Å–∫—Ä–∏–ø—Ç–æ–º
        try:
            result = subprocess.run(
                ['pgrep', '-f', script_name], 
                capture_output=True, 
                text=True
            )
            
            if result.returncode == 0:
                pids = result.stdout.strip().split('\n')
                for pid in pids:
                    pid = pid.strip()
                    if pid and pid != str(current_pid):
                        logger.info(f"üö´ –ó–∞–≤–µ—Ä—à–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å –±–æ—Ç–∞: PID {pid}")
                        try:
                            # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–≤–µ—Ä—à–∏—Ç—å
                            os.kill(int(pid), signal.SIGTERM)
                            time.sleep(2)
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å
                            try:
                                os.kill(int(pid), 0)
                                # –ï—Å–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å –µ—â–µ –∂–∏–≤, –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≤–µ—Ä—à–∞–µ–º
                                os.kill(int(pid), signal.SIGKILL)
                                logger.info(f"üíÄ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω –ø—Ä–æ—Ü–µ—Å—Å: PID {pid}")
                            except ProcessLookupError:
                                logger.info(f"‚úÖ –ü—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–µ–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ: PID {pid}")
                                
                        except (ProcessLookupError, ValueError) as e:
                            logger.debug(f"–ü—Ä–æ—Ü–µ—Å—Å —É–∂–µ –∑–∞–≤–µ—Ä—à–µ–Ω: {pid}")
                            
        except FileNotFoundError:
            logger.warning("‚ö†Ô∏è pgrep –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤")
            
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ ps
        try:
            result = subprocess.run(
                ['ps', 'aux'], 
                capture_output=True, 
                text=True
            )
            if result.returncode == 0:
                lines = result.stdout.split('\n')
                for line in lines:
                    if script_name in line and str(current_pid) not in line:
                        parts = line.split()
                        if len(parts) > 1:
                            pid = parts[1]
                            if pid.isdigit() and pid != str(current_pid):
                                logger.info(f"üö´ –ó–∞–≤–µ—Ä—à–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å —á–µ—Ä–µ–∑ ps: PID {pid}")
                                try:
                                    os.kill(int(pid), signal.SIGTERM)
                                    time.sleep(1)
                                    try:
                                        os.kill(int(pid), 0)
                                        os.kill(int(pid), signal.SIGKILL)
                                    except ProcessLookupError:
                                        pass
                                except (ProcessLookupError, ValueError):
                                    pass
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ ps: {e}")
            
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ kill_previous_instances: {e}")

def check_single_instance():
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞"""
    try:
        lock_file = "/tmp/telegram-bot.lock"
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–≤–µ—Ä—à–∏—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã
        kill_previous_instances()
        time.sleep(2)  # –î–∞–µ–º –≤—Ä–µ–º—è –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π lock-—Ñ–∞–π–ª
        if os.path.exists(lock_file):
            with open(lock_file, 'r') as f:
                old_pid = f.read().strip()
            
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∂–∏–≤ –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å
                os.kill(int(old_pid), 0)
                logger.info(f"‚ùå –ë–æ—Ç —É–∂–µ –∑–∞–ø—É—â–µ–Ω –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ {old_pid}. –ó–∞–≤–µ—Ä—à–∞–µ–º.")
                
                # –ü—ã—Ç–∞–µ–º—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–≤–µ—Ä—à–∏—Ç—å —Å—Ç–∞—Ä—ã–π –ø—Ä–æ—Ü–µ—Å—Å
                try:
                    os.kill(int(old_pid), signal.SIGTERM)
                    time.sleep(3)
                except:
                    pass
                    
                # –ï—Å–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å –≤—Å–µ –µ—â–µ –∂–∏–≤, –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≤–µ—Ä—à–∞–µ–º
                try:
                    os.kill(int(old_pid), 0)
                    os.kill(int(old_pid), signal.SIGKILL)
                    time.sleep(2)
                except:
                    pass
                    
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π lock-—Ñ–∞–π–ª
                os.remove(lock_file)
                
            except (ProcessLookupError, ValueError):
                logger.info("üîÑ –°—Ç–∞—Ä—ã–π –ø—Ä–æ—Ü–µ—Å—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, —É–¥–∞–ª—è–µ–º lock-—Ñ–∞–π–ª")
                os.remove(lock_file)
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π lock-—Ñ–∞–π–ª
        with open(lock_file, 'w') as f:
            f.write(str(os.getpid()))
        
        atexit.register(cleanup_lock)
        logger.info(f"üîí –§–∞–π–ª –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —Å–æ–∑–¥–∞–Ω (PID: {os.getpid()})")
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏: {e}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –ø–µ—Ä–µ–¥ –≤—Å–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–º
check_single_instance()

# ===== –£–õ–£–ß–®–ï–ù–ù–´–ô HEALTH CHECK SERVER =====
class HealthServer:
    def __init__(self, port=8080):
        self.port = port
        self.app = web.Application()
        self.app.router.add_get('/health', self.health_check)
        self.app.router.add_get('/readiness', self.readiness_check)
        self.runner = None
        self.site = None
    
    async def health_check(self, request):
        return web.Response(text='OK', status=200)
    
    async def readiness_check(self, request):
        return web.Response(text='READY', status=200)
    
    async def start(self):
        try:
            self.runner = web.AppRunner(self.app)
            await self.runner.setup()
            self.site = web.TCPSite(self.runner, '0.0.0.0', self.port)
            await self.site.start()
            logger.info(f"üåê Health server started on port {self.port}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ health server: {e}")
    
    async def stop(self):
        try:
            if self.site:
                await self.site.stop()
            if self.runner:
                await self.runner.cleanup()
            logger.info("‚úÖ Health server –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ health server: {e}")

# ===== –ó–ê–ì–†–£–ó–ö–ê –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò =====
load_dotenv()

BOT_TOKEN = os.getenv('BOT_TOKEN')
if not BOT_TOKEN:
    logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
    sys.exit(1)

# ===== –£–õ–£–ß–®–ï–ù–ù–´–ô –ö–õ–ê–°–° –ü–û–ò–°–ö–ê –ù–û–í–û–°–¢–ï–ô =====
class ImprovedNewsSearcher:
    def __init__(self):
        self.session = None
        self.cache = {}
        self.cache_timeout = 300
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–æ–º–µ–Ω–æ–≤ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        self.russian_domains = [
            'cbr.ru', 'banki.ru', 'government.ru', 'kremlin.ru', 'minfin.ru',  # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            'rbc.ru', 'vedomosti.ru', 'kommersant.ru', 'ria.ru', 'tass.ru',
            'rt.com', 'lenta.ru', 'gazeta.ru', 'iz.ru', 'mk.ru', 'aif.ru',
            'rg.ru', 'vesti.ru', 'newsru.com', 'fontanka.ru', 'ng.ru',
            'echo.msk.ru', 'bfm.ru', 'forbes.ru', 'vc.ru', 'rb.ru',
            'yandex.ru', 'mail.ru', 'rambler.ru', 'interfax.ru', 'banknn.ru',
            'sputniknews.com', 'rbth.com', 'russian.rt.com', 'themoscowtimes.com',
            'finmarket.ru', 'bankir.ru', 'kommersant.ru', 'vedomosti.ru'  # –î–æ–±–∞–≤–ª–µ–Ω—ã —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        ]
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)
        self.priority_domains = ['cbr.ru', 'banki.ru', 'government.ru', 'kremlin.ru', 'minfin.ru', 'interfax.ru']
        
        # –ß–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è URL (–æ–±—â–∏–µ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
        self.url_blacklist_patterns = [
            r'/news/?$', r'/latest/?$', r'/trending/?$', r'/top-news/?$',
            r'/headlines/?$', r'/breaking/?$', r'/updates/?$', r'/analysis/?$',
            r'/market-news/?$', r'/section/', r'/category/', r'/tag/', r'/topic/',
            r'news\.google\.com$', r'news\.google\.com/',
        ]

    async def get_session(self):
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=30, connect=10, sock_read=20)
            connector = aiohttp.TCPConnector(limit=10, keepalive_timeout=30)
            self.session = aiohttp.ClientSession(timeout=timeout, connector=connector)
        return self.session

    def get_cached_results(self, query):
        cache_key = f"search_{hash(query)}"
        if cache_key in self.cache:
            cache_time, results = self.cache[cache_key]
            if datetime.now() - cache_time < timedelta(seconds=self.cache_timeout):
                return results
        return None

    def set_cached_results(self, query, results):
        cache_key = f"search_{hash(query)}"
        self.cache[cache_key] = (datetime.now(), results)

    def is_russian_domain(self, url):
        try:
            domain = urllib.parse.urlparse(url).netloc.lower()
            return any(russian_domain in domain for russian_domain in self.russian_domains)
        except BaseException:
            return False

    def is_priority_domain(self, url):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –¥–æ–º–µ–Ω –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–º (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)"""
        try:
            domain = urllib.parse.urlparse(url).netloc.lower()
            return any(priority_domain in domain for priority_domain in self.priority_domains)
        except BaseException:
            return False

    def is_russian_text(self, text):
        return bool(re.search('[–∞-—è–ê-–Ø]', text))

    def is_specific_article_url(self, url):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–µ–π, –∞ –Ω–µ –æ–±—â–µ–π –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π
        """
        try:
            url_lower = url.lower()
            parsed = urllib.parse.urlparse(url_lower)
            path = parsed.path
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            for pattern in self.url_blacklist_patterns:
                if re.search(pattern, url_lower):
                    return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏
            article_indicators = [
                r'/\d{4}/', r'/\d{2}/',  # —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞—Ç—É
                r'-\d+\.', r'/\d+',      # —Å–æ–¥–µ—Ä–∂–∏—Ç —Ü–∏—Ñ—Ä—ã (ID —Å—Ç–∞—Ç—å–∏)
                r'\.html', r'\.php', r'\.aspx',  # –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                r'/article/', r'/story/', r'/news/', r'/post/',  # –ø—É—Ç–∏ —Å—Ç–∞—Ç–µ–π
            ]
            
            for indicator in article_indicators:
                if re.search(indicator, url_lower):
                    return True
            
            # –ï—Å–ª–∏ URL –∫–æ—Ä–æ—Ç–∫–∏–π (–º–µ–Ω—å—à–µ 40 —Å–∏–º–≤–æ–ª–æ–≤), –≤–µ—Ä–æ—è—Ç–Ω–æ —ç—Ç–æ –Ω–µ —Å—Ç–∞—Ç—å—è
            if len(url) < 40:
                return False
                
            # –ï—Å–ª–∏ –ø—É—Ç—å —Å–æ–¥–µ—Ä–∂–∏—Ç –º–Ω–æ–≥–æ —Å–ª–µ—à–µ–π, –≤–µ—Ä–æ—è—Ç–Ω–æ —ç—Ç–æ —Å—Ç–∞—Ç—å—è
            if path.count('/') >= 3:
                return True
                
            return False
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ URL: {e}")
            return True  # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –ø—Ä–∏–Ω–∏–º–∞–µ–º URL

    def normalize_title(self, title):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if not title:
            return ""
        
        normalized = title.lower()
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        normalized = re.sub(r'[^\w\s]', '', normalized)
        
        stop_words = ['–Ω–æ–≤–æ—Å—Ç–∏', '—Å–µ–≥–æ–¥–Ω—è', '—Å–µ–π—á–∞—Å', '–ø–æ—Å–ª–µ–¥–Ω–∏–µ', '—Å–≤–µ–∂–∏–µ']
        words = normalized.split()
        filtered_words = [word for word in words if word not in stop_words]
        
        return ' '.join(filtered_words)

    def is_duplicate_article(self, article, existing_articles, similarity_threshold=0.8):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç–∞—Ç—å—è –¥—É–±–ª–∏–∫–∞—Ç–æ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö"""
        if not article or not existing_articles:
            return False
        
        new_title_normalized = self.normalize_title(article.get('title', ''))
        new_url = article.get('url', '')
        
        for existing in existing_articles:
            existing_title_normalized = self.normalize_title(existing.get('title', ''))
            existing_url = existing.get('url', '')
            
            if self.is_same_domain(new_url, existing_url):
                if self.calculate_similarity(new_title_normalized, existing_title_normalized) > similarity_threshold:
                    return True
            
            if self.calculate_similarity(new_title_normalized, existing_title_normalized) > 0.9:
                return True
        
        return False

    def is_same_domain(self, url1, url2):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç –ª–∏ URL –æ–¥–Ω–æ–º—É –¥–æ–º–µ–Ω—É"""
        try:
            domain1 = urllib.parse.urlparse(url1).netloc
            domain2 = urllib.parse.urlparse(url2).netloc
            return domain1 == domain2
        except:
            return False

    def calculate_similarity(self, text1, text2):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
        if not text1 or not text2:
            return 0
        
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 or not words2:
            return 0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0

    async def correct_spelling_auto(self, text):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏—è —á–µ—Ä–µ–∑ Yandex Speller API"""
        try:
            if not self.is_russian_text(text):
                return text
                
            session = await self.get_session()
            encoded_text = urllib.parse.quote(text)
            
            url = f"https://speller.yandex.net/services/spellservice.json/checkText?text={encoded_text}&lang=ru,en"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    corrections = await response.json()
                    
                    if corrections:
                        corrected_text = text
                        for correction in reversed(corrections):
                            if correction.get('s'):
                                fixed_word = correction['s'][0]
                                wrong_word = correction['word']
                                corrected_text = corrected_text.replace(wrong_word, fixed_word)
                        
                        logger.info(f"üìù –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏–µ: '{text}' -> '{corrected_text}'")
                        return corrected_text
                    
            return text
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏—è: {e}")
            return text

    async def translate_to_english_auto(self, text):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π"""
        try:
            if not self.is_russian_text(text):
                return text
                
            session = await self.get_session()
            encoded_text = urllib.parse.quote(text)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π API –ø–µ—Ä–µ–≤–æ–¥–∞
            url = f"https://api.mymemory.translated.net/get?q={encoded_text}&langpair=ru|en"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    if data.get('responseData', {}).get('translatedText'):
                        translated = data['responseData']['translatedText']
                        logger.info(f"üåç –ê–≤—Ç–æ–ø–µ—Ä–µ–≤–æ–¥: '{text}' -> '{translated}'")
                        return translated
            
            return text
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
            return text

    async def prepare_international_query(self, query):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        try:
            logger.info(f"üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
            corrected_query = await self.correct_spelling_auto(query)
            translated_query = await self.translate_to_english_auto(corrected_query)
            logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{translated_query}'")
            return translated_query
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return query

    async def search_yandex_regular(self, query):
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –æ–±—ã—á–Ω—ã–π –Ø–Ω–¥–µ–∫—Å —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤"""
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫ –Ø–Ω–¥–µ–∫—Å
            url = f"https://yandex.ru/search/?text={encoded_query}&lr=213&numdoc=50"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
                'Referer': 'https://yandex.ru/',
                'Cache-Control': 'no-cache'
            }

            async with session.get(url, headers=headers, timeout=25) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []
                    
                    # –ò—â–µ–º –≤—Å–µ –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
                    search_results = soup.find_all('li', class_='serp-item')
                    
                    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    if not search_results:
                        search_results = soup.find_all('div', class_='organic')
                    if not search_results:
                        search_results = soup.find_all('div', class_=re.compile(r'organic|result'))
                    
                    logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞: {len(search_results)}")

                    for result in search_results[:30]:  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 30 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                        try:
                            # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                            title_elem = (result.find('h2') or 
                                        result.find('a', class_='organic__url') or
                                        result.find('a', class_=re.compile(r'link|title')) or
                                        result.find('a', attrs={'href': True}))
                            
                            if not title_elem:
                                continue
                                
                            title = title_elem.get_text().strip()
                            
                            # –ò—â–µ–º —Å—Å—ã–ª–∫—É
                            link = title_elem.get('href', '')
                            
                            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ø–Ω–¥–µ–∫—Å
                            if link.startswith('/'):
                                link = f"https://yandex.ru{link}"
                            elif link.startswith('//'):
                                link = f"https:{link}"
                            
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–∞–º –Ø–Ω–¥–µ–∫—Å –∏ –¥—Ä—É–≥–∏–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã
                            if any(domain in link for domain in ['yandex.ru', 'google.com', 'bing.com']):
                                continue
                            
                            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ URL, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                            news_keywords = ['press', 'news', 'novosti', 'article', 'stati', 'zhurnal', 'doc', 'documents']
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ä–æ—Å—Å–∏–π—Å–∫–∏–π –¥–æ–º–µ–Ω –ò —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                            if (link and link.startswith('http') and 
                                self.is_russian_domain(link) and
                                any(keyword in link.lower() for keyword in news_keywords)):
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
                                title_lower = title.lower()
                                query_lower = query.lower()
                                
                                query_words = set(query_lower.split())
                                title_words = set(title_lower.split())
                                common_words = query_words.intersection(title_words)
                                
                                # –°—á–∏—Ç–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (—á–µ–º –±–æ–ª—å—à–µ –æ–±—â–∏—Ö —Å–ª–æ–≤, —Ç–µ–º –ª—É—á—à–µ)
                                relevance_score = len(common_words)
                                
                                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∞–ª–ª—ã –∑–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã
                                if self.is_priority_domain(link):
                                    relevance_score += 3
                                
                                # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                                if relevance_score >= 1:
                                    articles.append({
                                        'title': title,
                                        'url': link,
                                        'language': 'ru',
                                        'priority': self.is_priority_domain(link),
                                        'relevance_score': relevance_score
                                    })
                                    
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
                            continue

                    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
                    articles.sort(key=lambda x: (x.get('priority', False), x.get('relevance_score', 0)), reverse=True)
                    
                    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
                    unique_articles = []
                    seen_urls = set()
                    for article in articles:
                        if article['url'] not in seen_urls:
                            seen_urls.add(article['url'])
                            unique_articles.append(article)
                    
                    logger.info(f"‚úÖ –Ø–Ω–¥–µ–∫—Å –ø–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–æ–º –Ω–æ–≤–æ—Å—Ç–µ–π: –Ω–∞–π–¥–µ–Ω–æ {len(unique_articles)} —Å—Ç–∞—Ç–µ–π")
                    return unique_articles[:10]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-10
                    
            return []
        except asyncio.TimeoutError:
            logger.warning("‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –Ø–Ω–¥–µ–∫—Å")
            return []
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ø–Ω–¥–µ–∫—Å –ø–æ–∏—Å–∫–∞: {e}")
            return []

    async def search_bing_news_improved(self, query, market='ru-RU', exclude_russian=False):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤ Bing News"""
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            
            if market == 'en-US':
                url = f"https://www.bing.com/news/search?q={encoded_query}&cc=us&setlang=en"
            else:
                url = f"https://www.bing.com/news/search?q={encoded_query}&cc={market}"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept-Language': 'en-US,en;q=0.9' if market == 'en-US' else 'ru-RU,ru;q=0.9',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers, timeout=20) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []

                    # –†–∞—Å—à–∏—Ä—è–µ–º –ø–æ–∏—Å–∫ –∫–∞—Ä—Ç–æ—á–µ–∫
                    news_cards = soup.find_all('div', class_='news-card')[:15]
                    if not news_cards:
                        news_cards = soup.find_all('div', class_='tile')[:15]
                    if not news_cards:
                        news_cards = soup.find_all('article')[:15]

                    for card in news_cards:
                        try:
                            title_elem = (card.find('a', class_='title') or
                                        card.find('a', class_=re.compile('title')) or
                                        card.find('h2') or
                                        card.find('h3') or
                                        card.find('a', attrs={'href': True}))

                            if title_elem and title_elem.get('href'):
                                title = title_elem.get_text().strip()
                                url = title_elem.get('href')

                                if url.startswith('/'):
                                    url = f"https://www.bing.com{url}"

                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Å—Ç–∞—Ç—å—è
                                if url and url.startswith('http') and self.is_specific_article_url(url):
                                    # –î–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä—É—Å—Å–∫–∏–µ –¥–æ–º–µ–Ω—ã
                                    if exclude_russian and self.is_russian_domain(url):
                                        continue
                                        
                                    articles.append({
                                        'title': title,
                                        'url': url,
                                        'language': 'en' if market == 'en-US' else 'ru',
                                        'priority': self.is_priority_domain(url)
                                    })
                        except Exception:
                            continue

                    return articles
            return []
        except asyncio.TimeoutError:
            logger.warning("‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ Bing News")
            return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ Bing News: {e}")
            return []

    async def search_google_news_international(self, query):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤ Google News –¥–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://news.google.com/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers, timeout=20) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []
                    # –ò—â–µ–º –≤—Å–µ —Å—Ç–∞—Ç—å–∏
                    news_articles = soup.find_all('article')[:20]

                    for article in news_articles:
                        try:
                            # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                            title_elem = article.find('h3') or article.find('h4') or article.find('a')
                            if not title_elem:
                                continue
                                
                            title = title_elem.get_text().strip()
                            
                            # –ò—â–µ–º —Å—Å—ã–ª–∫—É
                            link_elem = article.find('a')
                            if link_elem and link_elem.get('href'):
                                relative_url = link_elem.get('href')
                                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É –≤ –∞–±—Å–æ–ª—é—Ç–Ω—É—é
                                if relative_url.startswith('./'):
                                    full_url = f"https://news.google.com{relative_url[1:]}"
                                else:
                                    full_url = f"https://news.google.com{relative_url}" if relative_url.startswith('/') else relative_url
                                
                                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–∞–º Google News
                                if 'news.google.com' in full_url:
                                    continue
                                    
                                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä—É—Å—Å–∫–∏–µ –¥–æ–º–µ–Ω—ã
                                if self.is_russian_domain(full_url):
                                    continue
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Å—Ç–∞—Ç—å—è
                                if self.is_specific_article_url(full_url):
                                    articles.append({
                                        'title': title,
                                        'url': full_url,
                                        'language': 'en',
                                        'source': 'google'
                                    })
                                
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏ Google: {e}")
                            continue

                    logger.info(f"‚úÖ Google News International: –Ω–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
                    return articles
                    
            return []
        except asyncio.TimeoutError:
            logger.warning("‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ Google News International")
            return []
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Google News International: {e}")
            return []

    async def search_duckduckgo_international(self, query):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤ DuckDuckGo –¥–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://html.duckduckgo.com/html/?q={encoded_query}+news&kl=us-en"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers, timeout=20) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []
                    results = soup.find_all('div', class_='result')[:15]

                    for result in results:
                        try:
                            title_elem = result.find('a', class_='result__a')
                            if title_elem:
                                title = title_elem.get_text().strip()
                                url = title_elem.get('href', '')

                                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º DuckDuckGo redirect
                                if 'duckduckgo.com' in url:
                                    match = re.search(r'uddg=([^&]+)', url)
                                    if match:
                                        url = urllib.parse.unquote(match.group(1))

                                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Ä—É—Å—Å–∫–∏–µ –¥–æ–º–µ–Ω—ã
                                if any(domain in url for domain in ['google.com/search', 'bing.com/search', 'yandex.ru/search']):
                                    continue
                                    
                                if self.is_russian_domain(url):
                                    continue

                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Å—Ç–∞—Ç—å—è
                                if url and url.startswith('http') and self.is_specific_article_url(url):
                                    articles.append({
                                        'title': title,
                                        'url': url,
                                        'language': 'en',
                                        'source': 'duckduckgo'
                                    })
                        except Exception:
                            continue

                    logger.info(f"‚úÖ DuckDuckGo International: –Ω–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
                    return articles
            return []
        except asyncio.TimeoutError:
            logger.warning("‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ DuckDuckGo")
            return []
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ DuckDuckGo International: {e}")
            return []

    async def search_only_russian(self, query):
        """–£–õ–£–ß–®–ï–ù–ù–´–ô –ø–æ–∏—Å–∫ –¢–û–õ–¨–ö–û –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö"""
        cache_key = f"russian_only_{hash(query)}"
        cached_results = self.get_cached_results(cache_key)
        if cached_results:
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (—Ç–æ–ª—å–∫–æ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏)")
            return cached_results

        logger.info(f"üîç –£–õ–£–ß–®–ï–ù–ù–´–ô –ø–æ–∏—Å–∫ –¢–û–õ–¨–ö–û –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö: {query}")

        try:
            # –û–°–ù–û–í–ù–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –Ø–Ω–¥–µ–∫—Å –ø–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –Ω–æ–≤–æ—Å—Ç–µ–π
            yandex_results = await self.search_yandex_regular(query)
            logger.info(f"‚úÖ –Ø–Ω–¥–µ–∫—Å –ø–æ–∏—Å–∫ (–Ω–æ–≤–æ—Å—Ç–∏): {len(yandex_results)} —Å—Ç–∞—Ç–µ–π")

            # –£–ü–†–û–©–ï–ù–ù–ê–Ø —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è - –ø—Ä–∏–Ω–∏–º–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
            filtered_results = []
            seen_urls = set()
            
            for result in yandex_results:
                if result and result.get('url'):
                    url = result['url'].lower()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                    news_keywords = ['press', 'news', 'novosti', 'article', 'stati', 'zhurnal', 'doc', 'documents']
                    is_news_url = any(keyword in url for keyword in news_keywords)
                    
                    if (url.startswith('http') and 
                        url not in seen_urls and 
                        is_news_url):
                        
                        seen_urls.add(url)
                        filtered_results.append(result)

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º: —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ, –ø–æ—Ç–æ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            priority_articles = [r for r in filtered_results if r.get('priority') or self.is_priority_domain(r['url'])]
            regular_articles = [r for r in filtered_results if not (r.get('priority') or self.is_priority_domain(r['url']))]
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            regular_articles.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            final_results = priority_articles + regular_articles
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            final_results = final_results[:8]
            
            self.set_cached_results(cache_key, final_results)
            logger.info(f"üìä –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(final_results)} —Å—Ç–∞—Ç–µ–π (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö: {len(priority_articles)})")
            return final_results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–æ–∏—Å–∫–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []

    async def search_international_only(self, query):
        """–ü–æ–∏—Å–∫ –¢–û–õ–¨–ö–û –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö"""
        cache_key = f"international_only_{hash(query)}"
        cached_results = self.get_cached_results(cache_key)
        if cached_results:
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ)")
            return cached_results

        logger.info(f"üåç –ü–æ–∏—Å–∫ –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö: {query}")

        all_results = []

        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            international_query = await self.prepare_international_query(query)
            logger.info(f"üåç –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {international_query}")

            # –ü–æ–∏—Å–∫ –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
            google_results = await self.search_google_news_international(international_query)
            all_results.extend(google_results)
            logger.info(f"‚úÖ Google News International: {len(google_results)} —Å—Ç–∞—Ç–µ–π")

            bing_en_results = await self.search_bing_news_improved(international_query, 'en-US', exclude_russian=True)
            all_results.extend(bing_en_results)
            logger.info(f"‚úÖ Bing International: {len(bing_en_results)} —Å—Ç–∞—Ç–µ–π")

            duckduckgo_results = await self.search_duckduckgo_international(international_query)
            all_results.extend(duckduckgo_results)
            logger.info(f"‚úÖ DuckDuckGo International: {len(duckduckgo_results)} —Å—Ç–∞—Ç–µ–π")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–º –ø–æ–∏—Å–∫–µ: {e}")

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
        filtered_results = []
        seen_urls = set()
        
        for result in all_results:
            if result and result.get('url'):
                url = result['url'].lower()
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä—É—Å—Å–∫–∏–µ –¥–æ–º–µ–Ω—ã –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Å—Ç–∞—Ç—å—è
                if not self.is_russian_domain(url) and url.startswith('http') and url not in seen_urls and self.is_specific_article_url(url):
                    seen_urls.add(url)
                    filtered_results.append(result)

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        final_results = filtered_results[:10]
        
        self.set_cached_results(cache_key, final_results)
        logger.info(f"üìä –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(final_results)} —Å—Ç–∞—Ç–µ–π")
        return final_results

    async def universal_search(self, query, search_type="all"):
        cache_key = f"{search_type}_{query}"
        cached_results = self.get_cached_results(cache_key)
        if cached_results:
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            return cached_results

        all_results = []

        try:
            if search_type in ["all", "russian"]:
                logger.info(f"üîç –ü–æ–∏—Å–∫ –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö: {query}")

                # –û–°–ù–û–í–ù–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –Ø–Ω–¥–µ–∫—Å –ø–æ–∏—Å–∫
                yandex_results = await self.search_yandex_regular(query)
                all_results.extend(yandex_results)
                logger.info(f"‚úÖ –Ø–Ω–¥–µ–∫—Å –ø–æ–∏—Å–∫: {len(yandex_results)} —Å—Ç–∞—Ç–µ–π")

                bing_ru_results = await self.search_bing_news_improved(query, 'ru-RU')
                all_results.extend(bing_ru_results)
                logger.info(f"‚úÖ Bing –†–æ—Å—Å–∏—è: {len(bing_ru_results)} —Å—Ç–∞—Ç–µ–π")

            if search_type in ["all", "international"]:
                logger.info(f"üåç –ü–æ–∏—Å–∫ –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö: {query}")

                international_results = await self.search_international_only(query)
                all_results.extend(international_results)
                logger.info(f"‚úÖ –í—Å–µ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: {len(international_results)} —Å—Ç–∞—Ç–µ–π")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º –ø–æ–∏—Å–∫–µ: {e}")

        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è - —Ç–æ–ª—å–∫–æ –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ URL –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º —Å—Ç–∞—Ç—å—è–º
        filtered_results = []
        seen_urls = set()
        
        for result in all_results:
            if result and result.get('url') and result['url'] not in seen_urls and self.is_specific_article_url(result['url']):
                seen_urls.add(result['url'])
                filtered_results.append(result)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –¥–ª—è —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        if search_type in ["all", "russian"]:
            priority_articles = [r for r in filtered_results if r.get('priority') or (r.get('language') == 'ru' and self.is_priority_domain(r['url']))]
            regular_articles = [r for r in filtered_results if r not in priority_articles]
            filtered_results = priority_articles + regular_articles

        self.set_cached_results(cache_key, filtered_results[:15])
        logger.info(f"üìä –ò—Ç–æ–≥–æ–≤—ã–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(filtered_results)} —Å—Ç–∞—Ç–µ–π")
        return filtered_results[:15]

    async def get_fresh_news_today(self):
        cache_key = "fresh_news_today"
        cached_results = self.get_cached_results(cache_key)
        if cached_results:
            return cached_results

        logger.info("üîç –ü–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è...")

        today_queries = [
            "–≠–ü–† —Å–µ–≥–æ–¥–Ω—è",
            "–≠–ü–† –Ω–æ–≤–æ—Å—Ç–∏ —Å–µ–≥–æ–¥–Ω—è", 
            "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞ —Å–µ–≥–æ–¥–Ω—è",
            "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º –Ω–æ–≤–æ—Å—Ç–∏",
            "—Ü–∏—Ñ—Ä–æ–≤—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∞–∫—Ç–∏–≤—ã",
            "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã–µ –ø–µ—Å–æ—á–Ω–∏—Ü—ã –†–æ—Å—Å–∏—è",
            "–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏ –≠–ü–†",
            "–¶–ë –†–§ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º"
        ]

        all_articles = []

        for query in today_queries:
            try:
                logger.info(f"üì¢ –ü–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {query}")

                # –û–°–ù–û–í–ù–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –Ø–Ω–¥–µ–∫—Å –ø–æ–∏—Å–∫
                yandex_results = await self.search_yandex_regular(query)
                bing_results = await self.search_bing_news_improved(query, 'ru-RU')

                for article in yandex_results + bing_results:
                    if not self.is_duplicate_article(article, all_articles) and self.is_specific_article_url(article['url']):
                        all_articles.append(article)

                await asyncio.sleep(1)

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
                continue

        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        filtered_articles = []
        seen_urls = set()
        
        for article in all_articles:
            if article and article.get('url') and article['url'] not in seen_urls and self.is_specific_article_url(article['url']):
                seen_urls.add(article['url'])
                filtered_articles.append(article)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        priority_articles = [a for a in filtered_articles if a.get('priority') or self.is_priority_domain(a.get('url', ''))]
        regular_articles = [a for a in filtered_articles if a not in priority_articles]
        
        filtered_articles = priority_articles + regular_articles

        if len(filtered_articles) < 4:
            logger.info("üîç –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π...")
            backup_queries = [
                "–≠–ü–†", 
                "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞ –†–æ—Å—Å–∏—è",
                "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º",
                "—Ü–∏—Ñ—Ä–æ–≤–∞—è –≤–∞–ª—é—Ç–∞ –¶–ë",
                "–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è"
            ]
            
            for query in backup_queries:
                try:
                    backup_results = await self.universal_search(query, "all")
                    for article in backup_results:
                        if article['url'] not in seen_urls and self.is_specific_article_url(article['url']):
                            seen_urls.add(article['url'])
                            filtered_articles.append(article)
                    await asyncio.sleep(1)
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")

        def relevance_score(article):
            title = article.get('title', '').lower()
            score = 0
            keywords = ['—ç–ø—Ä', '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º', '—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞', 
                       '—Ü–∏—Ñ—Ä–æ–≤–∞—è –≤–∞–ª—é—Ç–∞', '—Ü–± —Ä—Ñ', '—Ñ–∏–Ω—Ç–µ—Ö', '–±–ª–æ–∫—á–µ–π–Ω', '–±–∞–Ω–∫ —Ä–æ—Å—Å–∏–∏', 'cbr.ru']
            
            for keyword in keywords:
                if keyword in title:
                    score += 1
                    
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∞–ª–ª—ã –∑–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            if article.get('priority') or self.is_priority_domain(article.get('url', '')):
                score += 2
                
            return score

        filtered_articles.sort(key=relevance_score, reverse=True)
        final_articles = filtered_articles[:8]

        self.set_cached_results(cache_key, final_articles)

        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(final_articles)}")
        return final_articles

    async def close(self):
        if self.session and not self.session.closed:
            await self.session.close()

# ===== –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï =====
main_keyboard = ReplyKeyboardMarkup(
    keyboard=[
        [
            KeyboardButton(text="üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π"), 
            KeyboardButton(text="üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
        ],
        [
            KeyboardButton(text="‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏"), 
            KeyboardButton(text="üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫")
        ]
    ], 
    resize_keyboard=True
)

user_search_type = {}

# ===== –£–õ–£–ß–®–ï–ù–ù–´–ô ROBUST BOT CLASS =====
class RobustBot:
    def __init__(self):
        self.bot = Bot(token=BOT_TOKEN)
        self.dp = Dispatcher()
        self.news_searcher = ImprovedNewsSearcher()
        self._is_running = False
        self._shutdown_event = asyncio.Event()
        self.setup_handlers()
    
    def setup_handlers(self):
        @self.dp.message(Command("start"))
        async def cmd_start(message: types.Message):
            await message.answer(
                "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –æ–± –≠–ü–†\n\n"
                "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π ‚Äì —Ç–æ–ª—å–∫–æ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏\n"
                "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ‚Äì —Ç–æ–ª—å–∫–æ –∑–∞—Ä—É–±–µ–∂–Ω—ã–µ –°–ú–ò\n"
                "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ ‚Äì –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏\n"
                "üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ ‚Äì —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏\n\n"
                "üí° –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:\n"
                "‚Ä¢ –≠–ü–† –≤ —Ñ–∏–Ω–∞–Ω—Å–∞—Ö\n"
                "‚Ä¢ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞\n" 
                "‚Ä¢ —Ü–∏—Ñ—Ä–æ–≤—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∞–∫—Ç–∏–≤—ã\n"
                "‚Ä¢ –ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏ –≠–ü–†\n\n"
                "–ù–∞–ø–∏—à–∏—Ç–µ /help –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏",
                reply_markup=main_keyboard
            )

        @self.dp.message(Command("help"))
        async def cmd_help(message: types.Message):
            help_text = """<b>üìñ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –æ–± –≠–ü–†</b>

<b>–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:</b>
/start - –Ω–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É —Å –±–æ—Ç–æ–º
/help - –ø–æ–∫–∞–∑–∞—Ç—å —ç—Ç—É —Å–ø—Ä–∞–≤–∫—É

<b>–†–µ–∂–∏–º—ã –ø–æ–∏—Å–∫–∞:</b>
üîç <b>–ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π</b> - –¢–û–õ–¨–ö–û —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
üåç <b>–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏</b> - —Ç–æ–ª—å–∫–æ –∑–∞—Ä—É–±–µ–∂–Ω—ã–µ –°–ú–ò  
‚ö° <b>–°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏</b> - –ø–æ–∏—Å–∫ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è
üìä <b>–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫</b> - —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏

<b>üí° –ü—Ä–∏–º–µ—Ä—ã —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:</b>

<b>–ö–æ—Ä–æ—Ç–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã:</b>
‚Ä¢ –≠–ü–†
‚Ä¢ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞  
‚Ä¢ –ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏
‚Ä¢ —Ñ–∏–Ω—Ç–µ—Ö —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ

<b>–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã:</b>
‚Ä¢ –≠–ü–† –≤ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π —Å—Ñ–µ—Ä–µ
‚Ä¢ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º –¶–ë
‚Ä¢ —Ü–∏—Ñ—Ä–æ–≤—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∞–∫—Ç–∏–≤—ã –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ

<b>–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã:</b>
‚Ä¢ Russia fintech sandbox
‚Ä¢ digital financial assets Russia
‚Ä¢ Bank of Russia regulation

<b>‚ö° –°–æ–≤–µ—Ç—ã –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:</b>
‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã (2-5 —Å–ª–æ–≤)
‚Ä¢ –î–ª—è –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: \"–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏ –≠–ü–†\"
‚Ä¢ –£–∫–∞–∑—ã–≤–∞–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: \"—Ü–∏—Ñ—Ä–æ–≤—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∞–∫—Ç–∏–≤—ã\"
‚Ä¢ –î–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö: –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã"""
            await message.answer(help_text, parse_mode="HTML")

        @self.dp.message(lambda message: message.text == "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π")
        async def search_epr_news(message: types.Message):
            user_id = message.from_user.id
            user_search_type[user_id] = 'russian'
            await message.answer("üîç –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö:")

        @self.dp.message(lambda message: message.text == "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
        async def international_sources(message: types.Message):
            user_id = message.from_user.id
            user_search_type[user_id] = 'international'
            await message.answer("üåç –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π):")

        @self.dp.message(lambda message: message.text == "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏")
        async def fresh_news(message: types.Message):
            await message.answer("‚ö° –ò—â—É —Å–∞–º—ã–µ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏")
            try:
                articles = await self.news_searcher.get_fresh_news_today()
                if articles:
                    response = "‚ö° –°–∞–º—ã–µ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏:\n\n"
                    for i, article in enumerate(articles, 1):
                        response += f"{i}. {article['title']}\n"
                        response += f"   üîó {article['url']}\n\n"
                        if len(response) > 3500:
                            response += "... (–ø–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ —Å—Ç–∞—Ç—å–∏)"
                            break
                else:
                    response = "üòî –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è.\n\n"
                    response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É."
                await message.answer(response)
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
                await message.answer("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")

        @self.dp.message(lambda message: message.text == "üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫")
        async def quick_search(message: types.Message):
            user_id = message.from_user.id
            user_search_type[user_id] = 'quick'
            await message.answer("üìä –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º (—Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ):")

        @self.dp.message()
        async def handle_text(message: types.Message):
            if self._shutdown_event.is_set():
                await message.answer("‚ùå –ë–æ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ä–µ–∂–∏–º–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
                return

            user_text = message.text.strip()
            user_id = message.from_user.id

            buttons = [
                "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π",
                "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏", 
                "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏",
                "üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫"]
            if user_text.startswith('/') or user_text in buttons:
                return

            await message.answer(f"üîç –ò—â—É –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{user_text}'...")
            await self.process_search(message, user_text, user_id)

    async def process_search(self, message, user_text, user_id):
        try:
            search_type = user_search_type.pop(user_id, 'all')
            
            if search_type == 'quick':
                # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫: —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ + –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                russian_articles = await self.news_searcher.universal_search(user_text, "russian")
                international_articles = await self.news_searcher.search_international_only(user_text)
                
                if russian_articles or international_articles:
                    response = f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ '{user_text}':\n\n"
                    
                    if russian_articles:
                        response += "üá∑üá∫ –†–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:\n\n"
                        for i, article in enumerate(russian_articles[:3], 1):
                            response += f"{i}. {article['title']}\n"
                            response += f"   üîó {article['url']}\n\n"
                    
                    if international_articles:
                        response += "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:\n\n"
                        start_index = len(russian_articles[:3]) + 1
                        for i, article in enumerate(international_articles[:3], start_index):
                            response += f"{i}. {article['title']}\n"
                            response += f"   üîó {article['url']}\n\n"
                else:
                    response = f"üòî –ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_text}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π.\n\n"
                    response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞."
                    
            elif search_type == 'international':
                # –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                articles = await self.news_searcher.search_international_only(user_text)
                
                if articles:
                    response = f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ '{user_text}':\n\n"
                    for i, article in enumerate(articles[:6], 1):
                        response += f"{i}. {article['title']}\n"
                        response += f"   üîó {article['url']}\n\n"
                else:
                    response = f"üòî –ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_text}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö.\n\n"
                    response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã."
                    
            elif search_type == 'russian':
                # –¢–û–õ–¨–ö–û —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                articles = await self.news_searcher.search_only_russian(user_text)
                
                if articles:
                    response = f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ '{user_text}':\n\n"
                    for i, article in enumerate(articles[:6], 1):
                        response += f"{i}. {article['title']}\n"
                        response += f"   üîó {article['url']}\n\n"
                else:
                    response = f"üòî –ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_text}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö.\n\n"
                    response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞."
                    
            else:
                # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –±–µ–∑ –≤—ã–±–æ—Ä–∞ —Ç–∏–ø–∞)
                articles = await self.news_searcher.universal_search(user_text, "all")
                
                if articles:
                    russian_articles = [a for a in articles if a.get('language') == 'ru']
                    english_articles = [a for a in articles if a.get('language') == 'en']

                    response = f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ '{user_text}':\n\n"

                    if russian_articles:
                        response += "üá∑üá∫ –†–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:\n\n"
                        for i, article in enumerate(russian_articles[:3], 1):
                            response += f"{i}. {article['title']}\n"
                            response += f"   üîó {article['url']}\n\n"

                    if english_articles:
                        response += "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:\n\n"
                        for i, article in enumerate(english_articles[:3], 1):
                            response += f"{i}. {article['title']}\n"
                            response += f"   üîó {article['url']}\n\n"
                else:
                    response = f"üòî –ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_text}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π.\n\n"
                    response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞."

            await message.answer(response)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            await message.answer(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –∑–∞–ø—Ä–æ—Å.")

    async def start(self):
        """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            logger.info("üöÄ –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–º –ø–æ–∏—Å–∫–æ–º...")
            await self.bot.delete_webhook(drop_pending_updates=True)
            
            self._is_running = True
            self._shutdown_event.clear()
            
            # Polling —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –ø—Ä–æ–≤–µ—Ä–∫–æ–π shutdown
            while self._is_running and not self._shutdown_event.is_set():
                try:
                    await self.dp.start_polling(
                        self.bot, 
                        skip_updates=True,
                        timeout=10,
                        relax=0.5,
                        allowed_updates=['message', 'callback_query']
                    )
                except asyncio.CancelledError:
                    logger.info("üîÑ –ü–æ–ª–ª–∏–Ω–≥ –æ—Ç–º–µ–Ω–µ–Ω")
                    break
                except Exception as e:
                    if self._is_running and not self._shutdown_event.is_set():
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ polling: {e}")
                        logger.info("üîÑ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ polling —á–µ—Ä–µ–∑ 3 —Å–µ–∫—É–Ω–¥—ã...")
                        await asyncio.sleep(3)
                    else:
                        break
                        
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞: {e}")
            raise
    
    async def stop(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–æ—Ç–∞"""
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ—Å—Ç–∞–Ω–æ–≤–∫—É –±–æ—Ç–∞...")
        self._is_running = False
        self._shutdown_event.set()
        
        try:
            await self.dp.stop_polling()
            logger.info("‚úÖ –ü–æ–ª–ª–∏–Ω–≥ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ –ø–æ–ª–ª–∏–Ω–≥–∞: {e}")
        
        try:
            await self.news_searcher.close()
            logger.info("‚úÖ –ü–æ–∏—Å–∫–æ–≤–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞–∫—Ä—ã—Ç")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞: {e}")
        
        try:
            await self.bot.session.close()
            logger.info("‚úÖ –°–µ—Å—Å–∏—è –±–æ—Ç–∞ –∑–∞–∫—Ä—ã—Ç–∞")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ —Å–µ—Å—Å–∏–∏: {e}")
        
        logger.info("‚úÖ –ë–æ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

# ===== –£–õ–£–ß–®–ï–ù–ù–ê–Ø –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ó–ê–ü–£–°–ö–ê =====
async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π SIGTERM"""
    bot_instance = None
    health_server = None
    shutdown_manager = GracefulShutdown()
    
    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º health server –ø–µ—Ä–≤—ã–º –¥–µ–ª–æ–º
        health_server = HealthServer()
        await health_server.start()
        
        bot_instance = RobustBot()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π task
        bot_task = asyncio.create_task(bot_instance.start())
        
        logger.info("‚úÖ –í—Å–µ —Å–µ—Ä–≤–∏—Å—ã –∑–∞–ø—É—â–µ–Ω—ã, –±–æ—Ç –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        while not shutdown_manager.shutdown:
            await asyncio.sleep(2)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∂–∏–≤ –ª–∏ –±–æ—Ç –∏ –Ω–µ –∑–∞–ø—Ä–æ—à–µ–Ω–∞ –ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
            if bot_task.done() and not shutdown_manager.shutdown:
                if bot_task.exception():
                    logger.error(f"‚ùå –ë–æ—Ç —É–ø–∞–ª —Å –æ—à–∏–±–∫–æ–π: {bot_task.exception()}")
                    logger.info("üîÑ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞...")
                    bot_task = asyncio.create_task(bot_instance.start())
                else:
                    logger.warning("‚ö†Ô∏è –ë–æ—Ç –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –±–µ–∑ –æ—à–∏–±–∫–∏, –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º...")
                    bot_task = asyncio.create_task(bot_instance.start())
        
        # Graceful shutdown –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ SIGTERM
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º graceful shutdown...")
        
        if bot_instance:
            await bot_instance.stop()
        
        if bot_task and not bot_task.done():
            try:
                await asyncio.wait_for(bot_task, timeout=10.0)
                logger.info("‚úÖ –ó–∞–¥–∞—á–∞ –±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            except asyncio.TimeoutError:
                logger.warning("‚è∞ –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –±–æ—Ç–∞, –æ—Ç–º–µ–Ω—è–µ–º –∑–∞–¥–∞—á—É...")
                bot_task.cancel()
                try:
                    await bot_task
                except asyncio.CancelledError:
                    logger.info("‚úÖ –ó–∞–¥–∞—á–∞ –±–æ—Ç–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞")
            
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ main(): {e}")
    finally:
        # –í—Å–µ–≥–¥–∞ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º health server
        if health_server:
            await health_server.stop()
        
        if bot_instance:
            await bot_instance.stop()

# ===== –ó–ê–ü–£–°–ö –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø –° –ë–ï–°–ö–û–ù–ï–ß–ù–´–ú–ò –ü–ï–†–ï–ó–ê–ü–£–°–ö–ê–ú–ò =====
if __name__ == "__main__":
    import time
    
    restart_delay = 3  # –ù–∞—á–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    max_restart_delay = 300  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ (5 –º–∏–Ω—É—Ç)
    total_restarts = 0  # –°—á–µ—Ç—á–∏–∫ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–≤ –¥–ª—è –ª–æ–≥–æ–≤
    
    logger.info("‚ôæÔ∏è –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–º –ø–æ–∏—Å–∫–æ–º –∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–º–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–º–∏")
    
    # –ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–≤
    while True:
        try:
            total_restarts += 1
            logger.info(f"üîÑ –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ (–ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ #{total_restarts})...")
            
            asyncio.run(main())
            
            # –ï—Å–ª–∏ main() –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –±–µ–∑ –∏—Å–∫–ª—é—á–µ–Ω–∏—è, –∑–Ω–∞—á–∏—Ç –±–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–∏–ª—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
            logger.info("‚úÖ –ë–æ—Ç –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ, –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ 5 —Å–µ–∫—É–Ω–¥...")
            time.sleep(5)
            restart_delay = 3  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
            
        except KeyboardInterrupt:
            logger.info("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
            break
            
        except SystemExit as e:
            if e.code == 0:
                logger.info("‚úÖ –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã, –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ 5 —Å–µ–∫—É–Ω–¥...")
                time.sleep(5)
                restart_delay = 3
            else:
                logger.error(f"üö® –ê–≤–∞—Ä–∏–π–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Å –∫–æ–¥–æ–º {e.code}")
                logger.info(f"‚è≥ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ {restart_delay} —Å–µ–∫—É–Ω–¥...")
                time.sleep(restart_delay)
                restart_delay = min(restart_delay * 1.5, max_restart_delay)
                
        except Exception as e:
            logger.error(f"üí• –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")
            logger.info(f"‚è≥ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ {restart_delay} —Å–µ–∫—É–Ω–¥...")
            time.sleep(restart_delay)
            restart_delay = min(restart_delay * 1.5, max_restart_delay)
    
    logger.info(f"üëã –ë–æ—Ç –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É –ø–æ—Å–ª–µ {total_restarts} –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–≤")
