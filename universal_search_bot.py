from datetime import datetime
import os
import logging
import asyncio
import aiohttp
from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
from aiogram.types import ReplyKeyboardMarkup, KeyboardButton
from dotenv import load_dotenv
import urllib.parse
from bs4 import BeautifulSoup
import json
import re
import random
from datetime import datetime, timedelta

def format_date(date_str):
    if not date_str:
        return ""
    
    # –£–±–∏—Ä–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –≤—Ä–µ–º–µ–Ω–∏
    relative_patterns = [
        r'\d+\s*(–º–µ—Å|–º–µ—Å—è—Ü|–º–µ—Å—è—Ü–µ–≤|–º–µ—Å—è—Ü–∞)',
        r'\d+\s*(–≥–æ–¥|–≥–æ–¥–∞|–ª–µ—Ç)',
        r'\d+\s*(–¥–µ–Ω—å|–¥–Ω—è|–¥–Ω–µ–π)',
        r'\d+\s*(–Ω–µ–¥–µ–ª|–Ω–µ–¥–µ–ª–∏|–Ω–µ–¥–µ–ª—å)',
        r'\d+\s*(—á–∞—Å|—á–∞—Å–∞|—á–∞—Å–æ–≤)',
        r'\d+\s*(–º–∏–Ω—É—Ç|–º–∏–Ω—É—Ç—ã)',
        r'—Ç–æ–ª—å–∫–æ —á—Ç–æ',
        r'–≤—á–µ—Ä–∞',
        r'—Å–µ–≥–æ–¥–Ω—è'
    ]
    
    for pattern in relative_patterns:
        if re.search(pattern, date_str.lower()):
            return ""

    try:
        from datetime import datetime
        formats_to_try = [
            '%Y-%m-%d',
            '%d.%m.%Y',
            '%d/%m/%Y',
            '%m/%d/%Y',
            '%B %d, %Y',
            '%b %d, %Y',
            '%d %B %Y',
            '%d %b %Y',
            '%Y-%m-%dT%H:%M:%S',
            '%Y-%m-%d %H:%M:%S'
        ]
        for fmt in formats_to_try:
            try:
                dt = datetime.strptime(date_str.strip(), fmt)
                return dt.strftime('%d.%m.%Y')
            except ValueError:
                continue
    except Exception:
        pass
    return ""

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

load_dotenv()

BOT_TOKEN = os.getenv('BOT_TOKEN')
if not BOT_TOKEN:
    logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
    exit(1)

bot = Bot(token=BOT_TOKEN)
dp = Dispatcher()

main_keyboard = ReplyKeyboardMarkup(
    keyboard=[
        [
            KeyboardButton(text="üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π"), 
            KeyboardButton(text="üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
        ],
        [
            KeyboardButton(text="‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏"), 
            KeyboardButton(text="üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫")
        ]
    ], 
    resize_keyboard=True
)

class ImprovedNewsSearcher:
    def __init__(self):
        self.session = None
        self.cache = {}
        self.cache_timeout = 300
        self.russian_domains = [
            'rbc.ru', 'vedomosti.ru', 'kommersant.ru', 'ria.ru', 'tass.ru',
            'rt.com', 'lenta.ru', 'gazeta.ru', 'iz.ru', 'mk.ru', 'aif.ru',
            'rg.ru', 'vesti.ru', 'newsru.com', 'fontanka.ru', 'ng.ru',
            'echo.msk.ru', 'bfm.ru', 'forbes.ru', 'vc.ru', 'rb.ru', 'banki.ru',
            'cbr.ru', 'rosfinmonitoring.ru', 'government.ru', 'kremlin.ru',
            'minfin.ru', 'yandex.ru', 'mail.ru', 'rambler.ru',
            'sputniknews.com', 'rbth.com', 'russian.rt.com', 'themoscowtimes.com'
        ]

    async def get_session(self):
        if self.session is None:
            timeout = aiohttp.ClientTimeout(total=15, connect=5, sock_read=10)
            self.session = aiohttp.ClientSession(timeout=timeout)
        return self.session

    def get_cached_results(self, query):
        cache_key = f"search_{hash(query)}"
        if cache_key in self.cache:
            cache_time, results = self.cache[cache_key]
            if datetime.now() - cache_time < timedelta(seconds=self.cache_timeout):
                return results
        return None

    def set_cached_results(self, query, results):
        cache_key = f"search_{hash(query)}"
        self.cache[cache_key] = (datetime.now(), results)

    def is_russian_domain(self, url):
        try:
            domain = urllib.parse.urlparse(url).netloc.lower()
            return any(russian_domain in domain for russian_domain in self.russian_domains)
        except BaseException:
            return False

    async def translate_query(self, query):
        translation_dict = {
            '—ç–ø—Ä': 'EPR',
            '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π': 'experimental',
            '–ø—Ä–∞–≤–æ–≤–æ–π': 'legal',
            '—Ä–µ–∂–∏–º': 'regime',
            '—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è': 'regulatory',
            '–ø–µ—Å–æ—á–Ω–∏—Ü–∞': 'sandbox',
            '—Ñ–∏–Ω–∞–Ω—Å—ã': 'finance',
            '—Ñ–∏–Ω—Ç–µ—Ö': 'fintech',
            '–±–∞–Ω–∫': 'bank',
            '—Ä–æ—Å—Å–∏—è': 'Russia',
            '—Ä—Ñ': 'Russian Federation',
            '—Ü–∏—Ñ—Ä–æ–≤–æ–π': 'digital',
            '—ç–∫–æ–Ω–æ–º–∏–∫–∞': 'economy',
            '–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏': 'innovations',
            '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': 'technologies',
            '–∑–∞–∫–æ–Ω': 'law',
            '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ': 'government',
            '—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ': 'regulation'
        }

        words = query.lower().split()
        translated_words = []

        for word in words:
            clean_word = re.sub(r'[^\w\s]', '', word)
            if clean_word in translation_dict:
                translated_words.append(translation_dict[clean_word])
            else:
                translated_words.append(clean_word)

        translated_query = ' '.join(translated_words)

        if any(word in query.lower() for word in ['—ç–ø—Ä', '—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è', '–ø–µ—Å–æ—á–Ω–∏—Ü–∞']):
            translated_query += " Russia"

        return translated_query

    async def search_yandex_news_direct(self, query):
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://yandex.ru/news/search?text={encoded_query}"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []

                    news_cards = soup.find_all('article', class_='mg-card')[:10]

                    for card in news_cards:
                        try:
                            title_elem = card.find('h2', class_='mg-card__title') or card.find('a', class_='mg-card__link')
                            if not title_elem:
                                continue

                            title = title_elem.get_text().strip()
                            link = title_elem.get('href', '')

                            if link.startswith('https://news.yandex.ru/yandsearch?'):
                                match = re.search(r'cl4url=([^&]+)', link)
                                if match:
                                    link = urllib.parse.unquote(match.group(1))
                            elif link.startswith('/'):
                                link = f"https://yandex.ru{link}"

                            source_elem = card.find('span', class_='mg-card-source__source')
                            time_elem = card.find('span', class_='mg-card-source__time')
                            desc_elem = card.find('div', class_='mg-card__annotation')

                            if link and not any(
                                domain in link for domain in [
                                    'google.com/search',
                                    'yandex.ru/search']):
                                articles.append({
                                    'title': title,
                                    'url': link,
                                    'source': source_elem.get_text().strip() if source_elem else '–Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–∏',
                                    'date': time_elem.get_text().strip() if time_elem else '',
                                    'description': desc_elem.get_text().strip() if desc_elem else '',
                                    'language': 'ru'
                                })
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ –Ø–Ω–¥–µ–∫—Å: {e}")
                            continue

                    return articles
            return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []

    async def search_bing_news_improved(self, query, market='ru-RU', exclude_russian=False):
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            
            if market == 'en-US':
                url = f"https://www.bing.com/news/search?q={encoded_query}&cc=us&setlang=en"
            else:
                url = f"https://www.bing.com/news/search?q={encoded_query}&cc={market}"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept-Language': 'en-US,en;q=0.9' if market == 'en-US' else 'ru-RU,ru;q=0.9',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []

                    news_cards = soup.find_all('div', class_='news-card')[:10]
                    if not news_cards:
                        news_cards = soup.find_all('div', class_='tile')[:10]
                    if not news_cards:
                        news_cards = soup.find_all('article')[:10]

                    for card in news_cards:
                        try:
                            title_elem = (card.find('a', class_='title') or
                                        card.find('a', class_=re.compile('title')) or
                                        card.find('h2') or
                                        card.find('h3') or
                                        card.find('a', attrs={'href': True}))

                            if title_elem and title_elem.get('href'):
                                title = title_elem.get_text().strip()
                                url = title_elem.get('href')

                                if url.startswith('/'):
                                    url = f"https://www.bing.com{url}"

                                if 'bing.com/news/search' in url:
                                    continue

                                if exclude_russian and self.is_russian_domain(url):
                                    continue

                                source_elem = card.find(['div', 'span'], class_=re.compile('source|author'))
                                time_elem = card.find(['div', 'span'], class_=re.compile('time|date'))

                                if url and not any(
                                    search_domain in url for search_domain in [
                                        'google.com/search',
                                        'bing.com/search']):
                                    articles.append({
                                        'title': title,
                                        'url': url,
                                        'source': source_elem.get_text().strip() if source_elem else 'Bing News',
                                        'date': time_elem.get_text().strip() if time_elem else '',
                                        'language': 'en' if market == 'en-US' else 'ru'
                                    })
                        except Exception:
                            continue

                    return articles
            return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ Bing News: {e}")
            return []

    async def search_google_news_english(self, query):
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://news.google.com/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []
                    news_cards = soup.find_all('article')[:12]

                    for card in news_cards:
                        try:
                            title_elem = card.find('h3') or card.find('h4') or card.find('a', attrs={'href': True})
                            if title_elem:
                                title = title_elem.get_text().strip()
                                link_elem = title_elem.find_parent('a') if title_elem.name != 'a' else title_elem
                                if link_elem and link_elem.get('href'):
                                    url = link_elem.get('href')
                                    if url.startswith('./'):
                                        url = f"https://news.google.com{url[1:]}"
                                    
                                    if 'news.google.com' in url:
                                        continue

                                    if self.is_russian_domain(url):
                                        continue

                                    time_elem = card.find('time')
                                    source_elem = card.find(['div', 'span'], class_=re.compile('source'))

                                    if url and url.startswith('http'):
                                        articles.append({
                                            'title': title,
                                            'url': url,
                                            'source': source_elem.get_text().strip() if source_elem else 'Google News',
                                            'date': time_elem.get('datetime', '') if time_elem else '',
                                            'language': 'en'
                                        })
                        except Exception:
                            continue

                    return articles
            return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ Google News: {e}")
            return []

    async def search_duckduckgo_improved(self, query, exclude_russian=True):
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://html.duckduckgo.com/html/?q={encoded_query}+news&kl=us-en"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []
                    results = soup.find_all('div', class_='result')[:12]

                    for result in results:
                        try:
                            title_elem = result.find('a', class_='result__a')
                            if title_elem:
                                title = title_elem.get_text().strip()
                                url = title_elem.get('href', '')

                                if 'duckduckgo.com' in url:
                                    match = re.search(r'uddg=([^&]+)', url)
                                    if match:
                                        url = urllib.parse.unquote(match.group(1))

                                if any(
                                    domain in url for domain in [
                                        'google.com/search',
                                        'bing.com/search',
                                        'yandex.ru/search']):
                                    continue

                                if exclude_russian and self.is_russian_domain(url):
                                    continue

                                snippet_elem = result.find('a', class_='result__snippet')

                                if url and url.startswith('http'):
                                    articles.append({
                                        'title': title,
                                        'url': url,
                                        'source': 'DuckDuckGo',
                                        'description': snippet_elem.get_text().strip()[:150] + '...' if snippet_elem else '',
                                        'language': 'en'
                                    })
                        except Exception:
                            continue

                    return articles
            return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ DuckDuckGo: {e}")
            return []

    async def universal_search(self, query, search_type="all"):
        cache_key = f"{search_type}_{query}"
        cached_results = self.get_cached_results(cache_key)
        if cached_results:
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            return cached_results

        all_results = []

        try:
            if search_type in ["all", "russian"]:
                logger.info(f"üîç –ü–æ–∏—Å–∫ –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö: {query}")

                yandex_results = await self.search_yandex_news_direct(f"{query} –≠–ü–†")
                all_results.extend(yandex_results)
                logger.info(f"‚úÖ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–∏: {len(yandex_results)} —Å—Ç–∞—Ç–µ–π")

                bing_ru_results = await self.search_bing_news_improved(f"{query} –≠–ü–†", 'ru-RU')
                all_results.extend(bing_ru_results)
                logger.info(f"‚úÖ Bing –†–æ—Å—Å–∏—è: {len(bing_ru_results)} —Å—Ç–∞—Ç–µ–π")

            if search_type in ["all", "international"]:
                logger.info(f"üåç –ü–æ–∏—Å–∫ –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö: {query}")

                translated_query = await self.translate_query(query)
                logger.info(f"üåç –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {translated_query}")

                google_results = await self.search_google_news_english(translated_query)
                all_results.extend(google_results)
                logger.info(f"‚úÖ Google News: {len(google_results)} —Å—Ç–∞—Ç–µ–π")

                bing_en_results = await self.search_bing_news_improved(translated_query, 'en-US', exclude_russian=True)
                all_results.extend(bing_en_results)
                logger.info(f"‚úÖ Bing International: {len(bing_en_results)} —Å—Ç–∞—Ç–µ–π")

                duckduckgo_results = await self.search_duckduckgo_improved(translated_query, exclude_russian=True)
                all_results.extend(duckduckgo_results)
                logger.info(f"‚úÖ DuckDuckGo: {len(duckduckgo_results)} —Å—Ç–∞—Ç–µ–π")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º –ø–æ–∏—Å–∫–µ: {e}")

        filtered_results = []
        for result in all_results:
            if result and result.get('url'):
                url = result['url'].lower()
                if any(search_domain in url for search_domain in [
                    'google.com/search',
                    'bing.com/search',
                    'yandex.ru/search',
                    'news.google.com',
                    'news.yandex.ru/yandsearch'
                ]):
                    continue
                
                if search_type == "international" and self.is_russian_domain(url):
                    continue
                    
                if url.startswith('http') and len(url) > 20:
                    filtered_results.append(result)

        seen_urls = set()
        unique_results = []
        for result in filtered_results:
            if result['url'] not in seen_urls:
                seen_urls.add(result['url'])
                unique_results.append(result)

        self.set_cached_results(cache_key, unique_results[:10])

        logger.info(f"üìä –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(unique_results)} —Å—Ç–∞—Ç–µ–π")
        return unique_results[:10]

    async def get_fresh_news_today(self):
        cache_key = "fresh_news_today"
        cached_results = self.get_cached_results(cache_key)
        if cached_results:
            return cached_results

        logger.info("üîç –ü–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è...")

        today_queries = [
            "–≠–ü–† —Å–µ–≥–æ–¥–Ω—è",
            "–≠–ü–† –Ω–æ–≤–æ—Å—Ç–∏ —Å–µ–≥–æ–¥–Ω—è",
            "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞ —Å–µ–≥–æ–¥–Ω—è",
            "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º –Ω–æ–≤–æ—Å—Ç–∏"
        ]

        all_articles = []

        for query in today_queries:
            try:
                logger.info(f"üì¢ –ü–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {query}")

                yandex_results = await self.search_yandex_news_direct(query)
                bing_results = await self.search_bing_news_improved(query, 'ru-RU')

                all_articles.extend(yandex_results)
                all_articles.extend(bing_results)

                await asyncio.sleep(1)

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
                continue

        filtered_articles = []
        for article in all_articles:
            if article and article.get('url'):
                url = article['url'].lower()
                if not any(search_domain in url for search_domain in [
                    'google.com/search', 'bing.com/search', 'yandex.ru/search'
                ]) and url.startswith('http') and len(url) > 20:
                    filtered_articles.append(article)

        seen_urls = set()
        unique_articles = []
        for article in filtered_articles:
            if article['url'] not in seen_urls:
                seen_urls.add(article['url'])
                unique_articles.append(article)

        if len(unique_articles) < 4:
            logger.info("üîç –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π...")
            backup_queries = ["–≠–ü–†", "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞ –†–æ—Å—Å–∏—è"]
            for query in backup_queries:
                try:
                    backup_results = await self.universal_search(query, "all")
                    for article in backup_results:
                        if article['url'] not in seen_urls:
                            seen_urls.add(article['url'])
                            unique_articles.append(article)
                    await asyncio.sleep(1)
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")

        final_articles = unique_articles[:8]
        self.set_cached_results(cache_key, final_articles)

        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(final_articles)}")
        return final_articles

    async def close(self):
        if self.session:
            await self.session.close()

news_searcher = ImprovedNewsSearcher()

@dp.message(Command("start"))
async def cmd_start(message: types.Message):
    await message.answer(
        "üåê –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –æ–± –≠–ü–†\n\n"
        "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π ‚Äì —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏\n"
        "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ‚Äì —Ç–æ–ª—å–∫–æ –∑–∞—Ä—É–±–µ–∂–Ω—ã–µ –°–ú–ò\n"
        "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ ‚Äì –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏\n"
        "üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ ‚Äì –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n\n"
        "–ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ —á—Ç–æ –∏—â–µ—Ç–µ!",
        reply_markup=main_keyboard
    )

@dp.message(Command("help"))
async def cmd_help(message: types.Message):
    help_text = """
üìñ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –æ–± –≠–ü–†

üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π ‚Äì —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ‚Äì —Ç–æ–ª—å–∫–æ –∑–∞—Ä—É–±–µ–∂–Ω—ã–µ –°–ú–ò
‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ ‚Äì –ø–æ–∏—Å–∫ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è
üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ ‚Äì –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º

üí° –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:
    ‚Ä¢ –≠–ü–† –≤ —Ñ–∏–Ω–∞–Ω—Å–∞—Ö
‚Ä¢ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞
‚Ä¢ –Ω–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –≠–ü–†
‚Ä¢ Russia fintech regulation

‚ö° –ö–Ω–æ–ø–∫–∞ '–°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏' –∏—â–µ—Ç —Å–∞–º—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è!
"""
    await message.answer(help_text)

@dp.message(lambda message: message.text == "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π")
async def search_epr_news(message: types.Message):
    await message.answer("üîç –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π:")

@dp.message(lambda message: message.text == "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
async def international_sources(message: types.Message):
    await message.answer("üåç –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö:")

@dp.message(lambda message: message.text == "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏")
async def fresh_news(message: types.Message):
    await message.answer("‚ö° –ò—â—É —Å–∞–º—ã–µ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏")

    try:
        articles = await news_searcher.get_fresh_news_today()

        if articles:
            response = "‚ö° –°–∞–º—ã–µ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏:\n\n"

            for i, article in enumerate(articles, 1):
                response += f"{i}. {article['title']}\n"
                response += f"   üì∞ {article['source']}\n"
                if article.get('date'):
                    formatted_date = format_date(article['date'])
                    if formatted_date:
                        response += f"   üìÖ {formatted_date}\n"
                response += f"   üîó {article['url']}\n\n"

                if len(response) > 3500:
                    response += "... (–ø–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ —Å—Ç–∞—Ç—å–∏)"
                    break

        else:
            response = "üòî –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è.\n\n"
            response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É."

        await message.answer(response)

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
        await message.answer("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")

@dp.message(lambda message: message.text == "üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫")
async def quick_search(message: types.Message):
    await message.answer("üìä –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")

@dp.message()
async def handle_text(message: types.Message):
    user_text = message.text.strip()

    buttons = [
        "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π",
        "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏",
        "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏",
        "üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫"]
    if user_text.startswith('/') or user_text in buttons:
        return

    await message.answer(f"üîç –ò—â—É –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{user_text}'...")

    try:
        if any(word in user_text.lower()
           for word in ['russia', 'russian', 'international']):
            search_type = "international"
            response_note = "üåç –ü–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö\n"
        else:
            search_type = "all"
            response_note = "üîç –ü–æ–∏—Å–∫ –ø–æ —Ä—É—Å—Å–∫–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º\n"

        articles = await news_searcher.universal_search(user_text, search_type)

        if articles:
            russian_articles = [a for a in articles if a.get('language') == 'ru']
            english_articles = [a for a in articles if a.get('language') == 'en']

            response = f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ '{user_text}':\n\n"

            if russian_articles and search_type != "international":
                response += "üá∑üá∫ –†–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:\n\n"
                for i, article in enumerate(russian_articles[:4], 1):
                    response += f"{i}. {article['title']}\n"
                    response += f"   üì∞ {article['source']}\n"
                    if article.get('date'):
                        formatted_date = format_date(article['date'])
                        if formatted_date:
                            response += f"   üìÖ {formatted_date}\n"
                    response += f"   üîó {article['url']}\n\n"

            if english_articles and search_type == "international":
                response += "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:\n\n"
                for i, article in enumerate(english_articles[:4], 1):
                    response += f"{i}. {article['title']}\n"
                    response += f"   üì∞ {article['source']}\n"
                    if article.get('date'):
                        formatted_date = format_date(article['date'])
                        if formatted_date:
                            response += f"   üìÖ {formatted_date}\n"
                    response += f"   üîó {article['url']}\n\n"

            response += f"üìä –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}"

        else:
            response = f"üòî –ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_text}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π.\n\n"
            response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞."

        await message.answer(response)

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        await message.answer(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –∑–∞–ø—Ä–æ—Å.")

async def main():
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –±–æ—Ç–∞...")

    try:
        await bot.delete_webhook(drop_pending_updates=True)
        await dp.start_polling(bot)
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞: {e}")
    finally:
        await news_searcher.close()

if __name__ == "__main__":
    asyncio.run(main())
