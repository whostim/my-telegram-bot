from datetime import datetime, timedelta
import os
import logging
import asyncio
import aiohttp
from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
from aiogram.types import ReplyKeyboardMarkup, KeyboardButton
from dotenv import load_dotenv
import urllib.parse
from bs4 import BeautifulSoup
import json
import re
import random
import sys
import atexit
import signal
from aiohttp import web
import threading

# ===== –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –õ–û–ì–ì–ò–†–û–í–ê–ù–ò–Ø =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('bot.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ===== –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ó–ê–©–ò–¢–ê –û–¢ –ú–ù–û–ñ–ï–°–¢–í–ï–ù–ù–û–ì–û –ó–ê–ü–£–°–ö–ê =====
class GracefulShutdown:
    def __init__(self):
        self.shutdown = False
        self._setup_signal_handlers()
    
    def _setup_signal_handlers(self):
        signal.signal(signal.SIGTERM, self._handle_signal)
        signal.signal(signal.SIGINT, self._handle_signal)
        signal.signal(signal.SIGUSR1, signal.SIG_IGN)
    
    def _handle_signal(self, signum, frame):
        logger.info(f"üì¢ –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª {signum}, –Ω–∞—á–∏–Ω–∞—é graceful shutdown...")
        self.shutdown = True

def cleanup_lock():
    try:
        lock_file = "/tmp/telegram-bot.lock"
        if os.path.exists(lock_file):
            os.remove(lock_file)
            logger.info("üîì –§–∞–π–ª –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —É–¥–∞–ª–µ–Ω")
    except Exception as e:
        logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ lock-—Ñ–∞–π–ª–∞: {e}")

def check_single_instance():
    try:
        lock_file = "/tmp/telegram-bot.lock"
        if os.path.exists(lock_file):
            with open(lock_file, 'r') as f:
                old_pid = f.read().strip()
            try:
                os.kill(int(old_pid), 0)
                logger.info(f"‚ùå –ë–æ—Ç —É–∂–µ –∑–∞–ø—É—â–µ–Ω –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ {old_pid}. –ó–∞–≤–µ—Ä—à–∞–µ–º.")
                sys.exit(1)
            except (ProcessLookupError, ValueError):
                logger.info("üîÑ –°—Ç–∞—Ä—ã–π –ø—Ä–æ—Ü–µ—Å—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∑–∞–ø—É—Å–∫")
                os.remove(lock_file)
        
        with open(lock_file, 'w') as f:
            f.write(str(os.getpid()))
        
        atexit.register(cleanup_lock)
        logger.info(f"üîí –§–∞–π–ª –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —Å–æ–∑–¥–∞–Ω (PID: {os.getpid()})")
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏: {e}")

check_single_instance()

# ===== –£–õ–£–ß–®–ï–ù–ù–´–ô HEALTH CHECK SERVER =====
class HealthServer:
    def __init__(self, port=8080):
        self.port = port
        self.app = web.Application()
        self.app.router.add_get('/health', self.health_check)
        self.app.router.add_get('/readiness', self.readiness_check)
        self.runner = None
        self.site = None
    
    async def health_check(self, request):
        return web.Response(text='OK', status=200)
    
    async def readiness_check(self, request):
        return web.Response(text='READY', status=200)
    
    async def start(self):
        try:
            self.runner = web.AppRunner(self.app)
            await self.runner.setup()
            self.site = web.TCPSite(self.runner, '0.0.0.0', self.port)
            await self.site.start()
            logger.info(f"üåê Health server started on port {self.port}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ health server: {e}")
    
    async def stop(self):
        try:
            if self.site:
                await self.site.stop()
            if self.runner:
                await self.runner.cleanup()
            logger.info("‚úÖ Health server –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ health server: {e}")

# ===== –ó–ê–ì–†–£–ó–ö–ê –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò =====
load_dotenv()

BOT_TOKEN = os.getenv('BOT_TOKEN')
if not BOT_TOKEN:
    logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
    sys.exit(1)

# ===== –£–õ–£–ß–®–ï–ù–ù–´–ô –ö–õ–ê–°–° –ü–û–ò–°–ö–ê –ù–û–í–û–°–¢–ï–ô =====
class ImprovedNewsSearcher:
    def __init__(self):
        self.session = None
        self.cache = {}
        self.cache_timeout = 300
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–æ–º–µ–Ω–æ–≤ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        self.russian_domains = [
            'cbr.ru', 'banki.ru', 'government.ru', 'kremlin.ru', 'minfin.ru',  # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            'rbc.ru', 'vedomosti.ru', 'kommersant.ru', 'ria.ru', 'tass.ru',
            'rt.com', 'lenta.ru', 'gazeta.ru', 'iz.ru', 'mk.ru', 'aif.ru',
            'rg.ru', 'vesti.ru', 'newsru.com', 'fontanka.ru', 'ng.ru',
            'echo.msk.ru', 'bfm.ru', 'forbes.ru', 'vc.ru', 'rb.ru',
            'yandex.ru', 'mail.ru', 'rambler.ru',
            'sputniknews.com', 'rbth.com', 'russian.rt.com', 'themoscowtimes.com'
        ]
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)
        self.priority_domains = ['cbr.ru', 'banki.ru', 'government.ru', 'kremlin.ru', 'minfin.ru']

    async def get_session(self):
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=30, connect=10, sock_read=20)
            connector = aiohttp.TCPConnector(limit=10, keepalive_timeout=30)
            self.session = aiohttp.ClientSession(timeout=timeout, connector=connector)
        return self.session

    def get_cached_results(self, query):
        cache_key = f"search_{hash(query)}"
        if cache_key in self.cache:
            cache_time, results = self.cache[cache_key]
            if datetime.now() - cache_time < timedelta(seconds=self.cache_timeout):
                return results
        return None

    def set_cached_results(self, query, results):
        cache_key = f"search_{hash(query)}"
        self.cache[cache_key] = (datetime.now(), results)

    def is_russian_domain(self, url):
        try:
            domain = urllib.parse.urlparse(url).netloc.lower()
            return any(russian_domain in domain for russian_domain in self.russian_domains)
        except BaseException:
            return False

    def is_priority_domain(self, url):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –¥–æ–º–µ–Ω –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–º (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)"""
        try:
            domain = urllib.parse.urlparse(url).netloc.lower()
            return any(priority_domain in domain for priority_domain in self.priority_domains)
        except BaseException:
            return False

    def is_russian_text(self, text):
        return bool(re.search('[–∞-—è–ê-–Ø]', text))

    def normalize_title(self, title):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if not title:
            return ""
        
        normalized = title.lower()
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        normalized = re.sub(r'[^\w\s]', '', normalized)
        
        stop_words = ['–Ω–æ–≤–æ—Å—Ç–∏', '—Å–µ–≥–æ–¥–Ω—è', '—Å–µ–π—á–∞—Å', '–ø–æ—Å–ª–µ–¥–Ω–∏–µ', '—Å–≤–µ–∂–∏–µ']
        words = normalized.split()
        filtered_words = [word for word in words if word not in stop_words]
        
        return ' '.join(filtered_words)

    def is_duplicate_article(self, article, existing_articles, similarity_threshold=0.8):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç–∞—Ç—å—è –¥—É–±–ª–∏–∫–∞—Ç–æ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö"""
        if not article or not existing_articles:
            return False
        
        new_title_normalized = self.normalize_title(article.get('title', ''))
        new_url = article.get('url', '')
        
        for existing in existing_articles:
            existing_title_normalized = self.normalize_title(existing.get('title', ''))
            existing_url = existing.get('url', '')
            
            if self.is_same_domain(new_url, existing_url):
                if self.calculate_similarity(new_title_normalized, existing_title_normalized) > similarity_threshold:
                    return True
            
            if self.calculate_similarity(new_title_normalized, existing_title_normalized) > 0.9:
                return True
        
        return False

    def is_same_domain(self, url1, url2):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç –ª–∏ URL –æ–¥–Ω–æ–º—É –¥–æ–º–µ–Ω—É"""
        try:
            domain1 = urllib.parse.urlparse(url1).netloc
            domain2 = urllib.parse.urlparse(url2).netloc
            return domain1 == domain2
        except:
            return False

    def calculate_similarity(self, text1, text2):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
        if not text1 or not text2:
            return 0
        
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 or not words2:
            return 0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0

    def truncate_query_for_search(self, query, max_words=8):
        """–û–±—Ä–µ–∑–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞"""
        words = query.split()
        if len(words) > max_words:
            truncated = ' '.join(words[:max_words])
            logger.info(f"üîß –û–±—Ä–µ–∑–∞–Ω –¥–ª–∏–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{query}' -> '{truncated}'")
            return truncated
        return query

    async def correct_spelling_auto(self, text):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏—è —á–µ—Ä–µ–∑ Yandex Speller API"""
        try:
            if not self.is_russian_text(text):
                return text
                
            session = await self.get_session()
            encoded_text = urllib.parse.quote(text)
            
            url = f"https://speller.yandex.net/services/spellservice.json/checkText?text={encoded_text}&lang=ru,en"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    corrections = await response.json()
                    
                    if corrections:
                        corrected_text = text
                        for correction in reversed(corrections):
                            if correction.get('s'):
                                fixed_word = correction['s'][0]
                                wrong_word = correction['word']
                                corrected_text = corrected_text.replace(wrong_word, fixed_word)
                        
                        logger.info(f"üìù –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏–µ: '{text}' -> '{corrected_text}'")
                        return corrected_text
                    
            return text
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏—è: {e}")
            return text

    async def translate_to_english_auto(self, text):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π"""
        try:
            if not self.is_russian_text(text):
                return text
                
            session = await self.get_session()
            encoded_text = urllib.parse.quote(text)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π API –ø–µ—Ä–µ–≤–æ–¥–∞ (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à –∫–ª—é—á –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
            url = f"https://api.mymemory.translated.net/get?q={encoded_text}&langpair=ru|en"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    if data.get('responseData', {}).get('translatedText'):
                        translated = data['responseData']['translatedText']
                        logger.info(f"üåç –ê–≤—Ç–æ–ø–µ—Ä–µ–≤–æ–¥: '{text}' -> '{translated}'")
                        return translated
            
            return text
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
            return text

    async def prepare_international_query(self, query):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        try:
            logger.info(f"üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
            # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            truncated_query = self.truncate_query_for_search(query)
            corrected_query = await self.correct_spelling_auto(truncated_query)
            translated_query = await self.translate_to_english_auto(corrected_query)
            logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{translated_query}'")
            return translated_query
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return self.truncate_query_for_search(query)

    async def search_yandex_news_direct(self, query):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç—è—Ö —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        try:
            # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –Ø–Ω–¥–µ–∫—Å
            truncated_query = self.truncate_query_for_search(query, max_words=6)
            session = await self.get_session()
            encoded_query = urllib.parse.quote(truncated_query)
            url = f"https://yandex.ru/news/search?text={encoded_query}"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers, timeout=15) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []
                    news_cards = soup.find_all('article', class_='mg-card')[:10]

                    for card in news_cards:
                        try:
                            title_elem = card.find('h2', class_='mg-card__title') or card.find('a', class_='mg-card__link')
                            if not title_elem:
                                continue

                            title = title_elem.get_text().strip()
                            link = title_elem.get('href', '')

                            if link.startswith('https://news.yandex.ru/yandsearch?'):
                                match = re.search(r'cl4url=([^&]+)', link)
                                if match:
                                    link = urllib.parse.unquote(match.group(1))
                            elif link.startswith('/'):
                                link = f"https://yandex.ru{link}"

                            if link and not any(
                                domain in link for domain in [
                                    'google.com/search',
                                    'yandex.ru/search']):
                                articles.append({
                                    'title': title,
                                    'url': link,
                                    'language': 'ru',
                                    'priority': self.is_priority_domain(link)  # –û—Ç–º–µ—á–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                                })
                        except Exception as e:
                            continue

                    return articles
            return []
        except asyncio.TimeoutError:
            logger.warning("‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç—è—Ö")
            return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []

    async def search_bing_news_improved(self, query, market='ru-RU', exclude_russian=False):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤ Bing News"""
        try:
            # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            truncated_query = self.truncate_query_for_search(query)
            session = await self.get_session()
            encoded_query = urllib.parse.quote(truncated_query)
            
            if market == 'en-US':
                url = f"https://www.bing.com/news/search?q={encoded_query}&cc=us&setlang=en"
            else:
                url = f"https://www.bing.com/news/search?q={encoded_query}&cc={market}"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept-Language': 'en-US,en;q=0.9' if market == 'en-US' else 'ru-RU,ru;q=0.9',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers, timeout=15) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []

                    news_cards = soup.find_all('div', class_='news-card')[:10]
                    if not news_cards:
                        news_cards = soup.find_all('div', class_='tile')[:10]
                    if not news_cards:
                        news_cards = soup.find_all('article')[:10]

                    for card in news_cards:
                        try:
                            title_elem = (card.find('a', class_='title') or
                                        card.find('a', class_=re.compile('title')) or
                                        card.find('h2') or
                                        card.find('h3') or
                                        card.find('a', attrs={'href': True}))

                            if title_elem and title_elem.get('href'):
                                title = title_elem.get_text().strip()
                                url = title_elem.get('href')

                                if url.startswith('/'):
                                    url = f"https://www.bing.com{url}"

                                if 'bing.com/news/search' in url:
                                    continue

                                if exclude_russian and self.is_russian_domain(url):
                                    continue

                                if exclude_russian and self.is_russian_text(title):
                                    continue

                                if url and not any(
                                    search_domain in url for search_domain in [
                                        'google.com/search',
                                        'bing.com/search']):
                                    articles.append({
                                        'title': title,
                                        'url': url,
                                        'language': 'en' if market == 'en-US' else 'ru',
                                        'priority': self.is_priority_domain(url)
                                    })
                        except Exception:
                            continue

                    return articles
            return []
        except asyncio.TimeoutError:
            logger.warning("‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ Bing News")
            return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ Bing News: {e}")
            return []

    async def search_google_news_english(self, query, exclude_russian=True):
        try:
            truncated_query = self.truncate_query_for_search(query)
            session = await self.get_session()
            encoded_query = urllib.parse.quote(truncated_query)
            url = f"https://news.google.com/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers, timeout=15) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []
                    news_cards = soup.find_all('article')[:10]

                    for card in news_cards:
                        try:
                            title_elem = card.find('h3') or card.find('h4') or card.find('a', attrs={'href': True})
                            if title_elem:
                                title = title_elem.get_text().strip()
                                link_elem = title_elem.find_parent('a') if title_elem.name != 'a' else title_elem
                                if link_elem and link_elem.get('href'):
                                    url = link_elem.get('href')
                                    if url.startswith('./'):
                                        url = f"https://news.google.com{url[1:]}"
                                    
                                    if 'news.google.com' in url:
                                        continue

                                    if exclude_russian and (self.is_russian_domain(url) or self.is_russian_text(title)):
                                        continue

                                    if url and url.startswith('http'):
                                        articles.append({
                                            'title': title,
                                            'url': url,
                                            'language': 'en'
                                        })
                        except Exception:
                            continue

                    return articles
            return []
        except asyncio.TimeoutError:
            logger.warning("‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ Google News")
            return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ Google News: {e}")
            return []

    async def search_duckduckgo_improved(self, query, exclude_russian=True):
        try:
            truncated_query = self.truncate_query_for_search(query)
            session = await self.get_session()
            encoded_query = urllib.parse.quote(truncated_query)
            url = f"https://html.duckduckgo.com/html/?q={encoded_query}+news&kl=us-en"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers, timeout=15) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []
                    results = soup.find_all('div', class_='result')[:10]

                    for result in results:
                        try:
                            title_elem = result.find('a', class_='result__a')
                            if title_elem:
                                title = title_elem.get_text().strip()
                                url = title_elem.get('href', '')

                                if 'duckduckgo.com' in url:
                                    match = re.search(r'uddg=([^&]+)', url)
                                    if match:
                                        url = urllib.parse.unquote(match.group(1))

                                if any(
                                    domain in url for domain in [
                                        'google.com/search',
                                        'bing.com/search',
                                        'yandex.ru/search']):
                                    continue

                                if exclude_russian and (self.is_russian_domain(url) or self.is_russian_text(title)):
                                    continue

                                if url and url.startswith('http'):
                                    articles.append({
                                        'title': title,
                                        'url': url,
                                        'language': 'en'
                                    })
                        except Exception:
                            continue

                    return articles
            return []
        except asyncio.TimeoutError:
            logger.warning("‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ DuckDuckGo")
            return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ DuckDuckGo: {e}")
            return []

    async def search_only_russian(self, query):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¢–û–õ–¨–ö–û –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö"""
        cache_key = f"russian_only_{hash(query)}"
        cached_results = self.get_cached_results(cache_key)
        if cached_results:
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (—Ç–æ–ª—å–∫–æ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ)")
            return cached_results

        logger.info(f"üîç –ü–æ–∏—Å–∫ –¢–û–õ–¨–ö–û –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö: {query}")

        all_results = []

        try:
            # –¢–æ–ª—å–∫–æ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            yandex_results = await self.search_yandex_news_direct(query)
            all_results.extend(yandex_results)
            logger.info(f"‚úÖ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–∏: {len(yandex_results)} —Å—Ç–∞—Ç–µ–π")

            bing_ru_results = await self.search_bing_news_improved(query, 'ru-RU')
            all_results.extend(bing_ru_results)
            logger.info(f"‚úÖ Bing –†–æ—Å—Å–∏—è: {len(bing_ru_results)} —Å—Ç–∞—Ç–µ–π")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–æ–∏—Å–∫–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")

        # –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        filtered_results = []
        seen_titles = set()
        
        # –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)
        priority_articles = []
        regular_articles = []
        
        for result in all_results:
            if result and result.get('url'):
                url = result['url'].lower()
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
                if any(search_domain in url for search_domain in [
                    'google.com/search',
                    'bing.com/search', 
                    'yandex.ru/search',
                    'news.google.com',
                    'news.yandex.ru/yandsearch'
                ]):
                    continue
                
                # –í–ê–ñ–ù–û: –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ä–æ—Å—Å–∏–π—Å–∫–∏–π –¥–æ–º–µ–Ω
                if not self.is_russian_domain(url):
                    continue
                    
                if url.startswith('http') and len(url) > 20:
                    normalized_title = self.normalize_title(result.get('title', ''))
                    if normalized_title and normalized_title not in seen_titles and len(normalized_title) >= 15:
                        seen_titles.add(normalized_title)
                        
                        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∏ –æ–±—ã—á–Ω—ã–µ
                        if result.get('priority') or self.is_priority_domain(url):
                            priority_articles.append(result)
                        else:
                            regular_articles.append(result)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –ø–µ—Ä–≤—ã–º–∏
        filtered_results = priority_articles + regular_articles
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, –Ω–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –º–µ—Å—Ç–æ –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö
        final_results = filtered_results[:8]
        
        self.set_cached_results(cache_key, final_results)
        logger.info(f"üìä –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(final_results)} —Å—Ç–∞—Ç–µ–π (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö: {len(priority_articles)})")
        return final_results

    async def universal_search(self, query, search_type="all"):
        cache_key = f"{search_type}_{query}"
        cached_results = self.get_cached_results(cache_key)
        if cached_results:
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            return cached_results

        all_results = []

        try:
            if search_type in ["all", "russian"]:
                logger.info(f"üîç –ü–æ–∏—Å–∫ –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö: {query}")

                yandex_results = await self.search_yandex_news_direct(query)
                all_results.extend(yandex_results)
                logger.info(f"‚úÖ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–∏: {len(yandex_results)} —Å—Ç–∞—Ç–µ–π")

                bing_ru_results = await self.search_bing_news_improved(query, 'ru-RU')
                all_results.extend(bing_ru_results)
                logger.info(f"‚úÖ Bing –†–æ—Å—Å–∏—è: {len(bing_ru_results)} —Å—Ç–∞—Ç–µ–π")

            if search_type in ["all", "international"]:
                logger.info(f"üåç –ü–æ–∏—Å–∫ –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö: {query}")

                international_query = await self.prepare_international_query(query)
                logger.info(f"üåç –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {international_query}")

                google_results = await self.search_google_news_english(international_query, exclude_russian=True)
                all_results.extend(google_results)
                logger.info(f"‚úÖ Google News: {len(google_results)} —Å—Ç–∞—Ç–µ–π")

                bing_en_results = await self.search_bing_news_improved(international_query, 'en-US', exclude_russian=True)
                all_results.extend(bing_en_results)
                logger.info(f"‚úÖ Bing International: {len(bing_en_results)} —Å—Ç–∞—Ç–µ–π")

                duckduckgo_results = await self.search_duckduckgo_improved(international_query, exclude_russian=True)
                all_results.extend(duckduckgo_results)
                logger.info(f"‚úÖ DuckDuckGo: {len(duckduckgo_results)} —Å—Ç–∞—Ç–µ–π")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º –ø–æ–∏—Å–∫–µ: {e}")

        # –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        filtered_results = []
        seen_titles = set()
        priority_articles = []
        regular_articles = []
        
        for result in all_results:
            if result and result.get('url'):
                url = result['url'].lower()
                
                if any(search_domain in url for search_domain in [
                    'google.com/search',
                    'bing.com/search', 
                    'yandex.ru/search',
                    'news.google.com',
                    'news.yandex.ru/yandsearch'
                ]):
                    continue
                
                if search_type == "international":
                    if (self.is_russian_domain(url) or 
                        self.is_russian_text(result.get('title', ''))):
                        continue
                        
                if url.startswith('http') and len(url) > 20:
                    normalized_title = self.normalize_title(result.get('title', ''))
                    if normalized_title and normalized_title not in seen_titles and len(normalized_title) >= 15:
                        seen_titles.add(normalized_title)
                        
                        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∏ –æ–±—ã—á–Ω—ã–µ
                        if result.get('priority') or (search_type != "international" and self.is_priority_domain(url)):
                            priority_articles.append(result)
                        else:
                            regular_articles.append(result)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –ø–µ—Ä–≤—ã–º–∏
        filtered_results = priority_articles + regular_articles
        
        self.set_cached_results(cache_key, filtered_results[:12])
        logger.info(f"üìä –ò—Ç–æ–≥–æ–≤—ã–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(filtered_results)} —Å—Ç–∞—Ç–µ–π (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö: {len(priority_articles)})")
        return filtered_results[:12]

    async def get_fresh_news_today(self):
        cache_key = "fresh_news_today"
        cached_results = self.get_cached_results(cache_key)
        if cached_results:
            return cached_results

        logger.info("üîç –ü–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è...")

        today_queries = [
            "–≠–ü–† —Å–µ–≥–æ–¥–Ω—è",
            "–≠–ü–† –Ω–æ–≤–æ—Å—Ç–∏ —Å–µ–≥–æ–¥–Ω—è", 
            "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞ —Å–µ–≥–æ–¥–Ω—è",
            "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º –Ω–æ–≤–æ—Å—Ç–∏",
            "—Ü–∏—Ñ—Ä–æ–≤—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∞–∫—Ç–∏–≤—ã",
            "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã–µ –ø–µ—Å–æ—á–Ω–∏—Ü—ã –†–æ—Å—Å–∏—è",
            "–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏ –≠–ü–†",
            "–¶–ë –†–§ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º"
        ]

        all_articles = []

        for query in today_queries:
            try:
                logger.info(f"üì¢ –ü–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {query}")

                yandex_results = await self.search_yandex_news_direct(query)
                bing_results = await self.search_bing_news_improved(query, 'ru-RU')

                for article in yandex_results + bing_results:
                    if not self.is_duplicate_article(article, all_articles):
                        all_articles.append(article)

                await asyncio.sleep(1)

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
                continue

        filtered_articles = []
        seen_titles = set()
        priority_articles = []
        regular_articles = []
        
        for article in all_articles:
            if article and article.get('url'):
                url = article['url'].lower()
                
                if any(search_domain in url for search_domain in [
                    'google.com/search', 'bing.com/search', 'yandex.ru/search'
                ]) or len(url) < 20:
                    continue
                
                normalized_title = self.normalize_title(article.get('title', ''))
                
                if len(normalized_title) < 15:
                    continue
                    
                if normalized_title not in seen_titles:
                    seen_titles.add(normalized_title)
                    
                    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∏ –æ–±—ã—á–Ω—ã–µ
                    if article.get('priority') or self.is_priority_domain(url):
                        priority_articles.append(article)
                    else:
                        regular_articles.append(article)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        filtered_articles = priority_articles + regular_articles

        if len(filtered_articles) < 4:
            logger.info("üîç –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π...")
            backup_queries = [
                "–≠–ü–†", 
                "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞ –†–æ—Å—Å–∏—è",
                "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º",
                "—Ü–∏—Ñ—Ä–æ–≤–∞—è –≤–∞–ª—é—Ç–∞ –¶–ë",
                "–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è"
            ]
            
            for query in backup_queries:
                try:
                    backup_results = await self.universal_search(query, "all")
                    for article in backup_results:
                        normalized_title = self.normalize_title(article.get('title', ''))
                        if (normalized_title not in seen_titles and 
                            len(normalized_title) >= 15 and
                            not self.is_duplicate_article(article, filtered_articles)):
                            seen_titles.add(normalized_title)
                            if article.get('priority') or self.is_priority_domain(article.get('url', '')):
                                priority_articles.append(article)
                            else:
                                regular_articles.append(article)
                    await asyncio.sleep(1)
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")

            # –ü–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º filtered_articles
            filtered_articles = priority_articles + regular_articles

        def relevance_score(article):
            title = article.get('title', '').lower()
            score = 0
            keywords = ['—ç–ø—Ä', '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º', '—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞', 
                       '—Ü–∏—Ñ—Ä–æ–≤–∞—è –≤–∞–ª—é—Ç–∞', '—Ü–± —Ä—Ñ', '—Ñ–∏–Ω—Ç–µ—Ö', '–±–ª–æ–∫—á–µ–π–Ω', '–±–∞–Ω–∫ —Ä–æ—Å—Å–∏–∏', 'cbr.ru']
            
            for keyword in keywords:
                if keyword in title:
                    score += 1
                    
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∞–ª–ª—ã –∑–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            if article.get('priority') or self.is_priority_domain(article.get('url', '')):
                score += 2
                
            return score

        filtered_articles.sort(key=relevance_score, reverse=True)
        final_articles = filtered_articles[:8]

        self.set_cached_results(cache_key, final_articles)

        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(final_articles)} (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö: {len(priority_articles)})")
        return final_articles

    async def close(self):
        if self.session and not self.session.closed:
            await self.session.close()

# ===== –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï =====
main_keyboard = ReplyKeyboardMarkup(
    keyboard=[
        [
            KeyboardButton(text="üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π"), 
            KeyboardButton(text="üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
        ],
        [
            KeyboardButton(text="‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏"), 
            KeyboardButton(text="üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫")
        ]
    ], 
    resize_keyboard=True
)

user_search_type = {}

# ===== –£–õ–£–ß–®–ï–ù–ù–´–ô ROBUST BOT CLASS =====
class RobustBot:
    def __init__(self):
        self.bot = Bot(token=BOT_TOKEN)
        self.dp = Dispatcher()
        self.news_searcher = ImprovedNewsSearcher()
        self._is_running = False
        self._shutdown_event = asyncio.Event()
        self.setup_handlers()
    
    def setup_handlers(self):
        @self.dp.message(Command("start"))
        async def cmd_start(message: types.Message):
            await message.answer(
                "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –æ–± –≠–ü–†\n\n"
                "üîç <b>–ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π</b> ‚Äì —Ç–æ–ª—å–∫–æ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏\n"
                "üåç <b>–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏</b> ‚Äì —Ç–æ–ª—å–∫–æ –∑–∞—Ä—É–±–µ–∂–Ω—ã–µ –°–ú–ò\n"  
                "‚ö° <b>–°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏</b> ‚Äì –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è\n"
                "üìä <b>–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫</b> ‚Äì —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏\n\n"
                "üí° <b>–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:</b>\n"
                "‚Ä¢ –≠–ü–† –≤ —Ñ–∏–Ω–∞–Ω—Å–∞—Ö\n‚Ä¢ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞\n‚Ä¢ —Ü–∏—Ñ—Ä–æ–≤—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∞–∫—Ç–∏–≤—ã\n‚Ä¢ –ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏ –≠–ü–†\n\n"
                "–ù–∞–ø–∏—à–∏—Ç–µ /help –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏",
                reply_markup=main_keyboard,
                parse_mode="HTML"
            )

        @self.dp.message(Command("help"))
        async def cmd_help(message: types.Message):
            help_text = """
üìñ <b>–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –æ–± –≠–ü–†</b>

<b>–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:</b>
/start - –Ω–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É —Å –±–æ—Ç–æ–º
/help - –ø–æ–∫–∞–∑–∞—Ç—å —ç—Ç—É —Å–ø—Ä–∞–≤–∫—É

<b>–†–µ–∂–∏–º—ã –ø–æ–∏—Å–∫–∞:</b>
üîç <b>–ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π</b> - –¢–û–õ–¨–ö–û —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
üåç <b>–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏</b> - —Ç–æ–ª—å–∫–æ –∑–∞—Ä—É–±–µ–∂–Ω—ã–µ –°–ú–ò  
‚ö° <b>–°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏</b> - –ø–æ–∏—Å–∫ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è
üìä <b>–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫</b> - —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏

üí° <b>–ü—Ä–∏–º–µ—Ä—ã —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:</b>

<b>–ö–æ—Ä–æ—Ç–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã:</b>
‚Ä¢ –≠–ü–†
‚Ä¢ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞  
‚Ä¢ —Ü–∏—Ñ—Ä–æ–≤–∞—è –≤–∞–ª—é—Ç–∞
‚Ä¢ –ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏
‚Ä¢ —Ñ–∏–Ω—Ç–µ—Ö —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ

<b>–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã:</b>
‚Ä¢ –≠–ü–† –≤ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π —Å—Ñ–µ—Ä–µ
‚Ä¢ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º –¶–ë
‚Ä¢ –Ω–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –≠–ü–† 2024
‚Ä¢ —Ü–∏—Ñ—Ä–æ–≤—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∞–∫—Ç–∏–≤—ã –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ

<b>–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã:</b>
‚Ä¢ Russia fintech sandbox
‚Ä¢ digital financial assets Russia
‚Ä¢ Bank of Russia regulation

‚ö° <b>–°–æ–≤–µ—Ç—ã –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:</b>
‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã (2-5 —Å–ª–æ–≤)
‚Ä¢ –î–ª—è –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: "–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏ –≠–ü–†"
‚Ä¢ –£–∫–∞–∑—ã–≤–∞–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: "—Ü–∏—Ñ—Ä–æ–≤—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∞–∫—Ç–∏–≤—ã"
‚Ä¢ –î–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö: –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã

üîç <b>–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:</b>
‚Ä¢ –ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏ (cbr.ru)
‚Ä¢ –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –†–§
‚Ä¢ –ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤
‚Ä¢ –ö—Ä–µ–º–ª–∏–Ω.—Ä—É
"""
            await message.answer(help_text, parse_mode="HTML")

        @self.dp.message(lambda message: message.text == "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π")
        async def search_epr_news(message: types.Message):
            user_id = message.from_user.id
            user_search_type[user_id] = 'russian'
            await message.answer("üîç <b>–†–µ–∂–∏–º: —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏</b>\n\n–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –°–ú–ò (–Ω–∞–ø—Ä–∏–º–µ—Ä: '–≠–ü–† –ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏' –∏–ª–∏ '—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞'):", parse_mode="HTML")

        @self.dp.message(lambda message: message.text == "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
        async def international_sources(message: types.Message):
            user_id = message.from_user.id
            user_search_type[user_id] = 'international'
            await message.answer("üåç <b>–†–µ–∂–∏–º: –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏</b>\n\n–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –∑–∞—Ä—É–±–µ–∂–Ω—ã—Ö –°–ú–ò (–∑–∞–ø—Ä–æ—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–≤–µ–¥–µ—Ç—Å—è –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π):", parse_mode="HTML")

        @self.dp.message(lambda message: message.text == "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏")
        async def fresh_news(message: types.Message):
            await message.answer("‚ö° <b>–ò—â—É —Å–∞–º—ã–µ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è...</b>", parse_mode="HTML")
            try:
                articles = await self.news_searcher.get_fresh_news_today()
                if articles:
                    response = "‚ö° <b>–°–∞–º—ã–µ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏:</b>\n\n"
                    
                    # –û—Ç–¥–µ–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
                    priority_articles = [a for a in articles if a.get('priority') or self.news_searcher.is_priority_domain(a.get('url', ''))]
                    regular_articles = [a for a in articles if not (a.get('priority') or self.news_searcher.is_priority_domain(a.get('url', '')))]
                    
                    if priority_articles:
                        response += "üèõÔ∏è <b>–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:</b>\n\n"
                        for i, article in enumerate(priority_articles, 1):
                            domain = urllib.parse.urlparse(article['url']).netloc
                            response += f"{i}. {article['title']}\n"
                            response += f"   üîó <i>{domain}</i>\n   {article['url']}\n\n"
                    
                    if regular_articles:
                        if priority_articles:
                            response += "üì∞ <b>–î—Ä—É–≥–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:</b>\n\n"
                        for i, article in enumerate(regular_articles, len(priority_articles) + 1):
                            domain = urllib.parse.urlparse(article['url']).netloc
                            response += f"{i}. {article['title']}\n"
                            response += f"   üîó <i>{domain}</i>\n   {article['url']}\n\n"
                            
                        if len(response) > 3500:
                            response = response[:3500] + "\n... (–ø–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ —Å—Ç–∞—Ç—å–∏)"
                else:
                    response = "üòî <b>–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è.</b>\n\n"
                    response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É."
                await message.answer(response, parse_mode="HTML")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
                await message.answer("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")

        @self.dp.message(lambda message: message.text == "üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫")
        async def quick_search(message: types.Message):
            user_id = message.from_user.id
            user_search_type[user_id] = 'quick'
            await message.answer("üìä <b>–†–µ–∂–∏–º: –±—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫</b>\n\n–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º (—Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ):", parse_mode="HTML")

        @self.dp.message()
        async def handle_text(message: types.Message):
            if self._shutdown_event.is_set():
                await message.answer("‚ùå –ë–æ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ä–µ–∂–∏–º–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
                return

            user_text = message.text.strip()
            user_id = message.from_user.id

            buttons = [
                "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π",
                "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏", 
                "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏",
                "üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫"]
            if user_text.startswith('/') or user_text in buttons:
                return

            await message.answer(f"üîç <b>–ò—â—É –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É:</b> '{user_text}'...", parse_mode="HTML")
            await self.process_search(message, user_text, user_id)

    async def process_search(self, message, user_text, user_id):
        try:
            search_type = user_search_type.pop(user_id, 'all')
            
            if search_type == 'quick':
                # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫: —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ + –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                russian_articles = await self.news_searcher.universal_search(user_text, "russian")
                international_query = await self.news_searcher.prepare_international_query(user_text)
                international_articles = await self.news_searcher.universal_search(international_query, "international")
                
                if russian_articles or international_articles:
                    response = f"üîç <b>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ</b> '{user_text}':\n\n"
                    
                    if russian_articles:
                        response += "üá∑üá∫ <b>–†–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:</b>\n\n"
                        count = 0
                        for article in russian_articles:
                            if count >= 4:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                                break
                            domain = urllib.parse.urlparse(article['url']).netloc
                            priority_indicator = "üèõÔ∏è " if article.get('priority') or self.news_searcher.is_priority_domain(article['url']) else ""
                            response += f"{priority_indicator}{article['title']}\n"
                            response += f"   üîó <i>{domain}</i>\n   {article['url']}\n\n"
                            count += 1
                    
                    if international_articles:
                        response += "üåç <b>–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:</b>\n\n"
                        count = 0
                        for article in international_articles:
                            if count >= 3:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                                break
                            domain = urllib.parse.urlparse(article['url']).netloc
                            response += f"{article['title']}\n"
                            response += f"   üîó <i>{domain}</i>\n   {article['url']}\n\n"
                            count += 1
                else:
                    response = f"üòî <b>–ü–æ –∑–∞–ø—Ä–æ—Å—É</b> '{user_text}' <b>–Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π.</b>\n\n"
                    response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã."
                    
            elif search_type == 'international':
                # –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                international_query = await self.news_searcher.prepare_international_query(user_text)
                articles = await self.news_searcher.universal_search(international_query, "international")
                
                if articles:
                    response = f"üåç <b>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ</b> '{user_text}':\n\n"
                    for i, article in enumerate(articles[:6], 1):
                        domain = urllib.parse.urlparse(article['url']).netloc
                        response += f"{i}. {article['title']}\n"
                        response += f"   üîó <i>{domain}</i>\n   {article['url']}\n\n"
                else:
                    response = f"üòî <b>–ü–æ –∑–∞–ø—Ä–æ—Å—É</b> '{user_text}' <b>–Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö.</b>\n\n"
                    response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã."
                    
            elif search_type == 'russian':
                # –¢–û–õ–¨–ö–û —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                articles = await self.news_searcher.search_only_russian(user_text)
                
                if articles:
                    response = f"üîç <b>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –ø–æ</b> '{user_text}':\n\n"
                    
                    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ—Å—Ç–∏
                    priority_articles = [a for a in articles if a.get('priority') or self.news_searcher.is_priority_domain(a.get('url', ''))]
                    regular_articles = [a for a in articles if not (a.get('priority') or self.news_searcher.is_priority_domain(a.get('url', '')))]
                    
                    if priority_articles:
                        response += "üèõÔ∏è <b>–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:</b>\n\n"
                        for i, article in enumerate(priority_articles, 1):
                            domain = urllib.parse.urlparse(article['url']).netloc
                            response += f"{i}. {article['title']}\n"
                            response += f"   üîó <i>{domain}</i>\n   {article['url']}\n\n"
                    
                    if regular_articles:
                        start_num = len(priority_articles) + 1
                        if priority_articles:
                            response += "üì∞ <b>–î—Ä—É–≥–∏–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:</b>\n\n"
                        for i, article in enumerate(regular_articles, start_num):
                            domain = urllib.parse.urlparse(article['url']).netloc
                            response += f"{i}. {article['title']}\n"
                            response += f"   üîó <i>{domain}</i>\n   {article['url']}\n\n"
                else:
                    response = f"üòî <b>–ü–æ –∑–∞–ø—Ä–æ—Å—É</b> '{user_text}' <b>–Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö.</b>\n\n"
                    response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã\n‚Ä¢ –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: '–≠–ü–†', '–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏'\n‚Ä¢ –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏: '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º'"
                    
            else:
                # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                articles = await self.news_searcher.universal_search(user_text, "all")
                
                if articles:
                    russian_articles = [a for a in articles if a.get('language') == 'ru']
                    english_articles = [a for a in articles if a.get('language') == 'en']

                    response = f"üîç <b>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ</b> '{user_text}':\n\n"

                    if russian_articles:
                        response += "üá∑üá∫ <b>–†–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:</b>\n\n"
                        for i, article in enumerate(russian_articles[:3], 1):
                            domain = urllib.parse.urlparse(article['url']).netloc
                            priority_indicator = "üèõÔ∏è " if article.get('priority') or self.news_searcher.is_priority_domain(article['url']) else ""
                            response += f"{priority_indicator}{article['title']}\n"
                            response += f"   üîó <i>{domain}</i>\n   {article['url']}\n\n"

                    if english_articles:
                        response += "üåç <b>–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:</b>\n\n"
                        start_index = len(russian_articles[:3]) + 1
                        for i, article in enumerate(english_articles[:3], start_index):
                            domain = urllib.parse.urlparse(article['url']).netloc
                            response += f"{i}. {article['title']}\n"
                            response += f"   üîó <i>{domain}</i>\n   {article['url']}\n\n"
                else:
                    response = f"üòî <b>–ü–æ –∑–∞–ø—Ä–æ—Å—É</b> '{user_text}' <b>–Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π.</b>\n\n"
                    response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞."

            await message.answer(response, parse_mode="HTML")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            await message.answer(f"‚ùå <b>–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ.</b> –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫—É—é —Ñ—Ä–∞–∑—É.", parse_mode="HTML")

    async def start(self):
        """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            logger.info("üöÄ –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø–æ–∏—Å–∫–æ–º...")
            await self.bot.delete_webhook(drop_pending_updates=True)
            
            self._is_running = True
            self._shutdown_event.clear()
            
            while self._is_running and not self._shutdown_event.is_set():
                try:
                    await self.dp.start_polling(
                        self.bot, 
                        skip_updates=True,
                        timeout=10,
                        relax=0.5,
                        allowed_updates=['message', 'callback_query']
                    )
                except asyncio.CancelledError:
                    logger.info("üîÑ –ü–æ–ª–ª–∏–Ω–≥ –æ—Ç–º–µ–Ω–µ–Ω")
                    break
                except Exception as e:
                    if self._is_running and not self._shutdown_event.is_set():
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ polling: {e}")
                        logger.info("üîÑ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ polling —á–µ—Ä–µ–∑ 3 —Å–µ–∫—É–Ω–¥—ã...")
                        await asyncio.sleep(3)
                    else:
                        break
                        
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞: {e}")
            raise
    
    async def stop(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–æ—Ç–∞"""
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ—Å—Ç–∞–Ω–æ–≤–∫—É –±–æ—Ç–∞...")
        self._is_running = False
        self._shutdown_event.set()
        
        try:
            await self.dp.stop_polling()
            logger.info("‚úÖ –ü–æ–ª–ª–∏–Ω–≥ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ –ø–æ–ª–ª–∏–Ω–≥–∞: {e}")
        
        try:
            await self.news_searcher.close()
            logger.info("‚úÖ –ü–æ–∏—Å–∫–æ–≤–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞–∫—Ä—ã—Ç")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞: {e}")
        
        try:
            await self.bot.session.close()
            logger.info("‚úÖ –°–µ—Å—Å–∏—è –±–æ—Ç–∞ –∑–∞–∫—Ä—ã—Ç–∞")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ —Å–µ—Å—Å–∏–∏: {e}")
        
        logger.info("‚úÖ –ë–æ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

# ===== –£–õ–£–ß–®–ï–ù–ù–ê–Ø –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ó–ê–ü–£–°–ö–ê =====
async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π SIGTERM"""
    bot_instance = None
    health_server = None
    shutdown_manager = GracefulShutdown()
    
    try:
        health_server = HealthServer()
        await health_server.start()
        
        bot_instance = RobustBot()
        
        bot_task = asyncio.create_task(bot_instance.start())
        
        logger.info("‚úÖ –í—Å–µ —Å–µ—Ä–≤–∏—Å—ã –∑–∞–ø—É—â–µ–Ω—ã, –±–æ—Ç –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
        
        while not shutdown_manager.shutdown:
            await asyncio.sleep(2)
            
            if bot_task.done() and not shutdown_manager.shutdown:
                if bot_task.exception():
                    logger.error(f"‚ùå –ë–æ—Ç —É–ø–∞–ª —Å –æ—à–∏–±–∫–æ–π: {bot_task.exception()}")
                    logger.info("üîÑ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞...")
                    bot_task = asyncio.create_task(bot_instance.start())
                else:
                    logger.warning("‚ö†Ô∏è –ë–æ—Ç –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –±–µ–∑ –æ—à–∏–±–∫–∏, –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º...")
                    bot_task = asyncio.create_task(bot_instance.start())
        
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º graceful shutdown...")
        
        if bot_instance:
            await bot_instance.stop()
        
        if bot_task and not bot_task.done():
            try:
                await asyncio.wait_for(bot_task, timeout=10.0)
                logger.info("‚úÖ –ó–∞–¥–∞—á–∞ –±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            except asyncio.TimeoutError:
                logger.warning("‚è∞ –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –±–æ—Ç–∞, –æ—Ç–º–µ–Ω—è–µ–º –∑–∞–¥–∞—á—É...")
                bot_task.cancel()
                try:
                    await bot_task
                except asyncio.CancelledError:
                    logger.info("‚úÖ –ó–∞–¥–∞—á–∞ –±–æ—Ç–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞")
            
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ main(): {e}")
    finally:
        if health_server:
            await health_server.stop()
        
        if bot_instance:
            await bot_instance.stop()

# ===== –ó–ê–ü–£–°–ö –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø =====
if __name__ == "__main__":
    import time
    
    restart_delay = 3
    max_restart_delay = 300
    total_restarts = 0
    
    logger.info("‚ôæÔ∏è –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø–æ–∏—Å–∫–æ–º –∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–º–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–º–∏")
    
    while True:
        try:
            total_restarts += 1
            logger.info(f"üîÑ –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ (–ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ #{total_restarts})...")
            
            asyncio.run(main())
            
            logger.info("‚úÖ –ë–æ—Ç –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ, –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ 5 —Å–µ–∫—É–Ω–¥...")
            time.sleep(5)
            restart_delay = 3
            
        except KeyboardInterrupt:
            logger.info("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
            break
            
        except SystemExit as e:
            if e.code == 0:
                logger.info("‚úÖ –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã, –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ 5 —Å–µ–∫—É–Ω–¥...")
                time.sleep(5)
                restart_delay = 3
            else:
                logger.error(f"üö® –ê–≤–∞—Ä–∏–π–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Å –∫–æ–¥–æ–º {e.code}")
                logger.info(f"‚è≥ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ {restart_delay} —Å–µ–∫—É–Ω–¥...")
                time.sleep(restart_delay)
                restart_delay = min(restart_delay * 1.5, max_restart_delay)
                
        except Exception as e:
            logger.error(f"üí• –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")
            logger.info(f"‚è≥ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ {restart_delay} —Å–µ–∫—É–Ω–¥...")
            time.sleep(restart_delay)
            restart_delay = min(restart_delay * 1.5, max_restart_delay)
    
    logger.info(f"üëã –ë–æ—Ç –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É –ø–æ—Å–ª–µ {total_restarts} –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–≤")
