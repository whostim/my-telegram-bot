from datetime import datetime, timedelta
import os
import logging
import asyncio
import aiohttp
from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
from aiogram.types import ReplyKeyboardMarkup, KeyboardButton
from dotenv import load_dotenv
import urllib.parse
from bs4 import BeautifulSoup
import json
import re
import random
import sys
import atexit
import signal
from aiohttp import web
import threading
import subprocess
import time

# ===== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –õ–û–ì–ì–ò–†–û–í–ê–ù–ò–Ø =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('bot.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ===== –ó–ê–©–ò–¢–ê –û–¢ –ú–ù–û–ñ–ï–°–¢–í–ï–ù–ù–û–ì–û –ó–ê–ü–£–°–ö–ê =====
def check_single_instance():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞"""
    try:
        lock_file = "/tmp/telegram-bot.lock"
        
        if os.path.exists(lock_file):
            with open(lock_file, 'r') as f:
                old_pid = f.read().strip()
            
            try:
                os.kill(int(old_pid), 0)
                logger.info(f"‚ùå –ë–æ—Ç —É–∂–µ –∑–∞–ø—É—â–µ–Ω –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ {old_pid}. –ó–∞–≤–µ—Ä—à–∞–µ–º.")
                sys.exit(1)
            except (ProcessLookupError, ValueError):
                os.remove(lock_file)
        
        with open(lock_file, 'w') as f:
            f.write(str(os.getpid()))
        
        def cleanup():
            try:
                if os.path.exists(lock_file):
                    os.remove(lock_file)
            except:
                pass
        
        atexit.register(cleanup)
        logger.info(f"üîí –§–∞–π–ª –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —Å–æ–∑–¥–∞–Ω (PID: {os.getpid()})")
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏: {e}")

check_single_instance()

# ===== –ó–ê–ì–†–£–ó–ö–ê –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò =====
load_dotenv()

BOT_TOKEN = os.getenv('BOT_TOKEN')
if not BOT_TOKEN:
    logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
    sys.exit(1)

# ===== –†–ê–ë–û–ß–ò–ô –ö–õ–ê–°–° –ü–û–ò–°–ö–ê =====
class WorkingNewsSearcher:
    def __init__(self):
        self.session = None
        self.cache = {}
        self.cache_timeout = 300
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]

    async def get_session(self):
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=30, connect=10, sock_read=20)
            connector = aiohttp.TCPConnector(limit=10, keepalive_timeout=30)
            self.session = aiohttp.ClientSession(timeout=timeout, connector=connector)
        return self.session

    def get_cached_results(self, query):
        cache_key = f"search_{hash(query)}"
        if cache_key in self.cache:
            cache_time, results = self.cache[cache_key]
            if datetime.now() - cache_time < timedelta(seconds=self.cache_timeout):
                return results
        return None

    def set_cached_results(self, query, results):
        cache_key = f"search_{hash(query)}"
        self.cache[cache_key] = (datetime.now(), results)

    async def search_yandex_working(self, query):
        """–†–ê–ë–û–ß–ò–ô –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –Ø–Ω–¥–µ–∫—Å - –∏–º–∏—Ç–∏—Ä—É–µ–º —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–Ω—ã–µ URL –Ø–Ω–¥–µ–∫—Å
            urls = [
                f"https://yandex.ru/search/?text={encoded_query}&lr=213",
                f"https://www.yandex.ru/search/?text={encoded_query}&lr=213",
                f"https://yandex.com/search/?text={encoded_query}&lang=ru"
            ]
            
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1'
            }

            articles = []
            
            for url in urls:
                try:
                    logger.info(f"üîç –ü—Ä–æ–±—É–µ–º URL: {url}")
                    
                    async with session.get(url, headers=headers, timeout=30) as response:
                        if response.status == 200:
                            html = await response.text()
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                            with open('yandex_response.html', 'w', encoding='utf-8') as f:
                                f.write(html)
                            
                            soup = BeautifulSoup(html, 'html.parser')
                            
                            # –ü–†–û–°–¢–û–ô –ò –≠–§–§–ï–ö–¢–ò–í–ù–´–ô –ü–ê–†–°–ò–ù–ì
                            # –ò—â–µ–º –í–°–ï —Å—Å—ã–ª–∫–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞
                            all_links = soup.find_all('a', href=True)
                            
                            for link in all_links:
                                try:
                                    href = link.get('href', '')
                                    text = link.get_text().strip()
                                    
                                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                                    if (href.startswith('http') and 
                                        not any(domain in href for domain in ['yandex.ru', 'ya.ru', 'yandex.com']) and
                                        len(text) > 10 and  # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–π
                                        any(keyword in text.lower() for keyword in ['—ç–ø—Ä', '—Ä–µ–≥—É–ª—è—Ç–æ—Ä', '—Ñ–∏–Ω—Ç–µ—Ö', '–±–∞–Ω–∫', '–Ω–æ–≤–æ—Å—Ç–∏', '–ø–µ—Å–æ—á–Ω–∏—Ü–∞'])):
                                        
                                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–æ—Å—Å–∏–π—Å–∫–∏–π –¥–æ–º–µ–Ω
                                        domain = urllib.parse.urlparse(href).netloc.lower()
                                        if any(ru_domain in domain for ru_domain in ['.ru', '.—Ä—Ñ', '.su']):
                                            articles.append({
                                                'title': text,
                                                'url': href,
                                                'language': 'ru'
                                            })
                                            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Å—Ç–∞—Ç—å—è: {text[:60]}...")
                                            
                                except Exception as e:
                                    continue
                            
                            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Å—Ç–∞—Ç—å–∏, –≤—ã—Ö–æ–¥–∏–º
                            if articles:
                                break
                                
                        else:
                            logger.warning(f"‚ö†Ô∏è –°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞: {response.status}")
                            
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ {url}: {e}")
                    continue

            logger.info(f"üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}")
            return articles[:10]
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []

    async def search_google_news(self, query):
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google News"""
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            
            url = f"https://news.google.com/rss/search?q={encoded_query}+–†–æ—Å—Å–∏—è&hl=ru&gl=RU&ceid=RU:ru"
            
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'application/rss+xml, text/xml, */*'
            }

            async with session.get(url, headers=headers, timeout=20) as response:
                if response.status == 200:
                    xml_content = await response.text()
                    soup = BeautifulSoup(xml_content, 'xml')
                    
                    articles = []
                    items = soup.find_all('item')[:10]
                    
                    for item in items:
                        try:
                            title = item.find('title').text if item.find('title') else ''
                            link = item.find('link').text if item.find('link') else ''
                            
                            if title and link:
                                articles.append({
                                    'title': title,
                                    'url': link,
                                    'language': 'ru'
                                })
                        except:
                            continue
                    
                    logger.info(f"‚úÖ Google News: –Ω–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
                    return articles
                    
            return []
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Google News: {e}")
            return []

    async def search_only_russian(self, query):
        """–ü–æ–∏—Å–∫ –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö"""
        cache_key = f"russian_{hash(query)}"
        cached_results = self.get_cached_results(cache_key)
        if cached_results:
            return cached_results

        logger.info(f"üîç –ü–æ–∏—Å–∫: {query}")

        articles = []

        try:
            # –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –Ø–Ω–¥–µ–∫—Å
            yandex_articles = await self.search_yandex_working(query)
            articles.extend(yandex_articles)
            
            # –ï—Å–ª–∏ –Ø–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞—à–µ–ª, –∏—Å–ø–æ–ª—å–∑—É–µ–º Google News
            if not articles:
                google_articles = await self.search_google_news(query)
                articles.extend(google_articles)
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_articles = []
        seen_urls = set()
        
        for article in articles:
            if article and article.get('url') and article['url'] not in seen_urls:
                seen_urls.add(article['url'])
                unique_articles.append(article)

        self.set_cached_results(cache_key, unique_articles)
        logger.info(f"üìä –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(unique_articles)} —Å—Ç–∞—Ç–µ–π")
        return unique_articles

    async def get_fresh_news(self):
        """–°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏"""
        logger.info("üîç –ü–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π...")

        queries = [
            "–≠–ü–†",
            "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞",
            "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º", 
            "—Ü–∏—Ñ—Ä–æ–≤—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∞–∫—Ç–∏–≤—ã",
            "–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏",
            "—Ñ–∏–Ω—Ç–µ—Ö —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ"
        ]

        all_articles = []

        for query in queries:
            try:
                articles = await self.search_only_russian(query)
                all_articles.extend(articles)
                await asyncio.sleep(1)
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ {query}: {e}")
                continue

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_articles = []
        seen_urls = set()
        
        for article in all_articles:
            if article and article.get('url') and article['url'] not in seen_urls:
                seen_urls.add(article['url'])
                unique_articles.append(article)

        logger.info(f"‚úÖ –°–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(unique_articles)}")
        return unique_articles[:8]

    async def close(self):
        if self.session and not self.session.closed:
            await self.session.close()

# ===== –¢–ï–õ–ï–ì–†–ê–ú –ë–û–¢ =====
main_keyboard = ReplyKeyboardMarkup(
    keyboard=[
        [KeyboardButton(text="üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π")],
        [KeyboardButton(text="‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏")]
    ], 
    resize_keyboard=True
)

user_search_type = {}

class TelegramBot:
    def __init__(self):
        self.bot = Bot(token=BOT_TOKEN)
        self.dp = Dispatcher()
        self.searcher = WorkingNewsSearcher()
        self.setup_handlers()

    def setup_handlers(self):
        @self.dp.message(Command("start"))
        async def cmd_start(message: types.Message):
            await message.answer(
                "üîç –ë–æ—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –æ–± –≠–ü–†\n\n"
                "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π - –Ω–∞–π–¥—É —Å—Ç–∞—Ç—å–∏ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É\n"
                "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ - –ø–æ–∫–∞–∂—É –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏\n\n"
                "–ü—Ä–æ—Å—Ç–æ –Ω–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –∏ –≤–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å!",
                reply_markup=main_keyboard
            )

        @self.dp.message(Command("help"))
        async def cmd_help(message: types.Message):
            await message.answer(
                "üí° –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ—Ç–∞:\n\n"
                "1. –ù–∞–∂–º–∏—Ç–µ 'üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π'\n"
                "2. –í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å (–Ω–∞–ø—Ä–∏–º–µ—Ä: '–≠–ü–†' –∏–ª–∏ '—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞')\n"
                "3. –ë–æ—Ç –Ω–∞–π–¥–µ—Ç –∏ –ø–æ–∫–∞–∂–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏\n\n"
                "–ò–ª–∏ –Ω–∞–∂–º–∏—Ç–µ '‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏' –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"
            )

        @self.dp.message(lambda message: message.text == "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π")
        async def search_news(message: types.Message):
            user_search_type[message.from_user.id] = 'search'
            await message.answer("üîç –í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π:")

        @self.dp.message(lambda message: message.text == "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏")
        async def fresh_news(message: types.Message):
            await message.answer("‚ö° –ò—â—É —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏...")
            
            try:
                articles = await self.searcher.get_fresh_news()
                
                if articles:
                    response = "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏:\n\n"
                    for i, article in enumerate(articles, 1):
                        response += f"{i}. {article['title']}\n"
                        response += f"üîó {article['url']}\n\n"
                        
                        if len(response) > 3000:
                            response += "... (–ø–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ —Å—Ç–∞—Ç—å–∏)"
                            break
                else:
                    response = "üòî –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏.\n"
                    response += "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É."
                    
                await message.answer(response)
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
                await message.answer("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –Ω–æ–≤–æ—Å—Ç–µ–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")

        @self.dp.message()
        async def handle_message(message: types.Message):
            user_id = message.from_user.id
            text = message.text.strip()

            if text.startswith('/'):
                return

            if user_id in user_search_type and user_search_type[user_id] == 'search':
                del user_search_type[user_id]
                await message.answer(f"üîç –ò—â—É: '{text}'...")
                
                try:
                    articles = await self.searcher.search_only_russian(text)
                    
                    if articles:
                        response = f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ '{text}':\n\n"
                        for i, article in enumerate(articles[:5], 1):
                            response += f"{i}. {article['title']}\n"
                            response += f"üîó {article['url']}\n\n"
                    else:
                        response = f"üòî –ü–æ –∑–∞–ø—Ä–æ—Å—É '{text}' –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.\n"
                        response += "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ '‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏'"
                        
                    await message.answer(response)
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
                    await message.answer("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –∑–∞–ø—Ä–æ—Å.")
            else:
                await message.answer("‚ÑπÔ∏è –í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ —Å –ø–æ–º–æ—â—å—é –∫–Ω–æ–ø–æ–∫ –Ω–∏–∂–µ:", reply_markup=main_keyboard)

    async def start(self):
        """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
        try:
            logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ä–∞–±–æ—á–µ–≥–æ –±–æ—Ç–∞...")
            await self.bot.delete_webhook(drop_pending_updates=True)
            await self.dp.start_polling(self.bot)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞: {e}")
            raise

    async def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–æ—Ç–∞"""
        await self.searcher.close()
        await self.bot.session.close()

# ===== –ó–ê–ü–£–°–ö =====
async def main():
    bot = None
    try:
        bot = TelegramBot()
        await bot.start()
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
    except Exception as e:
        logger.error(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
    finally:
        if bot:
            await bot.stop()

if __name__ == "__main__":
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –†–ê–ë–û–ß–ï–ì–û –±–æ—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π")
    asyncio.run(main())
