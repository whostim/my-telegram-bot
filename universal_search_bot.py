from datetime import datetime, timedelta
import os
import logging
import asyncio
import aiohttp
from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
from aiogram.types import ReplyKeyboardMarkup, KeyboardButton
from dotenv import load_dotenv
import urllib.parse
from bs4 import BeautifulSoup
import re
import sys

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('bot.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

load_dotenv()
BOT_TOKEN = os.getenv('BOT_TOKEN')

class SimpleNewsSearcher:
    def __init__(self):
        self.session = None
        self.cache = {}
        self.cache_timeout = 300

    async def get_session(self):
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(timeout=timeout)
        return self.session

    def get_cached_results(self, query):
        cache_key = f"search_{hash(query)}"
        if cache_key in self.cache:
            cache_time, results = self.cache[cache_key]
            if datetime.now() - cache_time < timedelta(seconds=self.cache_timeout):
                return results
        return None

    def set_cached_results(self, query, results):
        cache_key = f"search_{hash(query)}"
        self.cache[cache_key] = (datetime.now(), results)

    async def search_yandex_news(self, query):
        """–ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –≤ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç—è—Ö"""
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://yandex.ru/news/search?text={encoded_query}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8'
            }

            async with session.get(url, headers=headers, timeout=15) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    articles = []
                    
                    # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
                    news_items = soup.find_all('article', class_='mg-card')
                    
                    for item in news_items[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
                        try:
                            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                            title_elem = item.find('h2', class_='mg-card__title') or item.find('a', class_='mg-card__link')
                            if not title_elem:
                                continue
                                
                            title = title_elem.get_text().strip()
                            
                            # –°—Å—ã–ª–∫–∞
                            link = title_elem.get('href', '')
                            if link.startswith('/'):
                                link = f"https://yandex.ru{link}"
                            elif link.startswith('https://news.yandex.ru/yandsearch?'):
                                # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞—Å—Ç–æ—è—â—É—é —Å—Å—ã–ª–∫—É –∏–∑ –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ø–Ω–¥–µ–∫—Å
                                match = re.search(r'cl4url=([^&]+)', link)
                                if match:
                                    link = urllib.parse.unquote(match.group(1))
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Å—ã–ª–∫–∞ –≤–∞–ª–∏–¥–Ω–∞—è
                            if link and link.startswith('http') and 'yandex' not in link:
                                articles.append({
                                    'title': title,
                                    'url': link,
                                    'source': 'yandex'
                                })
                                
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ –Ø–Ω–¥–µ–∫—Å: {e}")
                            continue
                    
                    logger.info(f"‚úÖ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–∏: –Ω–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
                    return articles
                    
            return []
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ø–Ω–¥–µ–∫—Å: {e}")
            return []

    async def search_google_news(self, query):
        """–ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –≤ Google News"""
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://news.google.com/search?q={encoded_query}&hl=ru&gl=RU&ceid=RU:ru"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8'
            }

            async with session.get(url, headers=headers, timeout=15) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    articles = []
                    
                    # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ Google News
                    news_items = soup.find_all('article')
                    
                    for item in news_items[:10]:
                        try:
                            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                            title_elem = item.find('h3') or item.find('h4') or item.find('a')
                            if not title_elem:
                                continue
                                
                            title = title_elem.get_text().strip()
                            
                            # –°—Å—ã–ª–∫–∞
                            link_elem = title_elem.find_parent('a') if title_elem.name != 'a' else title_elem
                            if link_elem and link_elem.get('href'):
                                link = link_elem.get('href')
                                if link.startswith('./'):
                                    link = f"https://news.google.com{link[1:]}"
                                
                                # Google News –¥–∞–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤–µ–¥—É—Ç –Ω–∞ –∏—Ö –¥–æ–º–µ–Ω
                                # –í —Ä–µ–∞–ª—å–Ω–æ–º –±–æ—Ç–µ –Ω—É–∂–Ω–æ –±—ã–ª–æ –±—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —ç—Ç–∏ —Å—Å—ã–ª–∫–∏,
                                # –Ω–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –æ—Å—Ç–∞–≤–∏–º –∫–∞–∫ –µ—Å—Ç—å
                                
                                if link and link.startswith('http'):
                                    articles.append({
                                        'title': title,
                                        'url': link,
                                        'source': 'google'
                                    })
                                    
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ Google: {e}")
                            continue
                    
                    logger.info(f"‚úÖ Google News: –Ω–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
                    return articles
                    
            return []
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ Google: {e}")
            return []

    async def search_bing_news(self, query):
        """–ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –≤ Bing News"""
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://www.bing.com/news/search?q={encoded_query}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8'
            }

            async with session.get(url, headers=headers, timeout=15) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    articles = []
                    
                    # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ Bing
                    news_items = soup.find_all('div', class_='news-card') or soup.find_all('div', class_='tile')
                    
                    for item in news_items[:10]:
                        try:
                            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                            title_elem = item.find('a', class_='title') or item.find('h2') or item.find('a')
                            if not title_elem:
                                continue
                                
                            title = title_elem.get_text().strip()
                            
                            # –°—Å—ã–ª–∫–∞
                            link = title_elem.get('href', '')
                            if link.startswith('/'):
                                link = f"https://www.bing.com{link}"
                            
                            if link and link.startswith('http') and 'bing.com' not in link:
                                articles.append({
                                    'title': title,
                                    'url': link,
                                    'source': 'bing'
                                })
                                
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ Bing: {e}")
                            continue
                    
                    logger.info(f"‚úÖ Bing News: –Ω–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
                    return articles
                    
            return []
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ Bing: {e}")
            return []

    async def simple_search(self, query, search_type="all"):
        """
        –ü—Ä–æ—Å—Ç–æ–π –∏ –ø—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
        search_type: "all", "russian", "international"
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = f"{search_type}_{query}"
        cached = self.get_cached_results(cache_key)
        if cached:
            return cached

        logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫: '{query}' (—Ç–∏–ø: {search_type})")
        
        all_articles = []
        
        try:
            # –í—Å–µ–≥–¥–∞ –∏—â–µ–º –≤ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç—è—Ö –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            if search_type in ["all", "russian"]:
                yandex_results = await self.search_yandex_news(query)
                all_articles.extend(yandex_results)
            
            # –î–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏–ª–∏ –≤—Å–µ—Ö - –¥–æ–±–∞–≤–ª—è–µ–º Google –∏ Bing
            if search_type in ["all", "international"]:
                google_results = await self.search_google_news(query)
                bing_results = await self.search_bing_news(query)
                all_articles.extend(google_results)
                all_articles.extend(bing_results)
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
        unique_articles = []
        seen_urls = set()
        
        for article in all_articles:
            if article['url'] not in seen_urls:
                seen_urls.add(article['url'])
                unique_articles.append(article)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        self.set_cached_results(cache_key, unique_articles[:8])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 8 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        
        logger.info(f"üìä –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(unique_articles)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
        return unique_articles[:8]

    async def close(self):
        if self.session and not self.session.closed:
            await self.session.close()

# –ö–ª–∞–≤–∏–∞—Ç—É—Ä–∞
main_keyboard = ReplyKeyboardMarkup(
    keyboard=[
        [KeyboardButton(text="üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π")],
        [KeyboardButton(text="üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")],
        [KeyboardButton(text="üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫")]
    ], 
    resize_keyboard=True
)

class SimpleNewsBot:
    def __init__(self):
        self.bot = Bot(token=BOT_TOKEN)
        self.dp = Dispatcher()
        self.searcher = SimpleNewsSearcher()
        self.user_states = {}  # user_id -> search_type
        self.setup_handlers()

    def setup_handlers(self):
        @self.dp.message(Command("start"))
        async def cmd_start(message: types.Message):
            await message.answer(
                "ü§ñ <b>–ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π</b>\n\n"
                "–Ø –∏—â—É –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö.\n\n"
                "üîç <b>–ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π</b> - —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏\n"
                "üåç <b>–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏</b> - –∑–∞—Ä—É–±–µ–∂–Ω—ã–µ –°–ú–ò\n"
                "üìä <b>–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫</b> - –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏\n\n"
                "–ü—Ä–æ—Å—Ç–æ –≤—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –ø–æ–∏—Å–∫–∞ –∏ –≤–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å!",
                reply_markup=main_keyboard,
                parse_mode="HTML"
            )

        @self.dp.message(Command("help"))
        async def cmd_help(message: types.Message):
            help_text = """
üìñ <b>–ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π</b>

<b>–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:</b>
1. –í—ã –≤–≤–æ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å
2. –Ø –∏—â—É –ø–æ —ç—Ç–æ–º—É –∑–∞–ø—Ä–æ—Å—É –≤ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö
3. –ü–æ–∫–∞–∑—ã–≤–∞—é –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏

<b>–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:</b>
‚Ä¢ –≠–ü–† –≤ —Ñ–∏–Ω–∞–Ω—Å–∞—Ö
‚Ä¢ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞
‚Ä¢ –ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏
‚Ä¢ —Ü–∏—Ñ—Ä–æ–≤—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∞–∫—Ç–∏–≤—ã
‚Ä¢ fintech regulation Russia

<b>–¢–∏–ø—ã –ø–æ–∏—Å–∫–∞:</b>
üîç <b>–ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π</b> - –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–∏
üåç <b>–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏</b> - Google News, Bing News  
üìä <b>–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫</b> - –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å—Ä–∞–∑—É

üí° <b>–°–æ–≤–µ—Ç:</b> –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤!
"""
            await message.answer(help_text, parse_mode="HTML")

        @self.dp.message(lambda message: message.text == "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π")
        async def set_russian_search(message: types.Message):
            self.user_states[message.from_user.id] = "russian"
            await message.answer("üîç <b>–†–µ–∂–∏–º: —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏</b>\n\n–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç—è—Ö:", parse_mode="HTML")

        @self.dp.message(lambda message: message.text == "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
        async def set_international_search(message: types.Message):
            self.user_states[message.from_user.id] = "international"
            await message.answer("üåç <b>–†–µ–∂–∏–º: –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏</b>\n\n–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ Google News –∏ Bing News:", parse_mode="HTML")

        @self.dp.message(lambda message: message.text == "üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫")
        async def set_quick_search(message: types.Message):
            self.user_states[message.from_user.id] = "all"
            await message.answer("üìä <b>–†–µ–∂–∏–º: –±—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫</b>\n\n–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–æ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö:", parse_mode="HTML")

        @self.dp.message()
        async def handle_search(message: types.Message):
            user_text = message.text.strip()
            
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—ã –∏ –∫–Ω–æ–ø–∫–∏
            if user_text.startswith('/') or user_text in ["üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π", "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏", "üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫"]:
                return

            user_id = message.from_user.id
            search_type = self.user_states.get(user_id, "all")  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—â–µ–º –≤–µ–∑–¥–µ

            await message.answer(f"üîç <b>–ò—â—É –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É:</b> '{user_text}'\n\n‚è≥ <i>–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥...</i>", parse_mode="HTML")
            
            try:
                # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –±–µ–∑ –ª–∏—à–Ω–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                articles = await self.searcher.simple_search(user_text, search_type)
                
                if articles:
                    response = f"üì∞ <b>–ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∑–∞–ø—Ä–æ—Å—É</b> '{user_text}':\n\n"
                    
                    for i, article in enumerate(articles, 1):
                        # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                        title = article['title']
                        if len(title) > 100:
                            title = title[:100] + "..."
                            
                        response += f"{i}. {title}\n"
                        response += f"   üîó {article['url']}\n\n"
                        
                        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Å–æ–æ–±—â–µ–Ω–∏—è
                        if len(response) > 3000:
                            response += "... (–ø–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã)"
                            break
                            
                else:
                    response = f"üòî <b>–ü–æ –∑–∞–ø—Ä–æ—Å—É</b> '{user_text}' <b>–Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π.</b>\n\n"
                    response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n‚Ä¢ –ò–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É\n‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞\n‚Ä¢ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å –Ω–∞ –æ–ø–µ—á–∞—Ç–∫–∏"
                
                await message.answer(response, parse_mode="HTML")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
                await message.answer("‚ùå <b>–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ.</b> –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.", parse_mode="HTML")

    async def start(self):
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Å—Ç–æ–≥–æ –±–æ—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π...")
        await self.bot.delete_webhook(drop_pending_updates=True)
        await self.dp.start_polling(self.bot)

    async def stop(self):
        await self.searcher.close()
        await self.bot.session.close()

# –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
async def main():
    bot = SimpleNewsBot()
    try:
        await bot.start()
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    finally:
        await bot.stop()

if __name__ == "__main__":
    asyncio.run(main())
