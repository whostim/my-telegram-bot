import os
import logging
import asyncio
import aiohttp
from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
from aiogram.types import ReplyKeyboardMarkup, KeyboardButton
from dotenv import load_dotenv
import urllib.parse
from bs4 import BeautifulSoup
import json
import re
import random
from datetime import datetime, timedelta

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –±–æ—Ç–∞
BOT_TOKEN = os.getenv('BOT_TOKEN')
if not BOT_TOKEN:
    logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
    exit(1)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞ –∏ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞
bot = Bot(token=BOT_TOKEN)
dp = Dispatcher()

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã
main_keyboard = ReplyKeyboardMarkup(
    keyboard=[
        [KeyboardButton(text="üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –≠–ü–†"), KeyboardButton(text="üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")],
        [KeyboardButton(text="‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏"), KeyboardButton(text="üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫")]
    ],
    resize_keyboard=True
)

class ImprovedNewsSearcher:
    def __init__(self):
        self.session = None
        self.cache = {}
        self.cache_timeout = 300
        
        # –ß–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤ –¥–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.russian_domains = [
            'rbc.ru', 'vedomosti.ru', 'kommersant.ru', 'ria.ru', 'tass.ru', 'rt.com',
            'lenta.ru', 'gazeta.ru', 'iz.ru', 'mk.ru', 'aif.ru', 'rg.ru', 'vesti.ru',
            'newsru.com', 'fontanka.ru', 'ng.ru', 'echo.msk.ru', 'bfm.ru', 'forbes.ru',
            'vc.ru', 'rb.ru', 'banki.ru', 'cbr.ru', 'rosfinmonitoring.ru', 'government.ru',
            'kremlin.ru', 'minfin.ru', 'yandex.ru', 'mail.ru', 'rambler.ru'
        ]
    
    async def get_session(self):
        if self.session is None:
            timeout = aiohttp.ClientTimeout(total=15, connect=5, sock_read=10)
            self.session = aiohttp.ClientSession(timeout=timeout)
        return self.session
    
    def get_cached_results(self, query):
        cache_key = f"search_{hash(query)}"
        if cache_key in self.cache:
            cache_time, results = self.cache[cache_key]
            if datetime.now() - cache_time < timedelta(seconds=self.cache_timeout):
                return results
        return None
    
    def set_cached_results(self, query, results):
        cache_key = f"search_{hash(query)}"
        self.cache[cache_key] = (datetime.now(), results)
    
    def is_russian_domain(self, url):
        try:
            domain = urllib.parse.urlparse(url).netloc.lower()
            return any(russian_domain in domain for russian_domain in self.russian_domains)
        except:
            return False
    
    async def translate_query(self, query):
        translation_dict = {
            '—ç–ø—Ä': 'EPR',
            '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π': 'experimental',
            '–ø—Ä–∞–≤–æ–≤–æ–π': 'legal',
            '—Ä–µ–∂–∏–º': 'regime',
            '—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è': 'regulatory',
            '–ø–µ—Å–æ—á–Ω–∏—Ü–∞': 'sandbox',
            '—Ñ–∏–Ω–∞–Ω—Å—ã': 'finance',
            '—Ñ–∏–Ω—Ç–µ—Ö': 'fintech',
            '–±–∞–Ω–∫': 'bank',
            '—Ä–æ—Å—Å–∏—è': 'Russia',
            '—Ä—Ñ': 'Russian Federation',
            '—Ü–∏—Ñ—Ä–æ–≤–æ–π': 'digital',
            '—ç–∫–æ–Ω–æ–º–∏–∫–∞': 'economy',
            '–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏': 'innovations',
            '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': 'technologies',
            '–∑–∞–∫–æ–Ω': 'law',
            '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ': 'government',
            '—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ': 'regulation'
        }
        
        words = query.lower().split()
        translated_words = []
        
        for word in words:
            clean_word = re.sub(r'[^\w\s]', '', word)
            if clean_word in translation_dict:
                translated_words.append(translation_dict[clean_word])
            else:
                translated_words.append(clean_word)
        
        translated_query = ' '.join(translated_words)
        
        if any(word in query.lower() for word in ['—ç–ø—Ä', '—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è', '–ø–µ—Å–æ—á–Ω–∏—Ü–∞']):
            translated_query += " Russia"
        
        return translated_query
    
    async def search_yandex_news_direct(self, query):
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://yandex.ru/news/search?text={encoded_query}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }
            
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    articles = []
                    
                    news_cards = soup.find_all('article', class_='mg-card')[:10]
                    
                    for card in news_cards:
                        try:
                            title_elem = card.find('h2', class_='mg-card__title') or card.find('a', class_='mg-card__link')
                            if not title_elem:
                                continue
                                
                            title = title_elem.get_text().strip()
                            link = title_elem.get('href', '')
                            
                            if link.startswith('https://news.yandex.ru/yandsearch?'):
                                match = re.search(r'cl4url=([^&]+)', link)
                                if match:
                                    link = urllib.parse.unquote(match.group(1))
                            elif link.startswith('/'):
                                link = f"https://yandex.ru{link}"
                            
                            source_elem = card.find('span', class_='mg-card-source__source')
                            time_elem = card.find('span', class_='mg-card-source__time')
                            desc_elem = card.find('div', class_='mg-card__annotation')
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Å—ã–ª–∫–∞ —Ä–∞–±–æ—á–∞—è
                            if link and not any(domain in link for domain in ['google.com/search', 'yandex.ru/search']):
                                articles.append({
                                    'title': title,
                                    'url': link,
                                    'source': source_elem.get_text().strip() if source_elem else '–Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–∏',
                                    'date': time_elem.get_text().strip() if time_elem else '',
                                    'description': desc_elem.get_text().strip() if desc_elem else '',
                                    'language': 'ru'
                                })
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ –Ø–Ω–¥–µ–∫—Å: {e}")
                            continue
                    
                    return articles
                return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []
    
    async def search_bing_news_improved(self, query, market='ru-RU', exclude_russian=False):
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://www.bing.com/news/search?q={encoded_query}&cc={market}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }
            
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    articles = []
                    
                    selectors = [
                        'div.news-card',
                        'div.topic',
                        'div.newsitem',
                        'article.news-card',
                        'div[class*="news"]'
                    ]
                    
                    for selector in selectors:
                        news_cards = soup.select(selector)[:10]
                        for card in news_cards:
                            try:
                                title_elem = (card.find('a', class_='title') or 
                                            card.find('a', class_=re.compile('title')) or
                                            card.find('h2') or
                                            card.find('h3') or
                                            card.find('a', attrs={'href': True}))
                                
                                if title_elem and title_elem.get('href'):
                                    title = title_elem.get_text().strip()
                                    url = title_elem.get('href')
                                    
                                    if url.startswith('/'):
                                        url = f"https://www.bing.com{url}"
                                    
                                    if 'bing.com/news/search' in url:
                                        continue
                                    
                                    if exclude_russian and self.is_russian_domain(url):
                                        continue
                                    
                                    source_elem = card.find(['div', 'span'], class_=re.compile('source|author'))
                                    time_elem = card.find(['div', 'span'], class_=re.compile('time|date'))
                                    
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—á—É—é —Å—Å—ã–ª–∫—É
                                    if url and not any(search_domain in url for search_domain in ['google.com/search', 'bing.com/search']):
                                        articles.append({
                                            'title': title,
                                            'url': url,
                                            'source': source_elem.get_text().strip() if source_elem else 'Bing News',
                                            'date': time_elem.get_text().strip() if time_elem else '',
                                            'language': 'ru' if market == 'ru-RU' else 'en'
                                        })
                            except Exception:
                                continue
                        if articles:
                            break
                    
                    return articles
                return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ Bing News: {e}")
            return []
    
    async def search_duckduckgo_improved(self, query, exclude_russian=False):
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://html.duckduckgo.com/html/?q={encoded_query}+news"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }
            
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    articles = []
                    results = soup.find_all('div', class_='result')[:12]
                    
                    for result in results:
                        try:
                            title_elem = result.find('a', class_='result__a')
                            if title_elem:
                                title = title_elem.get_text().strip()
                                url = title_elem.get('href', '')
                                
                                if 'duckduckgo.com' in url:
                                    match = re.search(r'uddg=([^&]+)', url)
                                    if match:
                                        url = urllib.parse.unquote(match.group(1))
                                
                                if any(domain in url for domain in ['google.com/search', 'bing.com/search', 'yandex.ru/search']):
                                    continue
                                
                                if exclude_russian and self.is_russian_domain(url):
                                    continue
                                
                                snippet_elem = result.find('a', class_='result__snippet')
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—á—É—é —Å—Å—ã–ª–∫—É
                                if url and url.startswith('http'):
                                    articles.append({
                                        'title': title,
                                        'url': url,
                                        'source': 'DuckDuckGo',
                                        'description': snippet_elem.get_text().strip()[:150] + '...' if snippet_elem else '',
                                        'language': 'en'
                                    })
                        except Exception:
                            continue
                    
                    return articles
                return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ DuckDuckGo: {e}")
            return []
    
    async def universal_search(self, query, search_type="all"):
        cache_key = f"{search_type}_{query}"
        cached_results = self.get_cached_results(cache_key)
        if cached_results:
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            return cached_results
        
        all_results = []
        
        try:
            if search_type in ["all", "russian"]:
                logger.info(f"üîç –ü–æ–∏—Å–∫ –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö: {query}")
                
                yandex_results = await self.search_yandex_news_direct(f"{query} –≠–ü–†")
                all_results.extend(yandex_results)
                logger.info(f"‚úÖ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–∏: {len(yandex_results)} —Å—Ç–∞—Ç–µ–π")
                
                bing_ru_results = await self.search_bing_news_improved(f"{query} –≠–ü–†", 'ru-RU')
                all_results.extend(bing_ru_results)
                logger.info(f"‚úÖ Bing –†–æ—Å—Å–∏—è: {len(bing_ru_results)} —Å—Ç–∞—Ç–µ–π")
            
            if search_type in ["all", "international"]:
                logger.info(f"üåç –ü–æ–∏—Å–∫ –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö: {query}")
                
                translated_query = await self.translate_query(query)
                
                duckduckgo_results = await self.search_duckduckgo_improved(translated_query, exclude_russian=True)
                all_results.extend(duckduckgo_results)
                logger.info(f"‚úÖ DuckDuckGo: {len(duckduckgo_results)} —Å—Ç–∞—Ç–µ–π")
                
                bing_en_results = await self.search_bing_news_improved(translated_query, 'en-US', exclude_russian=True)
                all_results.extend(bing_en_results)
                logger.info(f"‚úÖ Bing International: {len(bing_en_results)} —Å—Ç–∞—Ç–µ–π")
        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º –ø–æ–∏—Å–∫–µ: {e}")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã - —É–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ–∏—Å–∫–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º URL
        filtered_results = []
        for result in all_results:
            if result and result.get('url'):
                url = result['url'].lower()
                if any(search_domain in url for search_domain in [
                    'google.com/search', 
                    'bing.com/search',
                    'yandex.ru/search',
                    'news.google.com',
                    'news.yandex.ru/yandsearch'
                ]):
                    continue
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ URL –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ —Ä–µ–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è
                if url.startswith('http') and len(url) > 20:
                    filtered_results.append(result)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        seen_urls = set()
        unique_results = []
        for result in filtered_results:
            if result['url'] not in seen_urls:
                seen_urls.add(result['url'])
                unique_results.append(result)
        
        self.set_cached_results(cache_key, unique_results[:10])
        
        logger.info(f"üìä –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(unique_results)} —Å—Ç–∞—Ç–µ–π")
        return unique_results[:10]
    
    async def get_fresh_news_today(self):
        """–ü–æ–∏—Å–∫ –°–í–ï–ñ–ò–• –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è"""
        cache_key = "fresh_news_today"
        cached_results = self.get_cached_results(cache_key)
        if cached_results:
            return cached_results
        
        logger.info("üîç –ü–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è...")
        
        # –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        today_queries = [
            "–≠–ü–† —Å–µ–≥–æ–¥–Ω—è",
            "–≠–ü–† –Ω–æ–≤–æ—Å—Ç–∏ —Å–µ–≥–æ–¥–Ω—è",
            "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞ —Å–µ–≥–æ–¥–Ω—è",
            "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º –Ω–æ–≤–æ—Å—Ç–∏"
        ]
        
        all_articles = []
        
        for query in today_queries:
            try:
                logger.info(f"üì¢ –ü–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {query}")
                
                # –†–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                yandex_results = await self.search_yandex_news_direct(query)
                bing_results = await self.search_bing_news_improved(query, 'ru-RU')
                
                all_articles.extend(yandex_results)
                all_articles.extend(bing_results)
                
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
                continue
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        filtered_articles = []
        for article in all_articles:
            if article and article.get('url'):
                url = article['url'].lower()
                # –ò—Å–∫–ª—é—á–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º URL
                if not any(search_domain in url for search_domain in [
                    'google.com/search', 'bing.com/search', 'yandex.ru/search'
                ]) and url.startswith('http') and len(url) > 20:
                    filtered_articles.append(article)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        seen_urls = set()
        unique_articles = []
        for article in filtered_articles:
            if article['url'] not in seen_urls:
                seen_urls.add(article['url'])
                unique_articles.append(article)
        
        # –ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –º–∞–ª–æ, –¥–µ–ª–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –æ–±—â–∏–º –∑–∞–ø—Ä–æ—Å–∞–º
        if len(unique_articles) < 4:
            logger.info("üîç –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π...")
            backup_queries = ["–≠–ü–†", "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞ –†–æ—Å—Å–∏—è"]
            for query in backup_queries:
                try:
                    backup_results = await self.universal_search(query, "all")
                    for article in backup_results:
                        if article['url'] not in seen_urls:
                            seen_urls.add(article['url'])
                            unique_articles.append(article)
                    await asyncio.sleep(1)
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏ –∫—ç—à–∏—Ä—É–µ–º
        final_articles = unique_articles[:8]
        self.set_cached_results(cache_key, final_articles)
        
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(final_articles)}")
        return final_articles
    
    async def close(self):
        if self.session:
            await self.session.close()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞
news_searcher = ImprovedNewsSearcher()

@dp.message(Command("start"))
async def cmd_start(message: types.Message):
    await message.answer(
        "üåê –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –æ–± –≠–ü–†\n\n"
        "‚Ä¢ üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –≠–ü–† - —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏\n"
        "‚Ä¢ üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ - —Ç–æ–ª—å–∫–æ –∑–∞—Ä—É–±–µ–∂–Ω—ã–µ –°–ú–ò\n"  
        "‚Ä¢ ‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ - –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è\n"
        "‚Ä¢ üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ - –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n\n"
        "–ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ —á—Ç–æ –∏—â–µ—Ç–µ!",
        reply_markup=main_keyboard
    )

@dp.message(Command("help"))
async def cmd_help(message: types.Message):
    help_text = """
üìñ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –æ–± –≠–ü–†

üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –≠–ü–† - —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ - —Ç–æ–ª—å–∫–æ –∑–∞—Ä—É–±–µ–∂–Ω—ã–µ –°–ú–ò
‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ - –ø–æ–∏—Å–∫ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è
üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ - –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º

üí° –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:
‚Ä¢ –≠–ü–† –≤ —Ñ–∏–Ω–∞–Ω—Å–∞—Ö
‚Ä¢ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞
‚Ä¢ –Ω–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –≠–ü–†
‚Ä¢ Russia fintech regulation

‚ö° –ö–Ω–æ–ø–∫–∞ '–°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏' –∏—â–µ—Ç —Å–∞–º—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è!
"""
    await message.answer(help_text)

@dp.message(lambda message: message.text == "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –≠–ü–†")
async def search_epr_news(message: types.Message):
    await message.answer("üîç –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –æ–± –≠–ü–† (–≤–∫–ª—é—á–∞—è —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏):")

@dp.message(lambda message: message.text == "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
async def international_sources(message: types.Message):
    await message.answer("üåç –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö. –ò—Å–∫–ª—é—á–µ–Ω—ã –≤—Å–µ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ —Å–∞–π—Ç—ã.")

@dp.message(lambda message: message.text == "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏")
async def fresh_news(message: types.Message):
    await message.answer("‚ö° –ò—â—É —Å–∞–º—ã–µ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ–± –≠–ü–† –∑–∞ —Å–µ–≥–æ–¥–Ω—è...")
    
    try:
        articles = await news_searcher.get_fresh_news_today()
        
        if articles:
            response = "‚ö° –°–∞–º—ã–µ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ–± –≠–ü–†:\n\n"
            
            for i, article in enumerate(articles, 1):
                response += f"{i}. {article['title']}\n"
                response += f"   üì∞ {article['source']}\n"
                if article.get('date'):
                    response += f"   üìÖ {article['date']}\n"
                response += f"   üîó {article['url']}\n\n"
                
                if len(response) > 3500:
                    response += "... (–ø–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ —Å—Ç–∞—Ç—å–∏)"
                    break
            
            response += "üí° –í—Å–µ —Å—Å—ã–ª–∫–∏ –≤–µ–¥—É—Ç –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏"
        else:
            response = "üòî –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è.\n\n"
            response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É."
        
        await message.answer(response)
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
        await message.answer("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")

@dp.message(lambda message: message.text == "üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫")
async def quick_search(message: types.Message):
    await message.answer("üìä –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")

@dp.message()
async def handle_text(message: types.Message):
    user_text = message.text.strip()
    
    buttons = ["üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –≠–ü–†", "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏", "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏", "üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫"]
    if user_text.startswith('/') or user_text in buttons:
        return
    
    await message.answer(f"üîç –ò—â—É –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{user_text}'...")
    
    try:
        if any(word in user_text.lower() for word in ['russia', 'russian', 'international']):
            search_type = "international"
            response_note = "üåç –ü–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö\n"
        else:
            search_type = "all"
            response_note = "üîç –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º\n"
        
        articles = await news_searcher.universal_search(user_text, search_type)
        
        if articles:
            russian_articles = [a for a in articles if a.get('language') == 'ru']
            english_articles = [a for a in articles if a.get('language') == 'en']
            
            response = f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ '{user_text}':\n\n{response_note}\n"
            
            if russian_articles and search_type != "international":
                response += "üá∑üá∫ –†–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:\n\n"
                for i, article in enumerate(russian_articles[:4], 1):
                    response += f"{i}. {article['title']}\n"
                    response += f"   üì∞ {article['source']}\n"
                    response += f"   üîó {article['url']}\n\n"
            
            if english_articles:
                response += "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:\n\n"
                for i, article in enumerate(english_articles[:4], 1):
                    response += f"{i}. {article['title']}\n"
                    response += f"   üì∞ {article['source']}\n"
                    response += f"   üîó {article['url']}\n\n"
            
            response += f"üìä –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}\n"
            response += "‚úÖ –í—Å–µ —Å—Å—ã–ª–∫–∏ –≤–µ–¥—É—Ç –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏"
            
        else:
            response = f"üòî –ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_text}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π.\n\n"
            response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞."
        
        await message.answer(response)
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        await message.answer(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –∑–∞–ø—Ä–æ—Å.")

async def main():
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –±–æ—Ç–∞...")
    
    try:
        await bot.delete_webhook(drop_pending_updates=True)
        await dp.start_polling(bot)
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞: {e}")
    finally:
        await news_searcher.close()
