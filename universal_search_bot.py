from datetime import datetime, timedelta
import os
import logging
import asyncio
import aiohttp
from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
from aiogram.types import ReplyKeyboardMarkup, KeyboardButton
from dotenv import load_dotenv
import urllib.parse
from bs4 import BeautifulSoup
import json
import re
import random
import sys
import atexit
import signal
from aiohttp import web

# ===== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –õ–û–ì–ì–ò–†–û–í–ê–ù–ò–Ø =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('bot.log')
    ]
)
logger = logging.getLogger(__name__)

# ===== –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ó–ê–©–ò–¢–ê –û–¢ –ú–ù–û–ñ–ï–°–¢–í–ï–ù–ù–û–ì–û –ó–ê–ü–£–°–ö–ê =====
class GracefulShutdown:
    def __init__(self):
        self.shutdown = False
        self._setup_signal_handlers()
    
    def _setup_signal_handlers(self):
        signal.signal(signal.SIGTERM, self._handle_signal)
        signal.signal(signal.SIGINT, self._handle_signal)
    
    def _handle_signal(self, signum, frame):
        logger.info(f"üì¢ –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª {signum}, –Ω–∞—á–∏–Ω–∞—é graceful shutdown...")
        self.shutdown = True

def cleanup_lock():
    try:
        lock_file = "/tmp/telegram-bot.lock"
        if os.path.exists(lock_file):
            os.remove(lock_file)
            logger.info("üîì –§–∞–π–ª –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —É–¥–∞–ª–µ–Ω")
    except Exception as e:
        logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ lock-—Ñ–∞–π–ª–∞: {e}")

def check_single_instance():
    try:
        lock_file = "/tmp/telegram-bot.lock"
        if os.path.exists(lock_file):
            with open(lock_file, 'r') as f:
                old_pid = f.read().strip()
            try:
                os.kill(int(old_pid), 0)
                logger.info(f"‚ùå –ë–æ—Ç —É–∂–µ –∑–∞–ø—É—â–µ–Ω –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ {old_pid}. –ó–∞–≤–µ—Ä—à–∞–µ–º.")
                sys.exit(1)
            except (ProcessLookupError, ValueError):
                logger.info("üîÑ –°—Ç–∞—Ä—ã–π –ø—Ä–æ—Ü–µ—Å—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∑–∞–ø—É—Å–∫")
                os.remove(lock_file)
        
        with open(lock_file, 'w') as f:
            f.write(str(os.getpid()))
        
        atexit.register(cleanup_lock)
        logger.info(f"üîí –§–∞–π–ª –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —Å–æ–∑–¥–∞–Ω (PID: {os.getpid()})")
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏: {e}")

check_single_instance()

# ===== HEALTH CHECK SERVER =====
class HealthServer:
    def __init__(self, port=8080):
        self.port = port
        self.app = web.Application()
        self.app.router.add_get('/health', self.health_check)
        self.runner = None
        self.site = None
    
    async def health_check(self, request):
        return web.Response(text='OK', status=200)
    
    async def start(self):
        self.runner = web.AppRunner(self.app)
        await self.runner.setup()
        self.site = web.TCPSite(self.runner, '0.0.0.0', self.port)
        await self.site.start()
        logger.info(f"üåê Health server started on port {self.port}")
    
    async def stop(self):
        if self.site:
            await self.site.stop()
        if self.runner:
            await self.runner.cleanup()

# ===== –ó–ê–ì–†–£–ó–ö–ê –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò =====
load_dotenv()

BOT_TOKEN = os.getenv('BOT_TOKEN')
if not BOT_TOKEN:
    logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
    sys.exit(1)

# ===== –ö–õ–ê–°–° –ü–û–ò–°–ö–ê –ù–û–í–û–°–¢–ï–ô =====
class ImprovedNewsSearcher:
    def __init__(self):
        self.session = None
        self.cache = {}
        self.cache_timeout = 300
        self.russian_domains = [
            'rbc.ru', 'vedomosti.ru', 'kommersant.ru', 'ria.ru', 'tass.ru',
            'rt.com', 'lenta.ru', 'gazeta.ru', 'iz.ru', 'mk.ru', 'aif.ru',
            'rg.ru', 'vesti.ru', 'newsru.com', 'fontanka.ru', 'ng.ru',
            'echo.msk.ru', 'bfm.ru', 'forbes.ru', 'vc.ru', 'rb.ru', 'banki.ru',
            'cbr.ru', 'rosfinmonitoring.ru', 'government.ru', 'kremlin.ru',
            'minfin.ru', 'yandex.ru', 'mail.ru', 'rambler.ru',
            'sputniknews.com', 'rbth.com', 'russian.rt.com', 'themoscowtimes.com'
        ]

    async def get_session(self):
        if self.session is None:
            # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ —Ç–∞–π–º–∞—É—Ç—ã –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            timeout = aiohttp.ClientTimeout(total=30, connect=10, sock_read=20)
            connector = aiohttp.TCPConnector(limit=10, keepalive_timeout=30)
            self.session = aiohttp.ClientSession(timeout=timeout, connector=connector)
        return self.session

    def get_cached_results(self, query):
        cache_key = f"search_{hash(query)}"
        if cache_key in self.cache:
            cache_time, results = self.cache[cache_key]
            if datetime.now() - cache_time < timedelta(seconds=self.cache_timeout):
                return results
        return None

    def set_cached_results(self, query, results):
        cache_key = f"search_{hash(query)}"
        self.cache[cache_key] = (datetime.now(), results)

    def is_russian_domain(self, url):
        try:
            domain = urllib.parse.urlparse(url).netloc.lower()
            return any(russian_domain in domain for russian_domain in self.russian_domains)
        except BaseException:
            return False

    def is_russian_text(self, text):
        return bool(re.search('[–∞-—è–ê-–Ø]', text))

    def normalize_title(self, title):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if not title:
            return ""
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        normalized = title.lower()
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        
        # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è (–∫—Ä–æ–º–µ –±—É–∫–≤, —Ü–∏—Ñ—Ä –∏ –ø—Ä–æ–±–µ–ª–æ–≤)
        normalized = re.sub(r'[^\w\s]', '', normalized)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö
        stop_words = ['–Ω–æ–≤–æ—Å—Ç–∏', '—Å–µ–≥–æ–¥–Ω—è', '—Å–µ–π—á–∞—Å', '–ø–æ—Å–ª–µ–¥–Ω–∏–µ', '—Å–≤–µ–∂–∏–µ']
        words = normalized.split()
        filtered_words = [word for word in words if word not in stop_words]
        
        return ' '.join(filtered_words)

    def is_duplicate_article(self, article, existing_articles, similarity_threshold=0.8):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç–∞—Ç—å—è –¥—É–±–ª–∏–∫–∞—Ç–æ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö"""
        if not article or not existing_articles:
            return False
        
        new_title_normalized = self.normalize_title(article.get('title', ''))
        new_url = article.get('url', '')
        
        for existing in existing_articles:
            existing_title_normalized = self.normalize_title(existing.get('title', ''))
            existing_url = existing.get('url', '')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –¥–æ–º–µ–Ω—É –∏ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            if self.is_same_domain(new_url, existing_url):
                # –ï—Å–ª–∏ URL —Å –æ–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                if self.calculate_similarity(new_title_normalized, existing_title_normalized) > similarity_threshold:
                    return True
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–∞–∂–µ —Å —Ä–∞–∑–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
            if self.calculate_similarity(new_title_normalized, existing_title_normalized) > 0.9:
                return True
        
        return False

    def is_same_domain(self, url1, url2):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç –ª–∏ URL –æ–¥–Ω–æ–º—É –¥–æ–º–µ–Ω—É"""
        try:
            domain1 = urllib.parse.urlparse(url1).netloc
            domain2 = urllib.parse.urlparse(url2).netloc
            return domain1 == domain2
        except:
            return False

    def calculate_similarity(self, text1, text2):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
        if not text1 or not text2:
            return 0
        
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 or not words2:
            return 0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0

    async def correct_spelling_auto(self, text):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏—è —á–µ—Ä–µ–∑ Yandex Speller API"""
        try:
            if not self.is_russian_text(text):
                return text
                
            session = await self.get_session()
            encoded_text = urllib.parse.quote(text)
            
            url = f"https://speller.yandex.net/services/spellservice.json/checkText?text={encoded_text}&lang=ru,en"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    corrections = await response.json()
                    
                    if corrections:
                        corrected_text = text
                        for correction in reversed(corrections):
                            if correction.get('s'):
                                fixed_word = correction['s'][0]
                                wrong_word = correction['word']
                                corrected_text = corrected_text.replace(wrong_word, fixed_word)
                        
                        logger.info(f"üìù –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏–µ: '{text}' -> '{corrected_text}'")
                        return corrected_text
                    
            return text
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏—è: {e}")
            return text

    async def translate_to_english_auto(self, text):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π"""
        try:
            if not self.is_russian_text(text):
                return text
                
            session = await self.get_session()
            encoded_text = urllib.parse.quote(text)
            
            # Yandex Translate API
            url = f"https://translate.yandex.net/api/v1.5/tr.json/translate?key=trnsl.1.1.20230101T000000Z.1234567890.abcdef&lang=ru-en&text={encoded_text}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    if data.get('code') == 200 and data.get('text'):
                        translated = data['text'][0]
                        logger.info(f"üåç –ê–≤—Ç–æ–ø–µ—Ä–µ–≤–æ–¥: '{text}' -> '{translated}'")
                        return translated
            
            # Fallback
            return await self.translate_fallback(text)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
            return await self.translate_fallback(text)

    async def translate_fallback(self, text):
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫"""
        try:
            if not self.is_russian_text(text):
                return text
                
            session = await self.get_session()
            encoded_text = urllib.parse.quote(text)
            
            url = f"https://api.mymemory.translated.net/get?q={encoded_text}&langpair=ru|en"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    if data.get('responseData', {}).get('translatedText'):
                        translated = data['responseData']['translatedText']
                        logger.info(f"üåç –†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥: '{text}' -> '{translated}'")
                        return translated
            
            return text
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
            return text

    async def prepare_international_query(self, query):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        try:
            logger.info(f"üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
            corrected_query = await self.correct_spelling_auto(query)
            translated_query = await self.translate_to_english_auto(corrected_query)
            logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{translated_query}'")
            return translated_query
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return query

    async def search_yandex_news_direct(self, query):
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://yandex.ru/news/search?text={encoded_query}"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers, timeout=30) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []
                    news_cards = soup.find_all('article', class_='mg-card')[:10]

                    for card in news_cards:
                        try:
                            title_elem = card.find('h2', class_='mg-card__title') or card.find('a', class_='mg-card__link')
                            if not title_elem:
                                continue

                            title = title_elem.get_text().strip()
                            link = title_elem.get('href', '')

                            if link.startswith('https://news.yandex.ru/yandsearch?'):
                                match = re.search(r'cl4url=([^&]+)', link)
                                if match:
                                    link = urllib.parse.unquote(match.group(1))
                            elif link.startswith('/'):
                                link = f"https://yandex.ru{link}"

                            if link and not any(
                                domain in link for domain in [
                                    'google.com/search',
                                    'yandex.ru/search']):
                                articles.append({
                                    'title': title,
                                    'url': link,
                                    'language': 'ru'
                                })
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ –Ø–Ω–¥–µ–∫—Å: {e}")
                            continue

                    return articles
            return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []

    async def search_bing_news_improved(self, query, market='ru-RU', exclude_russian=False):
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            
            if market == 'en-US':
                url = f"https://www.bing.com/news/search?q={encoded_query}&cc=us&setlang=en"
            else:
                url = f"https://www.bing.com/news/search?q={encoded_query}&cc={market}"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept-Language': 'en-US,en;q=0.9' if market == 'en-US' else 'ru-RU,ru;q=0.9',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers, timeout=30) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []

                    news_cards = soup.find_all('div', class_='news-card')[:10]
                    if not news_cards:
                        news_cards = soup.find_all('div', class_='tile')[:10]
                    if not news_cards:
                        news_cards = soup.find_all('article')[:10]

                    for card in news_cards:
                        try:
                            title_elem = (card.find('a', class_='title') or
                                        card.find('a', class_=re.compile('title')) or
                                        card.find('h2') or
                                        card.find('h3') or
                                        card.find('a', attrs={'href': True}))

                            if title_elem and title_elem.get('href'):
                                title = title_elem.get_text().strip()
                                url = title_elem.get('href')

                                if url.startswith('/'):
                                    url = f"https://www.bing.com{url}"

                                if 'bing.com/news/search' in url:
                                    continue

                                if exclude_russian and self.is_russian_domain(url):
                                    continue

                                if exclude_russian and self.is_russian_text(title):
                                    continue

                                if url and not any(
                                    search_domain in url for search_domain in [
                                        'google.com/search',
                                        'bing.com/search']):
                                    articles.append({
                                        'title': title,
                                        'url': url,
                                        'language': 'en' if market == 'en-US' else 'ru'
                                    })
                        except Exception:
                            continue

                    return articles
            return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ Bing News: {e}")
            return []

    async def search_google_news_english(self, query, exclude_russian=True):
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://news.google.com/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers, timeout=30) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []
                    news_cards = soup.find_all('article')[:12]

                    for card in news_cards:
                        try:
                            title_elem = card.find('h3') or card.find('h4') or card.find('a', attrs={'href': True})
                            if title_elem:
                                title = title_elem.get_text().strip()
                                link_elem = title_elem.find_parent('a') if title_elem.name != 'a' else title_elem
                                if link_elem and link_elem.get('href'):
                                    url = link_elem.get('href')
                                    if url.startswith('./'):
                                        url = f"https://news.google.com{url[1:]}"
                                    
                                    if 'news.google.com' in url:
                                        continue

                                    if exclude_russian and (self.is_russian_domain(url) or self.is_russian_text(title)):
                                        continue

                                    if url and url.startswith('http'):
                                        articles.append({
                                            'title': title,
                                            'url': url,
                                            'language': 'en'
                                        })
                        except Exception:
                            continue

                    return articles
            return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ Google News: {e}")
            return []

    async def search_duckduckgo_improved(self, query, exclude_russian=True):
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://html.duckduckgo.com/html/?q={encoded_query}+news&kl=us-en"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers, timeout=30) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []
                    results = soup.find_all('div', class_='result')[:12]

                    for result in results:
                        try:
                            title_elem = result.find('a', class_='result__a')
                            if title_elem:
                                title = title_elem.get_text().strip()
                                url = title_elem.get('href', '')

                                if 'duckduckgo.com' in url:
                                    match = re.search(r'uddg=([^&]+)', url)
                                    if match:
                                        url = urllib.parse.unquote(match.group(1))

                                if any(
                                    domain in url for domain in [
                                        'google.com/search',
                                        'bing.com/search',
                                        'yandex.ru/search']):
                                    continue

                                if exclude_russian and (self.is_russian_domain(url) or self.is_russian_text(title)):
                                    continue

                                if url and url.startswith('http'):
                                    articles.append({
                                        'title': title,
                                        'url': url,
                                        'language': 'en'
                                    })
                        except Exception:
                            continue

                    return articles
            return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ DuckDuckGo: {e}")
            return []

    async def universal_search(self, query, search_type="all"):
        cache_key = f"{search_type}_{query}"
        cached_results = self.get_cached_results(cache_key)
        if cached_results:
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            return cached_results

        all_results = []

        try:
            if search_type in ["all", "russian"]:
                logger.info(f"üîç –ü–æ–∏—Å–∫ –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö: {query}")

                yandex_results = await self.search_yandex_news_direct(query)
                all_results.extend(yandex_results)
                logger.info(f"‚úÖ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–∏: {len(yandex_results)} —Å—Ç–∞—Ç–µ–π")

                bing_ru_results = await self.search_bing_news_improved(query, 'ru-RU')
                all_results.extend(bing_ru_results)
                logger.info(f"‚úÖ Bing –†–æ—Å—Å–∏—è: {len(bing_ru_results)} —Å—Ç–∞—Ç–µ–π")

            if search_type in ["all", "international"]:
                logger.info(f"üåç –ü–æ–∏—Å–∫ –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö: {query}")

                international_query = await self.prepare_international_query(query)
                logger.info(f"üåç –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {international_query}")

                google_results = await self.search_google_news_english(international_query, exclude_russian=True)
                all_results.extend(google_results)
                logger.info(f"‚úÖ Google News: {len(google_results)} —Å—Ç–∞—Ç–µ–π")

                bing_en_results = await self.search_bing_news_improved(international_query, 'en-US', exclude_russian=True)
                all_results.extend(bing_en_results)
                logger.info(f"‚úÖ Bing International: {len(bing_en_results)} —Å—Ç–∞—Ç–µ–π")

                duckduckgo_results = await self.search_duckduckgo_improved(international_query, exclude_russian=True)
                all_results.extend(duckduckgo_results)
                logger.info(f"‚úÖ DuckDuckGo: {len(duckduckgo_results)} —Å—Ç–∞—Ç–µ–π")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º –ø–æ–∏—Å–∫–µ: {e}")

        # –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        filtered_results = []
        seen_titles = set()
        
        for result in all_results:
            if result and result.get('url'):
                url = result['url'].lower()
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
                if any(search_domain in url for search_domain in [
                    'google.com/search',
                    'bing.com/search', 
                    'yandex.ru/search',
                    'news.google.com',
                    'news.yandex.ru/yandsearch'
                ]):
                    continue
                
                if search_type == "international":
                    if (self.is_russian_domain(url) or 
                        self.is_russian_text(result.get('title', ''))):
                        continue
                        
                if url.startswith('http') and len(url) > 20:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É –∑–∞–≥–æ–ª–æ–≤–∫—É
                    normalized_title = self.normalize_title(result.get('title', ''))
                    if normalized_title and normalized_title not in seen_titles and len(normalized_title) >= 20:
                        seen_titles.add(normalized_title)
                        filtered_results.append(result)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        filtered_results.sort(key=lambda x: len(x.get('title', '')), reverse=True)
        
        self.set_cached_results(cache_key, filtered_results[:10])
        logger.info(f"üìä –ò—Ç–æ–≥–æ–≤—ã–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(filtered_results)} —Å—Ç–∞—Ç–µ–π")
        return filtered_results[:10]

    async def get_fresh_news_today(self):
        cache_key = "fresh_news_today"
        cached_results = self.get_cached_results(cache_key)
        if cached_results:
            return cached_results

        logger.info("üîç –ü–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è...")

        today_queries = [
            "–≠–ü–† —Å–µ–≥–æ–¥–Ω—è",
            "–≠–ü–† –Ω–æ–≤–æ—Å—Ç–∏ —Å–µ–≥–æ–¥–Ω—è", 
            "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞ —Å–µ–≥–æ–¥–Ω—è",
            "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º –Ω–æ–≤–æ—Å—Ç–∏",
            "—Ü–∏—Ñ—Ä–æ–≤—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∞–∫—Ç–∏–≤—ã",
            "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã–µ –ø–µ—Å–æ—á–Ω–∏—Ü—ã –†–æ—Å—Å–∏—è"
        ]

        all_articles = []

        for query in today_queries:
            try:
                logger.info(f"üì¢ –ü–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {query}")

                yandex_results = await self.search_yandex_news_direct(query)
                bing_results = await self.search_bing_news_improved(query, 'ru-RU')

                # –§–∏–ª—å—Ç—Ä—É–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –Ω–∞ —ç—Ç–∞–ø–µ —Å–±–æ—Ä–∞
                for article in yandex_results + bing_results:
                    if not self.is_duplicate_article(article, all_articles):
                        all_articles.append(article)

                await asyncio.sleep(1)

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
                continue

        # –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        filtered_articles = []
        seen_titles = set()
        
        for article in all_articles:
            if article and article.get('url'):
                url = article['url'].lower()
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ URL
                if any(search_domain in url for search_domain in [
                    'google.com/search', 'bing.com/search', 'yandex.ru/search'
                ]) or len(url) < 20:
                    continue
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                normalized_title = self.normalize_title(article.get('title', ''))
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–∞—Ç—å–∏ —Å –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
                if len(normalized_title) < 20:
                    continue
                    
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É –∑–∞–≥–æ–ª–æ–≤–∫—É
                if normalized_title not in seen_titles:
                    seen_titles.add(normalized_title)
                    filtered_articles.append(article)

        # –ï—Å–ª–∏ –º–∞–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π, –¥–µ–ª–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫
        if len(filtered_articles) < 4:
            logger.info("üîç –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π...")
            backup_queries = [
                "–≠–ü–†", 
                "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞ –†–æ—Å—Å–∏—è",
                "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º",
                "—Ü–∏—Ñ—Ä–æ–≤–∞—è –≤–∞–ª—é—Ç–∞ –¶–ë"
            ]
            
            for query in backup_queries:
                try:
                    backup_results = await self.universal_search(query, "all")
                    for article in backup_results:
                        normalized_title = self.normalize_title(article.get('title', ''))
                        if (normalized_title not in seen_titles and 
                            len(normalized_title) >= 20 and
                            not self.is_duplicate_article(article, filtered_articles)):
                            seen_titles.add(normalized_title)
                            filtered_articles.append(article)
                    await asyncio.sleep(1)
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –≠–ü–† –∏ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã—Ö —Ç–µ–º)
        def relevance_score(article):
            title = article.get('title', '').lower()
            score = 0
            keywords = ['—ç–ø—Ä', '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º', '—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞', 
                       '—Ü–∏—Ñ—Ä–æ–≤–∞—è –≤–∞–ª—é—Ç–∞', '—Ü–± —Ä—Ñ', '—Ñ–∏–Ω—Ç–µ—Ö', '–±–ª–æ–∫—á–µ–π–Ω']
            
            for keyword in keywords:
                if keyword in title:
                    score += 1
            return score

        filtered_articles.sort(key=relevance_score, reverse=True)
        final_articles = filtered_articles[:8]  # –ë–µ—Ä–µ–º —Ç–æ–ø-8 —Å–∞–º—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö

        self.set_cached_results(cache_key, final_articles)

        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(final_articles)}")
        return final_articles

    async def close(self):
        if self.session:
            await self.session.close()

# ===== –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï =====
main_keyboard = ReplyKeyboardMarkup(
    keyboard=[
        [
            KeyboardButton(text="üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π"), 
            KeyboardButton(text="üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
        ],
        [
            KeyboardButton(text="‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏"), 
            KeyboardButton(text="üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫")
        ]
    ], 
    resize_keyboard=True
)

user_search_type = {}

# ===== ROBUST BOT CLASS =====
class RobustBot:
    def __init__(self):
        self.bot = Bot(token=BOT_TOKEN)
        self.dp = Dispatcher()
        self.news_searcher = ImprovedNewsSearcher()
        self._is_running = False
        self.setup_handlers()
    
    def setup_handlers(self):
        @self.dp.message(Command("start"))
        async def cmd_start(message: types.Message):
            await message.answer(
                "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –æ–± –≠–ü–†\n\n"
                "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π ‚Äì —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏\n"
                "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ‚Äì —Ç–æ–ª—å–∫–æ –∑–∞—Ä—É–±–µ–∂–Ω—ã–µ –°–ú–ò\n"
                "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ ‚Äì –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏\n"
                "üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ ‚Äì –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n\n"
                "–ü—Ä–æ—Å—Ç–æ –≤—ã–±–µ—Ä–∏—Ç–µ —á—Ç–æ –∏—â–µ—Ç–µ!",
                reply_markup=main_keyboard
            )

        @self.dp.message(Command("help"))
        async def cmd_help(message: types.Message):
            help_text = """
üìñ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –æ–± –≠–ü–†

üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π ‚Äì —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ‚Äì —Ç–æ–ª—å–∫–æ –∑–∞—Ä—É–±–µ–∂–Ω—ã–µ –°–ú–ò
‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ ‚Äì –ø–æ–∏—Å–∫ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è
üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ ‚Äì –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º

üí° –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:
    ‚Ä¢ –≠–ü–† –≤ —Ñ–∏–Ω–∞–Ω—Å–∞—Ö
‚Ä¢ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞
‚Ä¢ –Ω–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –≠–ü–†
‚Ä¢ Russia fintech regulation

‚ö° –ö–Ω–æ–ø–∫–∞ '–°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏' –∏—â–µ—Ç —Å–∞–º—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è!
"""
            await message.answer(help_text)

        @self.dp.message(lambda message: message.text == "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π")
        async def search_epr_news(message: types.Message):
            user_id = message.from_user.id
            user_search_type[user_id] = 'all'
            await message.answer("üîç –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π:")

        @self.dp.message(lambda message: message.text == "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
        async def international_sources(message: types.Message):
            user_id = message.from_user.id
            user_search_type[user_id] = 'international'
            await message.answer("üåç –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π):")

        @self.dp.message(lambda message: message.text == "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏")
        async def fresh_news(message: types.Message):
            await message.answer("‚ö° –ò—â—É —Å–∞–º—ã–µ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏")
            try:
                articles = await self.news_searcher.get_fresh_news_today()
                if articles:
                    response = "‚ö° –°–∞–º—ã–µ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏:\n\n"
                    for i, article in enumerate(articles, 1):
                        response += f"{i}. {article['title']}\n"
                        response += f"   üîó {article['url']}\n\n"
                        if len(response) > 3500:
                            response += "... (–ø–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ —Å—Ç–∞—Ç—å–∏)"
                            break
                else:
                    response = "üòî –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è.\n\n"
                    response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É."
                await message.answer(response)
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
                await message.answer("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")

        @self.dp.message(lambda message: message.text == "üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫")
        async def quick_search(message: types.Message):
            user_id = message.from_user.id
            user_search_type[user_id] = 'quick'
            await message.answer("üìä –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")

        @self.dp.message()
        async def handle_text(message: types.Message):
            user_text = message.text.strip()
            user_id = message.from_user.id

            buttons = [
                "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π",
                "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏", 
                "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏",
                "üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫"]
            if user_text.startswith('/') or user_text in buttons:
                return

            await message.answer(f"üîç –ò—â—É –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{user_text}'...")
            await self.process_search(message, user_text, user_id)

    async def process_search(self, message, user_text, user_id):
        try:
            search_type = user_search_type.pop(user_id, 'all')
            
            if search_type == 'quick':
                russian_articles = await self.news_searcher.universal_search(user_text, "russian")
                international_query = await self.news_searcher.prepare_international_query(user_text)
                international_articles = await self.news_searcher.universal_search(international_query, "international")
                articles = russian_articles[:3] + international_articles[:3]
                
                if articles:
                    response = f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ '{user_text}':\n\n"
                    for i, article in enumerate(articles, 1):
                        response += f"{i}. {article['title']}\n"
                        response += f"   üîó {article['url']}\n\n"
                else:
                    response = f"üòî –ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_text}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π.\n\n"
                    response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞."
                    
            elif search_type == 'international':
                international_query = await self.news_searcher.prepare_international_query(user_text)
                articles = await self.news_searcher.universal_search(international_query, "international")
                
                if articles:
                    response = f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ '{user_text}':\n\n"
                    response += "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:\n\n"
                    for i, article in enumerate(articles[:6], 1):
                        response += f"{i}. {article['title']}\n"
                        response += f"   üîó {article['url']}\n\n"
                else:
                    response = f"üòî –ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_text}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö.\n\n"
                    response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞."
                    
            else:
                articles = await self.news_searcher.universal_search(user_text, "all")
                
                if articles:
                    russian_articles = [a for a in articles if a.get('language') == 'ru']
                    english_articles = [a for a in articles if a.get('language') == 'en']

                    response = f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ '{user_text}':\n\n"

                    if russian_articles:
                        response += "üá∑üá∫ –†–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:\n\n"
                        for i, article in enumerate(russian_articles[:3], 1):
                            response += f"{i}. {article['title']}\n"
                            response += f"   üîó {article['url']}\n\n"

                    if english_articles:
                        response += "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:\n\n"
                        for i, article in enumerate(english_articles[:3], 1):
                            response += f"{i}. {article['title']}\n"
                            response += f"   üîó {article['url']}\n\n"
                else:
                    response = f"üòî –ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_text}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π.\n\n"
                    response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞."

            await message.answer(response)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            await message.answer(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –∑–∞–ø—Ä–æ—Å.")

    async def start(self):
        """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            logger.info("üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –±–æ—Ç–∞...")
            await self.bot.delete_webhook(drop_pending_updates=True)
            
            self._is_running = True
            
            # Polling —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
            while self._is_running:
                try:
                    await self.dp.start_polling(
                        self.bot, 
                        skip_updates=True,
                        timeout=60,
                        relax=1,
                        allowed_updates=['message', 'callback_query']
                    )
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ polling: {e}")
                    if self._is_running:  # –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –∑–∞–ø—Ä–æ—à–µ–Ω shutdown
                        logger.info("üîÑ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ polling —á–µ—Ä–µ–∑ 5 —Å–µ–∫—É–Ω–¥...")
                        await asyncio.sleep(5)
                    else:
                        break
                        
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞: {e}")
            raise
    
    async def stop(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–æ—Ç–∞"""
        self._is_running = False
        try:
            await self.dp.stop_polling()
            await self.news_searcher.close()
            await self.bot.session.close()
            logger.info("‚úÖ –ë–æ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ –±–æ—Ç–∞: {e}")

# ===== –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ó–ê–ü–£–°–ö–ê =====
async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    bot_instance = None
    shutdown_manager = GracefulShutdown()
    
    try:
        bot_instance = RobustBot()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π task –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ graceful shutdown
        bot_task = asyncio.create_task(bot_instance.start())
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        while not shutdown_manager.shutdown:
            await asyncio.sleep(5)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 5 —Å–µ–∫—É–Ω–¥
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∂–∏–≤ –ª–∏ –±–æ—Ç
            if bot_task.done():
                if bot_task.exception():
                    logger.error(f"‚ùå –ë–æ—Ç —É–ø–∞–ª —Å –æ—à–∏–±–∫–æ–π: {bot_task.exception()}")
                    # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É
                    bot_task = asyncio.create_task(bot_instance.start())
                else:
                    logger.warning("‚ö†Ô∏è –ë–æ—Ç –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –±–µ–∑ –æ—à–∏–±–∫–∏, –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º...")
                    bot_task = asyncio.create_task(bot_instance.start())
        
        # Graceful shutdown
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º graceful shutdown...")
        if bot_task and not bot_task.done():
            bot_task.cancel()
            try:
                await bot_task
            except asyncio.CancelledError:
                pass
        
        if bot_instance:
            await bot_instance.stop()
            
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ main(): {e}")
        if bot_instance:
            await bot_instance.stop()
        raise

async def main_with_health():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å health check —Å–µ—Ä–≤–µ—Ä–æ–º"""
    health_server = HealthServer()
    await health_server.start()
    
    try:
        await main()
    finally:
        await health_server.stop()

# ===== –ó–ê–ü–£–°–ö –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø =====
if __name__ == "__main__":
    import time
    
    restart_count = 0
    max_restarts = 15  # –£–≤–µ–ª–∏—á–∏–ª –ª–∏–º–∏—Ç –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–≤
    restart_delay = 5
    last_restart_time = time.time()
    
    while restart_count < max_restarts:
        try:
            current_time = time.time()
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫, –µ—Å–ª–∏ –ø—Ä–æ—à–ª–æ –±–æ–ª—å—à–µ —á–∞—Å–∞ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞
            if current_time - last_restart_time > 3600:
                restart_count = 0
            
            logger.info(f"üîÑ –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ (–ø–æ–ø—ã—Ç–∫–∞ {restart_count + 1}/{max_restarts})...")
            asyncio.run(main_with_health())  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Ä—Å–∏—é —Å health server
            
        except KeyboardInterrupt:
            logger.info("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
            break
            
        except SystemExit as e:
            if e.code == 0:
                logger.info("‚úÖ –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã")
                break
            else:
                logger.error(f"üö® –ê–≤–∞—Ä–∏–π–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Å –∫–æ–¥–æ–º {e.code}")
                restart_count += 1
                last_restart_time = time.time()
                
        except Exception as e:
            logger.error(f"üí• –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")
            restart_count += 1
            last_restart_time = time.time()
            
        if restart_count < max_restarts:
            logger.info(f"‚è≥ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ {restart_delay} —Å–µ–∫—É–Ω–¥...")
            time.sleep(restart_delay)
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É —Å –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–æ–π, –Ω–æ –Ω–µ –±–æ–ª–µ–µ 120 —Å–µ–∫—É–Ω–¥
            restart_delay = min(restart_delay * 1.5, 120)
    
    if restart_count >= max_restarts:
        logger.error("üö® –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–≤. –ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")
    else:
        logger.info("üëã –ë–æ—Ç –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É")
