from datetime import datetime, timedelta
import os
import logging
import asyncio
import aiohttp
from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
from aiogram.types import ReplyKeyboardMarkup, KeyboardButton
from dotenv import load_dotenv
import urllib.parse
from bs4 import BeautifulSoup
import json
import re
import random
import sys
import atexit
import signal

# ===== –ó–ê–©–ò–¢–ê –û–¢ –ú–ù–û–ñ–ï–°–¢–í–ï–ù–ù–û–ì–û –ó–ê–ü–£–°–ö–ê =====
def handle_exit(signum, frame):
    print(f"üì¢ –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª {signum}, –∑–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É...")
    sys.exit(0)

signal.signal(signal.SIGTERM, handle_exit)
signal.signal(signal.SIGINT, handle_exit)

lock_file = "/tmp/telegram-bot.lock"

def cleanup_lock():
    try:
        if os.path.exists(lock_file):
            os.remove(lock_file)
            print("üîì –§–∞–π–ª –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —É–¥–∞–ª–µ–Ω")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ lock-—Ñ–∞–π–ª–∞: {e}")

def check_single_instance():
    try:
        if os.path.exists(lock_file):
            with open(lock_file, 'r') as f:
                old_pid = f.read().strip()
            try:
                os.kill(int(old_pid), 0)
                print(f"‚ùå –ë–æ—Ç —É–∂–µ –∑–∞–ø—É—â–µ–Ω –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ {old_pid}. –ó–∞–≤–µ—Ä—à–∞–µ–º.")
                sys.exit(1)
            except (ProcessLookupError, ValueError):
                print("üîÑ –°—Ç–∞—Ä—ã–π –ø—Ä–æ—Ü–µ—Å—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∑–∞–ø—É—Å–∫")
                os.remove(lock_file)
        
        with open(lock_file, 'w') as f:
            f.write(str(os.getpid()))
        
        atexit.register(cleanup_lock)
        print(f"üîí –§–∞–π–ª –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —Å–æ–∑–¥–∞–Ω (PID: {os.getpid()})")
        
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏: {e}")

check_single_instance()
# ===== –ö–û–ù–ï–¶ –ó–ê–©–ò–¢–´ =====

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

load_dotenv()

BOT_TOKEN = os.getenv('BOT_TOKEN')
if not BOT_TOKEN:
    logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
    exit(1)

bot = Bot(token=BOT_TOKEN)
dp = Dispatcher()

main_keyboard = ReplyKeyboardMarkup(
    keyboard=[
        [
            KeyboardButton(text="üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π"), 
            KeyboardButton(text="üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
        ],
        [
            KeyboardButton(text="‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏"), 
            KeyboardButton(text="üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫")
        ]
    ], 
    resize_keyboard=True
)

# –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–∏–ø–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
user_search_type = {}

class ImprovedNewsSearcher:
    def __init__(self):
        self.session = None
        self.cache = {}
        self.cache_timeout = 300
        self.russian_domains = [
            'rbc.ru', 'vedomosti.ru', 'kommersant.ru', 'ria.ru', 'tass.ru',
            'rt.com', 'lenta.ru', 'gazeta.ru', 'iz.ru', 'mk.ru', 'aif.ru',
            'rg.ru', 'vesti.ru', 'newsru.com', 'fontanka.ru', 'ng.ru',
            'echo.msk.ru', 'bfm.ru', 'forbes.ru', 'vc.ru', 'rb.ru', 'banki.ru',
            'cbr.ru', 'rosfinmonitoring.ru', 'government.ru', 'kremlin.ru',
            'minfin.ru', 'yandex.ru', 'mail.ru', 'rambler.ru',
            'sputniknews.com', 'rbth.com', 'russian.rt.com', 'themoscowtimes.com'
        ]

    async def get_session(self):
        if self.session is None:
            timeout = aiohttp.ClientTimeout(total=15, connect=5, sock_read=10)
            self.session = aiohttp.ClientSession(timeout=timeout)
        return self.session

    def get_cached_results(self, query):
        cache_key = f"search_{hash(query)}"
        if cache_key in self.cache:
            cache_time, results = self.cache[cache_key]
            if datetime.now() - cache_time < timedelta(seconds=self.cache_timeout):
                return results
        return None

    def set_cached_results(self, query, results):
        cache_key = f"search_{hash(query)}"
        self.cache[cache_key] = (datetime.now(), results)

    def is_russian_domain(self, url):
        try:
            domain = urllib.parse.urlparse(url).netloc.lower()
            return any(russian_domain in domain for russian_domain in self.russian_domains)
        except BaseException:
            return False

    def is_russian_text(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã"""
        return bool(re.search('[–∞-—è–ê-–Ø]', text))

    async def correct_spelling_auto(self, text):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏—è –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏—è —á–µ—Ä–µ–∑ Yandex Speller API"""
        try:
            if not self.is_russian_text(text):
                return text
                
            session = await self.get_session()
            encoded_text = urllib.parse.quote(text)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Yandex Speller API –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏
            url = f"https://speller.yandex.net/services/spellservice.json/checkText?text={encoded_text}&lang=ru,en"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    corrections = await response.json()
                    
                    if corrections:
                        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫ —Ç–µ–∫—Å—Ç—É
                        corrected_text = text
                        for correction in reversed(corrections):
                            if correction.get('s'):
                                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
                                fixed_word = correction['s'][0]
                                wrong_word = correction['word']
                                corrected_text = corrected_text.replace(wrong_word, fixed_word)
                        
                        logger.info(f"üìù –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏–µ: '{text}' -> '{corrected_text}'")
                        return corrected_text
                    
            return text
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏—è: {e}")
            return text

    async def translate_to_english_auto(self, text):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —á–µ—Ä–µ–∑ Yandex Translate API"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ä—É—Å—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã
            if not self.is_russian_text(text):
                return text
                
            session = await self.get_session()
            encoded_text = urllib.parse.quote(text)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Yandex Translate API (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π, —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏)
            # –≠—Ç–æ—Ç API –∫–ª—é—á –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π, –¥–ª—è —Ä–∞–±–æ—Ç—ã –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —Å–≤–æ–π
            url = f"https://translate.yandex.net/api/v1.5/tr.json/translate?key=trnsl.1.1.20230101T000000Z.1234567890.abcdef&lang=ru-en&text={encoded_text}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    if data.get('code') == 200 and data.get('text'):
                        translated = data['text'][0]
                        logger.info(f"üåç –ê–≤—Ç–æ–ø–µ—Ä–µ–≤–æ–¥: '{text}' -> '{translated}'")
                        return translated
            
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º MyMemory Translation API –µ—Å–ª–∏ Yandex –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
            return await self.translate_fallback(text)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
            return await self.translate_fallback(text)

    async def translate_fallback(self, text):
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ —á–µ—Ä–µ–∑ MyMemory API"""
        try:
            if not self.is_russian_text(text):
                return text
                
            session = await self.get_session()
            encoded_text = urllib.parse.quote(text)
            
            url = f"https://api.mymemory.translated.net/get?q={encoded_text}&langpair=ru|en"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    if data.get('responseData', {}).get('translatedText'):
                        translated = data['responseData']['translatedText']
                        logger.info(f"üåç –†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥: '{text}' -> '{translated}'")
                        return translated
            
            return text
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
            return text

    async def prepare_international_query(self, query):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏—è + –ø–µ—Ä–µ–≤–æ–¥"""
        try:
            logger.info(f"üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
            
            # –®–∞–≥ 1: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏—è –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏—è
            corrected_query = await self.correct_spelling_auto(query)
            
            # –®–∞–≥ 2: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
            translated_query = await self.translate_to_english_auto(corrected_query)
            
            logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{translated_query}'")
            return translated_query
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return query

    async def search_yandex_news_direct(self, query):
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://yandex.ru/news/search?text={encoded_query}"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []

                    news_cards = soup.find_all('article', class_='mg-card')[:10]

                    for card in news_cards:
                        try:
                            title_elem = card.find('h2', class_='mg-card__title') or card.find('a', class_='mg-card__link')
                            if not title_elem:
                                continue

                            title = title_elem.get_text().strip()
                            link = title_elem.get('href', '')

                            if link.startswith('https://news.yandex.ru/yandsearch?'):
                                match = re.search(r'cl4url=([^&]+)', link)
                                if match:
                                    link = urllib.parse.unquote(match.group(1))
                            elif link.startswith('/'):
                                link = f"https://yandex.ru{link}"

                            if link and not any(
                                domain in link for domain in [
                                    'google.com/search',
                                    'yandex.ru/search']):
                                articles.append({
                                    'title': title,
                                    'url': link,
                                    'language': 'ru'
                                })
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ –Ø–Ω–¥–µ–∫—Å: {e}")
                            continue

                    return articles
            return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []

    async def search_bing_news_improved(self, query, market='ru-RU', exclude_russian=False):
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            
            if market == 'en-US':
                url = f"https://www.bing.com/news/search?q={encoded_query}&cc=us&setlang=en"
            else:
                url = f"https://www.bing.com/news/search?q={encoded_query}&cc={market}"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept-Language': 'en-US,en;q=0.9' if market == 'en-US' else 'ru-RU,ru;q=0.9',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []

                    news_cards = soup.find_all('div', class_='news-card')[:10]
                    if not news_cards:
                        news_cards = soup.find_all('div', class_='tile')[:10]
                    if not news_cards:
                        news_cards = soup.find_all('article')[:10]

                    for card in news_cards:
                        try:
                            title_elem = (card.find('a', class_='title') or
                                        card.find('a', class_=re.compile('title')) or
                                        card.find('h2') or
                                        card.find('h3') or
                                        card.find('a', attrs={'href': True}))

                            if title_elem and title_elem.get('href'):
                                title = title_elem.get_text().strip()
                                url = title_elem.get('href')

                                if url.startswith('/'):
                                    url = f"https://www.bing.com{url}"

                                if 'bing.com/news/search' in url:
                                    continue

                                # –°—Ç—Ä–æ–≥–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä—É—Å—Å–∫–∏—Ö –¥–æ–º–µ–Ω–æ–≤
                                if exclude_russian and self.is_russian_domain(url):
                                    continue

                                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä—É—Å—Å–∫–æ–º—É —Ç–µ–∫—Å—Ç—É –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
                                if exclude_russian and self.is_russian_text(title):
                                    continue

                                if url and not any(
                                    search_domain in url for search_domain in [
                                        'google.com/search',
                                        'bing.com/search']):
                                    articles.append({
                                        'title': title,
                                        'url': url,
                                        'language': 'en' if market == 'en-US' else 'ru'
                                    })
                        except Exception:
                            continue

                    return articles
            return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ Bing News: {e}")
            return []

    async def search_google_news_english(self, query, exclude_russian=True):
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://news.google.com/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []
                    news_cards = soup.find_all('article')[:12]

                    for card in news_cards:
                        try:
                            title_elem = card.find('h3') or card.find('h4') or card.find('a', attrs={'href': True})
                            if title_elem:
                                title = title_elem.get_text().strip()
                                link_elem = title_elem.find_parent('a') if title_elem.name != 'a' else title_elem
                                if link_elem and link_elem.get('href'):
                                    url = link_elem.get('href')
                                    if url.startswith('./'):
                                        url = f"https://news.google.com{url[1:]}"
                                    
                                    if 'news.google.com' in url:
                                        continue

                                    # –°—Ç—Ä–æ–≥–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä—É—Å—Å–∫–∏—Ö –¥–æ–º–µ–Ω–æ–≤ –∏ —Ç–µ–∫—Å—Ç–∞
                                    if exclude_russian and (self.is_russian_domain(url) or self.is_russian_text(title)):
                                        continue

                                    if url and url.startswith('http'):
                                        articles.append({
                                            'title': title,
                                            'url': url,
                                            'language': 'en'
                                        })
                        except Exception:
                            continue

                    return articles
            return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ Google News: {e}")
            return []

    async def search_duckduckgo_improved(self, query, exclude_russian=True):
        try:
            session = await self.get_session()
            encoded_query = urllib.parse.quote(query)
            url = f"https://html.duckduckgo.com/html/?q={encoded_query}+news&kl=us-en"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }

            async with session.get(url, headers=headers) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    articles = []
                    results = soup.find_all('div', class_='result')[:12]

                    for result in results:
                        try:
                            title_elem = result.find('a', class_='result__a')
                            if title_elem:
                                title = title_elem.get_text().strip()
                                url = title_elem.get('href', '')

                                if 'duckduckgo.com' in url:
                                    match = re.search(r'uddg=([^&]+)', url)
                                    if match:
                                        url = urllib.parse.unquote(match.group(1))

                                if any(
                                    domain in url for domain in [
                                        'google.com/search',
                                        'bing.com/search',
                                        'yandex.ru/search']):
                                    continue

                                # –°—Ç—Ä–æ–≥–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä—É—Å—Å–∫–∏—Ö –¥–æ–º–µ–Ω–æ–≤ –∏ —Ç–µ–∫—Å—Ç–∞
                                if exclude_russian and (self.is_russian_domain(url) or self.is_russian_text(title)):
                                    continue

                                if url and url.startswith('http'):
                                    articles.append({
                                        'title': title,
                                        'url': url,
                                        'language': 'en'
                                    })
                        except Exception:
                            continue

                    return articles
            return []
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ DuckDuckGo: {e}")
            return []

    async def universal_search(self, query, search_type="all"):
        cache_key = f"{search_type}_{query}"
        cached_results = self.get_cached_results(cache_key)
        if cached_results:
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            return cached_results

        all_results = []

        try:
            if search_type in ["all", "russian"]:
                logger.info(f"üîç –ü–æ–∏—Å–∫ –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö: {query}")

                yandex_results = await self.search_yandex_news_direct(query)
                all_results.extend(yandex_results)
                logger.info(f"‚úÖ –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–∏: {len(yandex_results)} —Å—Ç–∞—Ç–µ–π")

                bing_ru_results = await self.search_bing_news_improved(query, 'ru-RU')
                all_results.extend(bing_ru_results)
                logger.info(f"‚úÖ Bing –†–æ—Å—Å–∏—è: {len(bing_ru_results)} —Å—Ç–∞—Ç–µ–π")

            if search_type in ["all", "international"]:
                logger.info(f"üåç –ü–æ–∏—Å–∫ –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö: {query}")

                # –î–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
                international_query = await self.prepare_international_query(query)
                logger.info(f"üåç –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {international_query}")

                google_results = await self.search_google_news_english(international_query, exclude_russian=True)
                all_results.extend(google_results)
                logger.info(f"‚úÖ Google News: {len(google_results)} —Å—Ç–∞—Ç–µ–π")

                bing_en_results = await self.search_bing_news_improved(international_query, 'en-US', exclude_russian=True)
                all_results.extend(bing_en_results)
                logger.info(f"‚úÖ Bing International: {len(bing_en_results)} —Å—Ç–∞—Ç–µ–π")

                duckduckgo_results = await self.search_duckduckgo_improved(international_query, exclude_russian=True)
                all_results.extend(duckduckgo_results)
                logger.info(f"‚úÖ DuckDuckGo: {len(duckduckgo_results)} —Å—Ç–∞—Ç–µ–π")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º –ø–æ–∏—Å–∫–µ: {e}")

        # –°—Ç—Ä–æ–≥–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        filtered_results = []
        for result in all_results:
            if result and result.get('url'):
                url = result['url'].lower()
                if any(search_domain in url for search_domain in [
                    'google.com/search',
                    'bing.com/search',
                    'yandex.ru/search',
                    'news.google.com',
                    'news.yandex.ru/yandsearch'
                ]):
                    continue
                
                # –°—Ç—Ä–æ–≥–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
                if search_type == "international":
                    if (self.is_russian_domain(url) or 
                        self.is_russian_text(result.get('title', ''))):
                        continue
                    
                if url.startswith('http') and len(url) > 20:
                    filtered_results.append(result)

        seen_urls = set()
        unique_results = []
        for result in filtered_results:
            if result['url'] not in seen_urls:
                seen_urls.add(result['url'])
                unique_results.append(result)

        self.set_cached_results(cache_key, unique_results[:10])
        logger.info(f"üìä –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(unique_results)} —Å—Ç–∞—Ç–µ–π")
        return unique_results[:10]

    async def get_fresh_news_today(self):
        cache_key = "fresh_news_today"
        cached_results = self.get_cached_results(cache_key)
        if cached_results:
            return cached_results

        logger.info("üîç –ü–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è...")

        today_queries = [
            "–≠–ü–† —Å–µ–≥–æ–¥–Ω—è",
            "–≠–ü–† –Ω–æ–≤–æ—Å—Ç–∏ —Å–µ–≥–æ–¥–Ω—è",
            "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞ —Å–µ–≥–æ–¥–Ω—è",
            "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∞–≤–æ–≤–æ–π —Ä–µ–∂–∏–º –Ω–æ–≤–æ—Å—Ç–∏"
        ]

        all_articles = []

        for query in today_queries:
            try:
                logger.info(f"üì¢ –ü–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {query}")

                yandex_results = await self.search_yandex_news_direct(query)
                bing_results = await self.search_bing_news_improved(query, 'ru-RU')

                all_articles.extend(yandex_results)
                all_articles.extend(bing_results)

                await asyncio.sleep(1)

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
                continue

        filtered_articles = []
        for article in all_articles:
            if article and article.get('url'):
                url = article['url'].lower()
                if not any(search_domain in url for search_domain in [
                    'google.com/search', 'bing.com/search', 'yandex.ru/search'
                ]) and url.startswith('http') and len(url) > 20:
                    filtered_articles.append(article)

        seen_urls = set()
        unique_articles = []
        for article in filtered_articles:
            if article['url'] not in seen_urls:
                seen_urls.add(article['url'])
                unique_articles.append(article)

        if len(unique_articles) < 4:
            logger.info("üîç –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π...")
            backup_queries = ["–≠–ü–†", "—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞ –†–æ—Å—Å–∏—è"]
            for query in backup_queries:
                try:
                    backup_results = await self.universal_search(query, "all")
                    for article in backup_results:
                        if article['url'] not in seen_urls:
                            seen_urls.add(article['url'])
                            unique_articles.append(article)
                    await asyncio.sleep(1)
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")

        final_articles = unique_articles[:8]
        self.set_cached_results(cache_key, final_articles)

        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(final_articles)}")
        return final_articles

    async def close(self):
        if self.session:
            await self.session.close()

news_searcher = ImprovedNewsSearcher()

@dp.message(Command("start"))
async def cmd_start(message: types.Message):
    await message.answer(
        "üåê –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –æ–± –≠–ü–†\n\n"
        "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π ‚Äì —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏\n"
        "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ‚Äì —Ç–æ–ª—å–∫–æ –∑–∞—Ä—É–±–µ–∂–Ω—ã–µ –°–ú–ò\n"
        "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ ‚Äì –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏\n"
        "üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ ‚Äì –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n\n"
        "–ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ —á—Ç–æ –∏—â–µ—Ç–µ!",
        reply_markup=main_keyboard
    )

@dp.message(Command("help"))
async def cmd_help(message: types.Message):
    help_text = """
üìñ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –æ–± –≠–ü–†

üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π ‚Äì —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ‚Äì —Ç–æ–ª—å–∫–æ –∑–∞—Ä—É–±–µ–∂–Ω—ã–µ –°–ú–ò
‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ ‚Äì –ø–æ–∏—Å–∫ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è
üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ ‚Äì –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º

üí° –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:
    ‚Ä¢ –≠–ü–† –≤ —Ñ–∏–Ω–∞–Ω—Å–∞—Ö
‚Ä¢ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞
‚Ä¢ –Ω–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –≠–ü–†
‚Ä¢ Russia fintech regulation

‚ö° –ö–Ω–æ–ø–∫–∞ '–°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏' –∏—â–µ—Ç —Å–∞–º—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è!
"""
    await message.answer(help_text)

@dp.message(lambda message: message.text == "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π")
async def search_epr_news(message: types.Message):
    user_id = message.from_user.id
    user_search_type[user_id] = 'all'
    await message.answer("üîç –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π:")

@dp.message(lambda message: message.text == "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
async def international_sources(message: types.Message):
    user_id = message.from_user.id
    user_search_type[user_id] = 'international'
    await message.answer("üåç –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π):")

@dp.message(lambda message: message.text == "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏")
async def fresh_news(message: types.Message):
    await message.answer("‚ö° –ò—â—É —Å–∞–º—ã–µ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏")

    try:
        articles = await news_searcher.get_fresh_news_today()

        if articles:
            response = "‚ö° –°–∞–º—ã–µ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏:\n\n"

            for i, article in enumerate(articles, 1):
                response += f"{i}. {article['title']}\n"
                response += f"   üîó {article['url']}\n\n"

                if len(response) > 3500:
                    response += "... (–ø–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ —Å—Ç–∞—Ç—å–∏)"
                    break

        else:
            response = "üòî –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è.\n\n"
            response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É."

        await message.answer(response)

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
        await message.answer("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")

@dp.message(lambda message: message.text == "üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫")
async def quick_search(message: types.Message):
    user_id = message.from_user.id
    user_search_type[user_id] = 'quick'
    await message.answer("üìä –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")

@dp.message()
async def handle_text(message: types.Message):
    user_text = message.text.strip()
    user_id = message.from_user.id

    buttons = [
        "üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π",
        "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏",
        "‚ö° –°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏",
        "üìä –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫"]
    if user_text.startswith('/') or user_text in buttons:
        return

    await message.answer(f"üîç –ò—â—É –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{user_text}'...")

    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø–æ–∏—Å–∫–∞
        search_type = user_search_type.pop(user_id, 'all')
        
        if search_type == 'quick':
            # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫: 3 —Å—Ç–∞—Ç—å–∏ –∏–∑ —Ä—É—Å—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ 3 –∏–∑ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö
            russian_articles = await news_searcher.universal_search(user_text, "russian")
            
            # –î–ª—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–π —á–∞—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            international_query = await news_searcher.prepare_international_query(user_text)
            international_articles = await news_searcher.universal_search(international_query, "international")
            
            articles = russian_articles[:3] + international_articles[:3]
            
            if articles:
                response = f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ '{user_text}':\n\n"
                for i, article in enumerate(articles, 1):
                    response += f"{i}. {article['title']}\n"
                    response += f"   üîó {article['url']}\n\n"
            else:
                response = f"üòî –ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_text}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π.\n\n"
                response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞."
                
        elif search_type == 'international':
            # –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            international_query = await news_searcher.prepare_international_query(user_text)
            articles = await news_searcher.universal_search(international_query, "international")
            
            if articles:
                response = f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ '{user_text}':\n\n"
                response += "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:\n\n"
                for i, article in enumerate(articles[:6], 1):
                    response += f"{i}. {article['title']}\n"
                    response += f"   üîó {article['url']}\n\n"
            else:
                response = f"üòî –ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_text}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö.\n\n"
                response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞."
                
        else:  # search_type == 'all'
            # –û–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫: —Ä—É—Å—Å–∫–∏–µ –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            articles = await news_searcher.universal_search(user_text, "all")
            
            if articles:
                russian_articles = [a for a in articles if a.get('language') == 'ru']
                english_articles = [a for a in articles if a.get('language') == 'en']

                response = f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ '{user_text}':\n\n"

                if russian_articles:
                    response += "üá∑üá∫ –†–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:\n\n"
                    for i, article in enumerate(russian_articles[:3], 1):
                        response += f"{i}. {article['title']}\n"
                        response += f"   üîó {article['url']}\n\n"

                if english_articles:
                    response += "üåç –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:\n\n"
                    for i, article in enumerate(english_articles[:3], 1):
                        response += f"{i}. {article['title']}\n"
                        response += f"   üîó {article['url']}\n\n"
            else:
                response = f"üòî –ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_text}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π.\n\n"
                response += "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞."

        await message.answer(response)

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        await message.answer(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –∑–∞–ø—Ä–æ—Å.")

async def main():
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –±–æ—Ç–∞...")

    try:
        await bot.delete_webhook(drop_pending_updates=True)
        await dp.start_polling(bot)
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞: {e}")
    finally:
        await news_searcher.close()

if __name__ == "__main__":
    asyncio.run(main())
