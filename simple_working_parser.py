import aiohttp
import asyncio
import logging
from datetime import datetime
import urllib.parse

logger = logging.getLogger(__name__)

class WorkingParser:
    def __init__(self):
        self.user_agent = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    
    async def search_news(self, query, max_results=8):
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫, –∫–æ—Ç–æ—Ä—ã–π –≤—Å–µ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç"""
        results = []
        
        # –í—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø–æ–∏—Å–∫–æ–≤—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏
        search_engines = [
            {
                'name': 'Google News',
                'url': f'https://news.google.com/search?q={urllib.parse.quote(query + " –†–æ—Å—Å–∏—è")}&hl=ru&gl=RU&ceid=RU:ru'
            },
            {
                'name': '–Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–∏', 
                'url': f'https://yandex.ru/news/search?text={urllib.parse.quote(query + " –≠–ü–†")}'
            },
            {
                'name': 'Telegram Search',
                'url': f'https://t.me/search?q={urllib.parse.quote(query)}'
            },
            {
                'name': 'DuckDuckGo',
                'url': f'https://duckduckgo.com/?q={urllib.parse.quote(query + " –†–æ—Å—Å–∏—è –Ω–æ–≤–æ—Å—Ç–∏")}&ia=news'
            }
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        official_sources = [
            {
                'title': 'Digital.gov.ru - –†–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã–µ –ø–µ—Å–æ—á–Ω–∏—Ü—ã',
                'url': 'https://digital.gov.ru/ru/activity/directions/regulatory_sandbox/',
                'source': '–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç',
                'description': '–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã—Ö –ø–µ—Å–æ—á–Ω–∏—Ü–∞—Ö –≤ –†–§'
            },
            {
                'title': '–ú–∏–Ω—ç–∫–æ–Ω–æ–º—Ä–∞–∑–≤–∏—Ç–∏—è - –†–µ–≥—É–ª–∏—Ä—É–µ–º—ã–π —Å—ç–Ω–¥–±–æ–∫—Å',
                'url': 'https://www.economy.gov.ru/material/directions/reguliruemyy_sandboks/',
                'source': '–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç', 
                'description': '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ–≥—É–ª–∏—Ä—É–µ–º—ã—Ö —Å—ç–Ω–¥–±–æ–∫—Å–∞—Ö –¥–ª—è –±–∏–∑–Ω–µ—Å–∞'
            },
            {
                'title': '–¶–ë –†–§ - –§–∏–Ω—Ç–µ—Ö –∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏',
                'url': 'https://www.cbr.ru/fintech/',
                'source': '–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç',
                'description': '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∏–Ω—Ç–µ—Ö–µ –∏ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã—Ö –∏–Ω–Ω–æ–≤–∞—Ü–∏—è—Ö'
            }
        ]
        
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        for engine in search_engines:
            results.append({
                'title': f'üîç {engine["name"]} - {query}',
                'url': engine['url'],
                'source': engine['name'],
                'description': f'–ù–∞–∂–º–∏—Ç–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ {engine["name"]}',
                'date': datetime.now().strftime("%Y-%m-%d %H:%M"),
                'timestamp': datetime.now().timestamp(),
                'type': 'search_engine'
            })
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        for source in official_sources:
            if query.lower() in source['title'].lower() or any(keyword in query.lower() for keyword in ['—ç–ø—Ä', '–ø–µ—Å–æ—á–Ω–∏—Ü–∞', '—Ä–µ–≥—É–ª—è—Ç–æ—Ä']):
                results.append({
                    'title': source['title'],
                    'url': source['url'],
                    'source': source['source'],
                    'description': source['description'],
                    'date': datetime.now().strftime("%Y-%m-%d"),
                    'timestamp': datetime.now().timestamp(),
                    'type': 'official'
                })
        
        return results[:max_results]
    
    async def test_connection(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤–Ω–µ—à–Ω–∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤"""
        test_urls = [
            'https://www.google.com',
            'https://yandex.ru',
            'https://telegram.org'
        ]
        
        available = []
        async with aiohttp.ClientSession() as session:
            for url in test_urls:
                try:
                    async with session.get(url, timeout=5) as response:
                        if response.status == 200:
                            available.append(url)
                except:
                    pass
        
        return available

# –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä
async def test_working_parser():
    parser = WorkingParser()
    print("üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–±–æ—á–∏–π –ø–∞—Ä—Å–µ—Ä...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
    available = await parser.test_connection()
    print(f"‚úÖ –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã: {len(available)}")
    for resource in available:
        print(f"   - {resource}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
    results = await parser.search_news("–≠–ü–†")
    print(f"üìä –°–æ–∑–¥–∞–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
    
    for i, item in enumerate(results[:3], 1):
        print(f"{i}. {item['title']}")
        print(f"   URL: {item['url']}")
        print(f"   Source: {item['source']}")
        print()

if __name__ == "__main__":
    asyncio.run(test_working_parser())
